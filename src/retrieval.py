# src/retrieval.py
"""
Retrieval and QA evaluation (Phase 4)

æ ¸å¿ƒæ”¹åŠ¨ï¼š
- ç§»é™¤ HybridRetriever ä¾èµ–
- ä½¿ç”¨è‡ªå®šä¹‰ MultiHopRetriever
- æ·»åŠ  0-hop ä½œä¸º Baseline (çº¯å‘é‡æ£€ç´¢)
"""

import time
import pandas as pd
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path

from neo4j_graphrag.retrievers.base import Retriever
from neo4j_graphrag.types import RawSearchResult, RetrieverResultItem

from config import CONFIG, RESULT_DIR
from src.models import OllamaVectorEmbedder


# ============================================================
# 1. è‡ªå®šä¹‰å¤šè·³æ£€ç´¢å™¨ (MultiHopRetriever)
# ============================================================

class MultiHopRetriever(Retriever):
    """
    æ”¯æŒå¤šè·³æ¨ç†çš„è‡ªå®šä¹‰æ£€ç´¢å™¨
    
    æ£€ç´¢ç­–ç•¥:
    - 0-hop: çº¯å‘é‡æ£€ç´¢ (Baseline)ï¼Œåªè¿”å› Chunk
    - 1-hop: Chunk -> Entity (äº†è§£ Chunk é‡Œæœ‰å“ªäº›å®ä½“)
    - 2-hop: Chunk -> Entity -> Neighbor Entity (Graph RAG æ ‡å‡†æ¨¡å¼)
    - 3-hop: æ·±åº¦éå† (é€šå¸¸æœ‰å™ªå£°ï¼Œé€‚åˆæ¶ˆèå¯¹æ¯”)
    """
    
    def __init__(
        self,
        driver,
        vector_index_name: str,
        embedder,
        retrieval_depth: int = 0,  # é»˜è®¤æ”¹ä¸º 0 (Baseline)
        max_entities_per_hop: int = 10,
        neo4j_database: str = None,
    ):
        self.driver = driver
        self.vector_index_name = vector_index_name
        self.embedder = embedder
        self.retrieval_depth = retrieval_depth
        self.max_entities_per_hop = max_entities_per_hop
        self.neo4j_database = neo4j_database
        
    def search(
        self,
        query_text: str = None,
        query_vector: List[float] = None,
        top_k: int = 5,
    ) -> RawSearchResult:
        """æ‰§è¡Œå¤šè·³æ£€ç´¢"""
        
        # 1. è·å–æŸ¥è¯¢å‘é‡
        if query_vector is None and query_text is not None:
            query_vector = self.embedder.embed_query(query_text)
        
        # 2. æ„å»º Cypher æŸ¥è¯¢
        cypher_query = self._build_multihop_cypher()
        
        # 3. æ‰§è¡ŒæŸ¥è¯¢
        with self.driver.session(database=self.neo4j_database) as session:
            result = session.run(
                cypher_query,
                vector_index_name=self.vector_index_name,
                query_vector=query_vector,
                top_k=top_k,
                max_entities=self.max_entities_per_hop
            )
            # å°† Neo4j Result è½¬ä¸º listï¼Œç¬¦åˆ RawSearchResult è¦æ±‚
            records = list(result)
        
        return RawSearchResult(records=records)
    
    def _build_multihop_cypher(self) -> str:
        """æ ¹æ® retrieval_depth æ„å»ºä¸åŒçš„ Cypher æŸ¥è¯¢"""
        
        # âœ… æ–°å¢ï¼š0-hop (Vector Only Baseline)
        if self.retrieval_depth == 0:
            return """
            CALL db.index.vector.queryNodes($vector_index_name, $top_k, $query_vector)
            YIELD node, score
            // 0-hop åªè¿”å› Chunk èŠ‚ç‚¹æœ¬èº«
            RETURN node, score
            ORDER BY score DESC
            """
        
        elif self.retrieval_depth == 1:
            # 1-hop: Chunk -> Entity (æ£€ç´¢ Chunkï¼Œå¹¶é™„å¸¦å…¶åŒ…å«çš„å®ä½“ä¿¡æ¯)
            return """
            CALL db.index.vector.queryNodes($vector_index_name, $top_k, $query_vector)
            YIELD node AS initial_chunk, score
            
            // æ‰©å±•åˆ° MENTIONS çš„å®ä½“
            OPTIONAL MATCH (initial_chunk)-[:MENTIONS]->(e:Entity)
            
            // èšåˆè¿”å›
            WITH initial_chunk, score, collect(DISTINCT e.name) as entity_names
            
            // è¿”å›æ ‡å‡†æ ¼å¼
            RETURN initial_chunk as node, score, entity_names
            ORDER BY score DESC
            """
        
        elif self.retrieval_depth == 2:
            # 2-hop: Chunk -> Entity -> Neighbor Entity (Graph RAG æ ‡å‡†æ¨¡å¼)
            return """
            CALL db.index.vector.queryNodes($vector_index_name, $top_k, $query_vector)
            YIELD node AS initial_chunk, score
            
            // 1. æ‰¾åˆ°è¯¥ Chunk æåˆ°çš„å®ä½“
            MATCH (initial_chunk)-[:MENTIONS]->(e1:Entity)
            WITH initial_chunk, score, e1
            LIMIT $max_entities
            
            // 2. æ‰©å±•åˆ°é‚»å±…å®ä½“ (2-hop)
            OPTIONAL MATCH (e1)-[r:RELATION]->(e2:Entity)
            WITH initial_chunk, score, e1, e2
            LIMIT $max_entities * 2
            
            // 3. æ‰¾å›åŒ…å«è¿™äº›é‚»å±…å®ä½“çš„ *å…¶ä»–* Chunks (æ‰©å……ä¸Šä¸‹æ–‡)
            OPTIONAL MATCH (related_chunk:Chunk)-[:MENTIONS]->(e2)
            WHERE related_chunk <> initial_chunk
            
            // èšåˆæ‰€æœ‰ç›¸å…³ Chunk
            WITH initial_chunk, score, collect(DISTINCT related_chunk) AS related_chunks
            
            // å±•å¼€å¹¶æ··åˆ (åˆå§‹ Chunk + å…³è” Chunk)
            UNWIND [initial_chunk] + related_chunks AS node
            
            // å¯¹å…³è” Chunk è¿›è¡Œé™æƒ (Decay)
            WITH node, 
                 CASE WHEN node = initial_chunk THEN score ELSE score * 0.7 END AS adjusted_score
            
            RETURN DISTINCT node, adjusted_score AS score
            ORDER BY score DESC
            LIMIT $top_k * 2
            """
        
        elif self.retrieval_depth == 3:
            # 3-hop: æ·±åº¦éå† (é€šå¸¸ä¼šæœ‰å™ªå£°ï¼Œé€‚åˆæ¶ˆèå¯¹æ¯”)
            return """
            CALL db.index.vector.queryNodes($vector_index_name, $top_k, $query_vector)
            YIELD node AS initial_chunk, score
            
            // æ·±åº¦è·¯å¾„éå† (1~2 å±‚å…³ç³»)
            MATCH path = (initial_chunk)-[:MENTIONS]->(e1:Entity)-[:RELATION*1..2]->(e_final:Entity)
            WITH initial_chunk, score, e_final, length(path) AS path_length
            LIMIT $max_entities * 3
            
            OPTIONAL MATCH (related_chunk:Chunk)-[:MENTIONS]->(e_final)
            WHERE related_chunk <> initial_chunk
            
            WITH initial_chunk, score, path_length, collect(DISTINCT related_chunk) AS related_chunks
            
            UNWIND [initial_chunk] + related_chunks AS node
            WITH node, 
                 CASE 
                     WHEN node = initial_chunk THEN score 
                     ELSE score * 0.5 
                 END AS adjusted_score
                 
            RETURN DISTINCT node, adjusted_score AS score
            ORDER BY score DESC
            LIMIT $top_k * 3
            """
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ retrieval_depth: {self.retrieval_depth}")


# ============================================================
# 2. è¾…åŠ©å‡½æ•°ï¼šä¸Šä¸‹æ–‡æå–ä¸å¤„ç†
# ============================================================

def extract_contexts(raw_result: RawSearchResult, top_k: int) -> List[Dict[str, Any]]:
    """
    ä» RawSearchResult æå–ä¸Šä¸‹æ–‡
    
    Returns:
        List of dicts with keys: rank, score, text, chunk_id
    """
    contexts: List[Dict[str, Any]] = []
    
    if not raw_result or not raw_result.records:
        return contexts
    
    for rank, record in enumerate(raw_result.records[:top_k], start=1):
        node = record.get('node')
        score = record.get('score', 0.0)
        
        if node:
            # ä» Neo4j node æå–å±æ€§
            text = node.get('text', '')
            chunk_id = node.get('id', '')
            source = node.get('source', '')
            
            contexts.append({
                "rank": rank,
                "score": float(score) if score else 0.0,
                "text": text,
                "chunk_id": chunk_id,
                "source": source
            })
    
    return contexts


def expand_graph_context(driver, chunk_ids: List[str], limit_rel: int = 6) -> List[Dict[str, str]]:
    """
    æ‰©å±•å›¾è°±ä¸Šä¸‹æ–‡ï¼šä¸ºæ¯ä¸ª Chunk æå–å…¶ Entity å’Œ Relations
    
    ç”¨äºå¢å¼ºå¯è§£é‡Šæ€§
    """
    if not chunk_ids:
        return []
    
    with driver.session() as session:
        rows = session.run(
            """
            MATCH (c:Chunk)
            WHERE c.id IN $chunk_ids
            MATCH (c)-[:MENTIONS]->(e:Entity)
            OPTIONAL MATCH (e)-[r:RELATION]->(t:Entity)
            RETURN c.id AS chunk_id,
                   e.name AS entity,
                   collect({relation: r.type, tail: t.name})[0..$limit] AS relations
            """,
            chunk_ids=chunk_ids,
            limit=limit_rel,
        ).data()
    
    formatted: List[Dict[str, str]] = []
    for row in rows:
        relations = [
            f"{item.get('relation')}â†’{item.get('tail')}"
            for item in (row.get("relations") or [])
            if item.get("relation") and item.get("tail")
        ]
        formatted.append(
            {
                "chunk_id": row.get("chunk_id"),
                "entity": row.get("entity"),
                "relations": ", ".join(relations) if relations else "(æ— è¿ç»“)",
            }
        )
    
    return formatted


# ============================================================
# 3. é«˜å±‚æ£€ç´¢å¼•æ“ (RetrievalEngine)
# ============================================================

@dataclass
class QAResult:
    """å•æ¬¡ QA ç»“æœ"""
    question: str
    predicted_answer: str
    reference_answer: Optional[str]
    hop: int
    top_k: int
    num_chunks: int
    inference_latency_ms: float
    contexts: List[Dict[str, Any]]


class RetrievalEngine:
    """
    è´Ÿè´£å•æ¬¡é—®ç­”ä¸ä¸Šä¸‹æ–‡ç”Ÿæˆ
    """
    def __init__(self, driver, ollama_client):
        self.driver = driver
        self.ollama_client = ollama_client
        self.embedder = OllamaVectorEmbedder(
            ollama_client, 
            CONFIG["models"]["embed_model"]
        )
        self.llm_model = CONFIG["models"]["llm_model"]
        self.temperature = CONFIG["generation"]["temperature"]

    def run_qa(
        self, 
        question: str, 
        hop: int = 0, 
        top_k: int = 5,
        reference_answer: Optional[str] = None,
        verbose: bool = False
    ) -> QAResult:
        """
        æ‰§è¡Œå•æ¬¡ QA
        
        Args:
            question: é—®é¢˜æ–‡æœ¬
            hop: è·³æ•° (0=baseline, 1=1-hop, 2=2-hop, 3=3-hop)
            top_k: è¿”å›å‰ k ä¸ª chunks
            reference_answer: å‚è€ƒç­”æ¡ˆï¼ˆç”¨äºè¯„ä¼°ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            QAResult
        """
        start_time = time.perf_counter()
        
        # 1. åˆå§‹åŒ–æ£€ç´¢å™¨
        retriever = MultiHopRetriever(
            driver=self.driver,
            vector_index_name=CONFIG["infrastructure"]["vector_index_name"],
            embedder=self.embedder,
            retrieval_depth=hop,
            max_entities_per_hop=CONFIG["retrieval"].get("max_nodes_per_hop", 10)
        )
        
        # 2. æ£€ç´¢
        raw_result = retriever.search(query_text=question, top_k=top_k)
        
        # 3. æå–ä¸Šä¸‹æ–‡
        contexts = extract_contexts(raw_result, top_k)
        context_texts = [c["text"] for c in contexts if c["text"]]
        context_str = "\n\n".join(context_texts) if context_texts else "No context found."
        
        # 4. ç”Ÿæˆå›ç­”
        answer = self._generate_answer(question, context_str)
        
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        
        result = QAResult(
            question=question,
            predicted_answer=answer,
            reference_answer=reference_answer,
            hop=hop,
            top_k=top_k,
            num_chunks=len(contexts),
            inference_latency_ms=elapsed_ms,
            contexts=contexts
        )
        
        if verbose:
            self._print_qa_result(result)
        
        return result

    def _generate_answer(self, question: str, context: str) -> str:
        """
        ç”Ÿæˆå›ç­”
        """
        system_instruction = (
            "Answer requirements:\n"
            f"1. Answer in {CONFIG['models']['answer_language']} naturally and fluently.\n"  # è‡ªç„¶æµæš¢
            "2. Provide a concise but complete explanation based strictly on the context.\n" # ç°¡æ½”ä½†å®Œæ•´
            "3. Include causality or reasoning if the question asks 'why' or 'how'.\n" # åŒ…å«å› æœæ¨ç†
            "4. Do NOT use introductory phrases like 'Based on the text'.\n" # å»é™¤å»¢è©±
        )
        
        prompt = f"""Context:
{context}

Question: {question}

{system_instruction}

Answer:"""
        
        try:
            response = self.ollama_client.chat(
                model=self.llm_model,
                messages=[{"role": "user", "content": prompt}],
                options={"temperature": self.temperature, "top_p": 0.9},
            )
            content = response.get("message", {}).get("content", "")
            return content.strip()
        except Exception as e:
            return f"[Error: {e}]"
    
    def _print_qa_result(self, result: QAResult):
        """æ‰“å° QA ç»“æœ"""
        print(f"\n{'='*70}")
        print(f"â“ Question: {result.question}")
        print(f"ğŸŸ© Answer: {result.predicted_answer}")
        if result.reference_answer:
            print(f"ğŸ“– Reference: {result.reference_answer}")
        print(f"âš™ï¸  Hop={result.hop}, Top-K={result.top_k}, Chunks={result.num_chunks}")
        print(f"â±ï¸  Latency: {result.inference_latency_ms:.1f} ms")
        
        if result.contexts:
            print(f"\nğŸ“„ Retrieved Chunks:")
            for ctx in result.contexts[:3]:  # åªæ˜¾ç¤ºå‰ 3 ä¸ª
                preview = (ctx.get("text") or "").replace("\n", " ")[:100]
                print(f"  #{ctx['rank']} [score={ctx['score']:.3f}] {preview}...")
        print("="*70)


# ============================================================
# 4. ç®€å•çš„æµ‹è¯•å‡½æ•°ï¼ˆç”¨äºæ‰‹åŠ¨æµ‹è¯•ï¼‰
# ============================================================

def test_retrieval(driver, ollama_client, question: str = "What are the symptoms of goat disease?"):
    """
    å¿«é€Ÿæµ‹è¯•ä¸åŒ hop çš„æ£€ç´¢æ•ˆæœ
    """
    engine = RetrievalEngine(driver, ollama_client)
    
    print("\nğŸ§ª Testing MultiHopRetriever with different hops...\n")
    
    for hop in [0, 1, 2, 3]:
        print(f"\n{'='*70}")
        print(f"ğŸ¯ Testing Hop-{hop} {'(Baseline - Vector Only)' if hop == 0 else ''}")
        print("="*70)
        
        result = engine.run_qa(
            question=question,
            hop=hop,
            top_k=5,
            verbose=True
        )
