# src/inspector.py
"""
Structure diagnosis (Phase 3) - å­¸è¡“ç´šå°ˆæ¥­ç‰ˆ
å°è£ç‚º GraphInspector é¡ï¼Œé¿å… import æ™‚è‡ªå‹•åŸ·è¡Œ
"""
from typing import Dict, Any, List, Optional
import sys

class GraphInspector:
    """
    åœ–è­œå“è³ªæª¢æŸ¥å“¡ (Graph Inspector)
    è² è²¬åŸ·è¡Œå­¸è¡“ç´šå®Œæ•´åº¦é©—è­‰èˆ‡å“è³ªå ±å‘Šã€‚
    """
    def __init__(self, driver):
        self.driver = driver

    def run_basic_diagnosis(self, verbose: bool = True) -> Dict[str, Any]:
        """
        åŸ·è¡ŒåŸºæœ¬çš„åœ–è­œçµ±è¨ˆè¨ºæ–·
        
        Returns:
            Dict åŒ…å«: chunks, entities, relations_total, mentions_count, 
                      relation_count, density, avg_degree
        """
        results = {}
        
        with self.driver.session() as session:
            if verbose:
                print("\n" + "="*70)
                print("ğŸ” æ­¥é©Ÿä¸€ï¼šæ¨™æº–åŒ–è¨ˆæ•¸é©—è­‰")
                print("="*70)
            
            # A. è¨ˆç®—æ‰€æœ‰é¡å‹ç¯€é»çš„ç¸½æ•¸
            total_nodes = session.run("MATCH (n) RETURN count(n) AS cnt").single()["cnt"]
            if verbose:
                print(f"A. æ‰€æœ‰é¡å‹ç¯€é»ç¸½æ•¸ï¼š{total_nodes:,}")
            
            # B. è¨ˆç®—æ‰€æœ‰ Entity ç¯€é»çš„ç¸½æ•¸
            total_entities = session.run("MATCH (e:Entity) RETURN count(e) AS cnt").single()["cnt"]
            if verbose:
                print(f"B. Entity ç¯€é»ç¸½æ•¸ï¼š{total_entities:,}")
            
            # C. è¨ˆç®—æ‰€æœ‰ Chunk ç¯€é»çš„ç¸½æ•¸
            total_chunks = session.run("MATCH (c:Chunk) RETURN count(c) AS cnt").single()["cnt"]
            if verbose:
                print(f"C. Chunk ç¯€é»ç¸½æ•¸ï¼š{total_chunks:,}")
            
            # D. è¨ˆç®—æ‰€æœ‰é—œä¿‚çš„ç¸½æ•¸ï¼ˆæ¨™æº–æ–¹æ³•ï¼‰
            total_relationships = session.run("MATCH ()-[r]-() RETURN count(r) AS cnt").single()["cnt"]
            if verbose:
                print(f"D. æ‰€æœ‰é—œä¿‚ç¸½æ•¸ï¼ˆé›™å‘è¨ˆæ•¸ï¼‰ï¼š{total_relationships:,}")
            
            # E. è¨ˆç®— RELATION é¡å‹é—œä¿‚çš„ç¸½æ•¸ï¼ˆå–®å‘è¨ˆæ•¸ï¼‰
            relation_type_count = session.run("MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt").single()["cnt"]
            if verbose:
                print(f"E. RELATION é¡å‹é—œä¿‚ç¸½æ•¸ï¼ˆå–®å‘ï¼‰ï¼š{relation_type_count:,}")
            
            # F. è¨ˆç®— MENTIONS é¡å‹é—œä¿‚çš„ç¸½æ•¸ï¼ˆå–®å‘è¨ˆæ•¸ï¼‰
            mentions_count = session.run("MATCH ()-[r:MENTIONS]->() RETURN count(r) AS cnt").single()["cnt"]
            if verbose:
                print(f"F. MENTIONS é¡å‹é—œä¿‚ç¸½æ•¸ï¼ˆå–®å‘ï¼‰ï¼š{mentions_count:,}")
            
            # è¨ˆç®—å¯†åº¦å’Œå¹³å‡åº¦æ•¸
            density = (relation_type_count / (total_entities * (total_entities - 1))) if total_entities > 1 else 0
            avg_degree = (2 * relation_type_count / total_entities) if total_entities > 0 else 0
            
            results = {
                "chunks": total_chunks,
                "entities": total_entities,
                "relations_total": relation_type_count + mentions_count,
                "mentions_count": mentions_count,
                "relation_count": relation_type_count,
                "density": density,
                "avg_degree": avg_degree,
                "total_nodes": total_nodes,
                "total_relationships_bidirectional": total_relationships
            }
            
            if verbose:
                print("\n" + "="*70)
                print("ğŸ“Š è¨ºæ–·çµæœï¼š")
                print(f"  â€¢ å¯¦é«”ç¯€é»ï¼š{total_entities:,}")
                print(f"  â€¢ èªç¾©é—œä¿‚ï¼ˆRELATIONï¼‰ï¼š{relation_type_count:,}")
                print(f"  â€¢ ä¾†æºè¿½æº¯ï¼ˆMENTIONSï¼‰ï¼š{mentions_count:,}")
                print(f"  â€¢ é—œä¿‚ç¸½è¨ˆï¼š{relation_type_count + mentions_count:,}")
                print(f"  â€¢ é—œä¿‚å¯†åº¦ï¼š{density:.4f}")
                print(f"  â€¢ å¹³å‡åº¦æ•¸ï¼š{avg_degree:.2f}")
                print(f"  â€¢ é›™å‘è¨ˆæ•¸é©—è­‰ï¼š{total_relationships:,} (æ‡‰ç‚º {2 * (relation_type_count + mentions_count):,})")
                print("="*70 + "\n")
        
        return results
    
    def run_integrity_analysis(self, verbose: bool = True) -> Dict[str, Any]:
        """
        åŸ·è¡Œé—œä¿‚å®Œæ•´æ€§åˆ†æï¼ˆæª¢æ¸¬éºå¤±é—œä¿‚ï¼‰
        """
        results = {}
        
        with self.driver.session() as session:
            if verbose:
                print("\n" + "="*70)
                print("ğŸ” æ­¥é©ŸäºŒï¼šé—œä¿‚å®Œæ•´æ€§åˆ†æ")
                print("="*70 + "\n")
            
            total_entities = session.run("MATCH (e:Entity) RETURN count(e) AS cnt").single()["cnt"]
            
            # A. æª¢æŸ¥æœ‰å¤šå°‘å¯¦é«”æ²’æœ‰ä»»ä½• RELATION
            isolated_entities = session.run("""
                MATCH (e:Entity)
                WHERE NOT (e)-[:RELATION]-()
                RETURN count(e) AS cnt
            """).single()["cnt"]
            
            if verbose:
                print(f"A. å­¤ç«‹å¯¦é«”ï¼ˆç„¡ RELATIONï¼‰ï¼š{isolated_entities:,} / {total_entities:,} ({isolated_entities/total_entities*100:.2f}%)")
            
            results = {
                "isolated_entities": isolated_entities,
                "total_entities": total_entities,
                "isolated_ratio": (isolated_entities/total_entities*100) if total_entities > 0 else 0
            }
            
            if verbose:
                print("\n" + "="*70)
        
        return results


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¿ç•™èˆŠä»£ç¢¼ä½œç‚ºåƒè€ƒï¼Œä½†å·²å°è£åœ¨é¡ä¸­ä¸æœƒè‡ªå‹•åŸ·è¡Œ
# ä»¥ä¸‹æ‰€æœ‰ä»£ç¢¼éƒ½å·²è¨»é‡‹ï¼Œé¿å… import æ™‚è‡ªå‹•åŸ·è¡Œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
# åŸå§‹è…³æœ¬ä»£ç¢¼ï¼ˆå·²åœç”¨ï¼‰
# with GRAPH_DRIVER.session() as session:
#     print("ğŸ” æ­¥é©Ÿäº”ï¼šMERGE é‚è¼¯é©—è­‰\n")
#     ...
        MATCH (e:Entity)
        WITH e.name AS entity_name, count(e) AS cnt
        WHERE cnt > 1
        RETURN entity_name, cnt
        ORDER BY cnt DESC
        LIMIT 5000
    """).data()
    
    if duplicate_entities:
        print("âŒ A. ç™¼ç¾é‡è¤‡å¯¦é«”ç¯€é»ï¼š")
        for row in duplicate_entities:
            print(f"   â€¢ {row['entity_name']}: {row['cnt']} å€‹ç¯€é»")
    else:
        print("âœ… A. ç„¡é‡è¤‡å¯¦é«”ç¯€é»ï¼ˆMERGE å»é‡æ­£ç¢ºï¼‰")
    
    # B. æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„é—œä¿‚ï¼ˆåŸºæ–¼ head + type + tailï¼‰
    duplicate_relations = session.run("""
        MATCH (h:Entity)-[r:RELATION]->(t:Entity)
        WITH h.name AS head, r.type AS rel_type, t.name AS tail, count(r) AS cnt
        WHERE cnt > 1
        RETURN head, rel_type, tail, cnt
        ORDER BY cnt DESC
        LIMIT 5
    """).data()
    
    if duplicate_relations:
        print("\nâŒ B. ç™¼ç¾é‡è¤‡é—œä¿‚ï¼š")
        for row in duplicate_relations:
            print(f"   â€¢ ({row['head']}, {row['rel_type']}, {row['tail']}): {row['cnt']} å€‹é—œä¿‚")
    else:
        print("\nâœ… B. ç„¡é‡è¤‡é—œä¿‚ï¼ˆMERGE å»é‡æ­£ç¢ºï¼‰")
    
    # C. æª¢æŸ¥ MENTIONS é—œä¿‚çš„å»é‡
    duplicate_mentions = session.run("""
        MATCH (c:Chunk)-[m:MENTIONS]->(e:Entity)
        WITH c.id AS chunk_id, e.name AS entity_name, count(m) AS cnt
        WHERE cnt > 1
        RETURN chunk_id, entity_name, cnt
        ORDER BY cnt DESC
        LIMIT 5
    """).data()
    
    if duplicate_mentions:
        print("\nâŒ C. ç™¼ç¾é‡è¤‡ MENTIONS é—œä¿‚ï¼š")
        for row in duplicate_mentions:
            print(f"   â€¢ Chunk {row['chunk_id']} â†’ {row['entity_name']}: {row['cnt']} å€‹é—œä¿‚")
    else:
        print("\nâœ… C. ç„¡é‡è¤‡ MENTIONS é—œä¿‚ï¼ˆMERGE å»é‡æ­£ç¢ºï¼‰")
    
    # D. æŠ½æ¨£æª¢æŸ¥ r.chunks å±¬æ€§çš„å®Œæ•´æ€§
    sample_relations = session.run("""
        MATCH ()-[r:RELATION]->()
        WHERE size(r.chunks) >= 2
        RETURN r.type AS relation_type, 
               size(r.chunks) AS chunk_count, 
               r.chunks AS chunks
        ORDER BY chunk_count DESC
        LIMIT 5
    """).data()
    
    print("\nâœ… D. å¤šä¾†æºé—œä¿‚æŠ½æ¨£ï¼ˆå¢é‡å¯«å…¥æ­£ç¢ºï¼‰ï¼š")
    if sample_relations:
        for row in sample_relations:
            print(f"   â€¢ {row['relation_type']}: {row['chunk_count']} å€‹ä¾†æº {row['chunks'][:3]}...")
    else:
        print("   âš ï¸  æš«ç„¡å¤šä¾†æºé—œä¿‚ï¼ˆå¯èƒ½æ‰€æœ‰é—œä¿‚éƒ½æ˜¯å–®ä¸€ä¾†æºï¼‰")
    
    print("\n" + "="*70)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å¿«é€Ÿè¨ºæ–·ï¼šå­¤ç«‹å¯¦é«”åˆ†æï¼ˆäº†è§£ç‚ºä½• 41.5% å¯¦é«”å­¤ç«‹ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with GRAPH_DRIVER.session() as session:
    print("ğŸ” å­¤ç«‹å¯¦é«”æ·±åº¦åˆ†æ\n")
    
    # A. æŠ½æ¨£å­¤ç«‹å¯¦é«”ï¼ˆå‰ 20 å€‹ï¼‰
    isolated_samples = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
        RETURN e.name AS entity_name
        LIMIT 20
    """).data()
    
    print("A. å­¤ç«‹å¯¦é«”æ¨£æœ¬ï¼ˆå‰ 20 å€‹ï¼‰ï¼š")
    for i, row in enumerate(isolated_samples, 1):
        print(f"   {i:2d}. {row['entity_name']}")
    
    # B. åˆ†æå­¤ç«‹å¯¦é«”çš„åç¨±ç‰¹å¾µ
    print("\nB. å­¤ç«‹å¯¦é«”ç‰¹å¾µåˆ†æï¼š")
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºç´”æ•¸å­—å¯¦é«”
    numeric_isolated = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
          AND e.name =~ '^[0-9]+.*'
        RETURN count(e) AS cnt
    """).single()["cnt"]
    print(f"   â€¢ ç´”æ•¸å­—é–‹é ­å¯¦é«”ï¼š{numeric_isolated:,} ({numeric_isolated/6180*100:.1f}%)")
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºçŸ­åç¨±å¯¦é«”ï¼ˆå¯èƒ½æ˜¯å–®ä½ã€ç¬¦è™Ÿï¼‰
    short_name_isolated = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
          AND size(e.name) <= 3
        RETURN count(e) AS cnt
    """).single()["cnt"]
    print(f"   â€¢ çŸ­åç¨±å¯¦é«”ï¼ˆâ‰¤3å­—ç¬¦ï¼‰ï¼š{short_name_isolated:,} ({short_name_isolated/6180*100:.1f}%)")
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå–®è©å¯¦é«”ï¼ˆå¯èƒ½ç¼ºå°‘ä¸Šä¸‹æ–‡ï¼‰
    single_word_isolated = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
          AND NOT e.name CONTAINS ' '
        RETURN count(e) AS cnt
    """).single()["cnt"]
    print(f"   â€¢ å–®è©å¯¦é«”ï¼ˆç„¡ç©ºæ ¼ï¼‰ï¼š{single_word_isolated:,} ({single_word_isolated/6180*100:.1f}%)")
    
    # C. æª¢æŸ¥å­¤ç«‹å¯¦é«”æ˜¯å¦è¢« MENTIONSï¼ˆç¢ºèªæ•¸æ“šä¸€è‡´æ€§ï¼‰
    isolated_with_mentions = session.run("""
        MATCH (c:Chunk)-[:MENTIONS]->(e:Entity)
        WHERE NOT (e)-[:RELATION]-()
        RETURN count(DISTINCT e) AS cnt
    """).single()["cnt"]
    print(f"\nC. å­¤ç«‹ä½†è¢« MENTIONS çš„å¯¦é«”ï¼š{isolated_with_mentions:,} / {6180:,}")
    print(f"   âš ï¸  æ•¸æ“šä¸€è‡´æ€§ï¼š{isolated_with_mentions == 6180 and 'âœ… å®Œå…¨ä¸€è‡´' or 'âŒ å­˜åœ¨ä¸ä¸€è‡´'}")
    
    # D. æª¢æŸ¥å­¤ç«‹å¯¦é«”çš„ä¾†æºåˆ†ä½ˆ
    isolated_by_chunk = session.run("""
        MATCH (c:Chunk)-[:MENTIONS]->(e:Entity)
        WHERE NOT (e)-[:RELATION]-()
        WITH c.id AS chunk_id, count(DISTINCT e) AS isolated_count
        RETURN chunk_id, isolated_count
        ORDER BY isolated_count DESC
        LIMIT 5
    """).data()
    
    print(f"\nD. å­¤ç«‹å¯¦é«”æœ€å¤šçš„ Chunksï¼ˆå‰ 5 å€‹ï¼‰ï¼š")
    for row in isolated_by_chunk:
        print(f"   â€¢ {row['chunk_id']}: {row['isolated_count']} å€‹å­¤ç«‹å¯¦é«”")
    
    print("\n" + "="*70)
    print("ğŸ’¡ å»ºè­°ï¼š")
    print("   â€¢ å¦‚æœå­¤ç«‹å¯¦é«”å¤šç‚ºæ•¸å­—/å–®ä½/çŸ­ç¬¦è™Ÿ â†’ å¯æ¸…ç†")
    print("   â€¢ å¦‚æœå­¤ç«‹å¯¦é«”ç‚ºæœ‰æ„ç¾©æ¦‚å¿µ â†’ éœ€å¢å¼· LLM æå–")
    print("="*70)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç·Šæ€¥é©—è­‰ï¼šæª¢æŸ¥ã€Œå¹½éˆå¯¦é«”ã€çš„çœŸå¯¦ç‹€æ…‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with GRAPH_DRIVER.session() as session:
    print("ğŸš¨ å¹½éˆå¯¦é«”é©—è­‰\n")
    
    # 1. æª¢æŸ¥çœŸæ­£çš„å­¤å…’å¯¦é«”ï¼ˆç„¡ä»»ä½•é€£æ¥ï¼‰
    truly_orphan = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
          AND NOT ()-[:MENTIONS]->(e)
        RETURN count(e) AS cnt
    """).single()["cnt"]
    print(f"1. çœŸæ­£çš„å­¤å…’å¯¦é«”ï¼ˆç„¡ä»»ä½•é€£æ¥ï¼‰ï¼š{truly_orphan:,}")
    
    # 2. æª¢æŸ¥æœ‰ MENTIONS çš„å¯¦é«”ç¸½æ•¸
    mentioned_entities = session.run("""
        MATCH ()-[:MENTIONS]->(e:Entity)
        RETURN count(DISTINCT e) AS cnt
    """).single()["cnt"]
    print(f"2. è¢« MENTIONS çš„å¯¦é«”ç¸½æ•¸ï¼š{mentioned_entities:,}")
    
    # 3. æª¢æŸ¥æœ‰ RELATION çš„å¯¦é«”ç¸½æ•¸
    relation_entities = session.run("""
        MATCH (e:Entity)-[:RELATION]-()
        RETURN count(DISTINCT e) AS cnt
    """).single()["cnt"]
    print(f"3. æœ‰ RELATION çš„å¯¦é«”ç¸½æ•¸ï¼š{relation_entities:,}")
    
    # 4. è¨ˆç®—è¦†è“‹æƒ…æ³
    total_entities = 14880
    covered = mentioned_entities + relation_entities - truly_orphan
    print(f"\n4. å¯¦é«”è¦†è“‹åˆ†æï¼š")
    print(f"   â€¢ ç¸½å¯¦é«”ï¼š{total_entities:,}")
    print(f"   â€¢ è¢« MENTIONSï¼š{mentioned_entities:,} ({mentioned_entities/total_entities*100:.1f}%)")
    print(f"   â€¢ æœ‰ RELATIONï¼š{relation_entities:,} ({relation_entities/total_entities*100:.1f}%)")
    print(f"   â€¢ çœŸæ­£å­¤å…’ï¼š{truly_orphan:,} ({truly_orphan/total_entities*100:.1f}%)")
    
    # 5. æŠ½æ¨£æª¢æŸ¥å¹¾å€‹å­¤ç«‹å¯¦é«”çš„å¯¦éš›ç‹€æ…‹
    print(f"\n5. æŠ½æ¨£å­¤ç«‹å¯¦é«”çš„é€£æ¥ç‹€æ…‹ï¼ˆå‰ 5 å€‹ï¼‰ï¼š")
    sample_isolated = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
        WITH e LIMIT 5
        OPTIONAL MATCH (e)-[r]-()
        RETURN e.name AS entity_name, 
               count(r) AS total_connections,
               collect(DISTINCT type(r)) AS connection_types
    """).data()
    
    for row in sample_isolated:
        print(f"   â€¢ {row['entity_name']}:")
        print(f"     - ç¸½é€£æ¥æ•¸ï¼š{row['total_connections']}")
        print(f"     - é€£æ¥é¡å‹ï¼š{row['connection_types']}")
    
    # 6. æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§ï¼šMENTIONS æ•¸é‡ vs é æœŸ
    print(f"\n6. MENTIONS æ•¸é‡é©—è­‰ï¼š")
    mentions_count = session.run("""
        MATCH ()-[m:MENTIONS]->()
        RETURN count(m) AS cnt
    """).single()["cnt"]
    print(f"   â€¢ MENTIONS é—œä¿‚ç¸½æ•¸ï¼š{mentions_count:,}")
    print(f"   â€¢ å¹³å‡æ¯ Chunkï¼š{mentions_count / 25:.1f} å€‹ MENTIONS")
    
    # 7. æª¢æŸ¥æ˜¯å¦æœ‰ dataset å±¬æ€§ä¸åŒ¹é…çš„æƒ…æ³
    different_dataset = session.run("""
        MATCH (e:Entity)
        WHERE e.dataset IS NOT NULL 
          AND e.dataset <> $dataset
        RETURN count(e) AS cnt
    """, dataset=DATASET_ID).single()["cnt"]
    print(f"\n7. ä¸åŒ dataset çš„å¯¦é«”ï¼š{different_dataset:,}")
    
    print("\n" + "="*70)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åŸ·è¡Œæ¸…ç†ï¼šåˆªé™¤å­¤å…’å¯¦é«”ï¼ˆç„¡ä»»ä½•é€£æ¥çš„å¯¦é«”ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("âš ï¸  å³å°‡åˆªé™¤å­¤å…’å¯¦é«”ï¼ˆç„¡ä»»ä½• MENTIONS æˆ– RELATIONï¼‰\n")
print("é€™äº›å¯¦é«”çš„æ¨£æœ¬ï¼š")
print("  â€¢ è‰¯è´¨èŠ»æ–™ã€è„‚è´¨å †ç§¯ã€å¦Šå¨ æ¯’è¡€ç—‡é£é™©ã€ç©€ç‰©ã€é’è‰ä¹¾è‰...")
print("\né€™äº›å¯¦é«”å¯èƒ½ä¾†è‡ªï¼š")
print("  1. ä¹‹å‰é‹è¡Œçš„èˆŠæ•¸æ“šæ®˜ç•™")
print("  2. æ¸¬è©¦éšæ®µå‰µå»ºçš„å¯¦é«”")
print("  3. å·²è¢«ç§»é™¤çš„æ–‡æœ¬ç‰‡æ®µ\n")

user_confirm = input("ç¢ºèªåŸ·è¡Œæ¸…ç†ï¼Ÿ(yes/no): ")

if user_confirm.lower() in ['yes', 'y']:
    with GRAPH_DRIVER.session() as session:
        result = session.run("""
            MATCH (e:Entity)
            WHERE NOT (e)-[:RELATION]-()
              AND NOT ()-[:MENTIONS]->(e)
            DETACH DELETE e
            RETURN count(e) AS deleted
        """)
        deleted_count = result.single()["deleted"]
        print(f"\nâœ… æˆåŠŸåˆªé™¤ {deleted_count:,} å€‹å­¤å…’å¯¦é«”")
        
        # é©—è­‰æ¸…ç†æ•ˆæœ
        remaining_entities = session.run("MATCH (e:Entity) RETURN count(e) AS cnt").single()["cnt"]
        remaining_relations = session.run("MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt").single()["cnt"]
        
        print(f"\nğŸ“Š æ¸…ç†å¾Œç‹€æ…‹ï¼š")
        print(f"  â€¢ å¯¦é«”ç¯€é»ï¼š{remaining_entities:,}")
        print(f"  â€¢ èªç¾©é—œä¿‚ï¼š{remaining_relations:,}")
        print(f"  â€¢ é—œä¿‚å¯†åº¦ï¼š{remaining_relations/remaining_entities:.3f}")
        print(f"\nâœ… åœ–è­œå·²æ·¨åŒ–ï¼æ‰€æœ‰å¯¦é«”éƒ½æœ‰é€£æ¥ã€‚")
else:
    print("\nâŒ æ¸…ç†å·²å–æ¶ˆ")
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åœ–è­œå®Œæ•´åº¦èˆ‡è³ªé‡æœ€çµ‚æª¢é©—
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with GRAPH_DRIVER.session() as session:
    print("ğŸ” åœ–è­œå®Œæ•´åº¦èˆ‡è³ªé‡æœ€çµ‚æª¢é©—å ±å‘Š")
    print("="*70)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¤æŒ‡æ¨™
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ“Š ä¸€ã€åŸºç¤æŒ‡æ¨™")
    print("-"*70)
    
    entity_count = session.run("MATCH (e:Entity) RETURN count(e) AS cnt").single()["cnt"]
    relation_count = session.run("MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt").single()["cnt"]
    chunk_count = session.run(f"MATCH (c:Chunk {{dataset: '{DATASET_ID}'}}) RETURN count(c) AS cnt").single()["cnt"]
    mentions_count = session.run("MATCH ()-[m:MENTIONS]->() RETURN count(m) AS cnt").single()["cnt"]
    
    density = relation_count / entity_count if entity_count > 0 else 0.0
    
    avg_degree = session.run("""
        MATCH (e:Entity)
        OPTIONAL MATCH (e)-[r:RELATION]-()
        WITH e, count(r) AS degree
        RETURN avg(degree) AS avg_degree
    """).single()["avg_degree"] or 0.0
    
    print(f"  â€¢ å¯¦é«”ç¯€é»æ•¸ï¼š{entity_count:,}")
    print(f"  â€¢ èªç¾©é—œä¿‚æ•¸ï¼š{relation_count:,}")
    print(f"  â€¢ æ–‡æœ¬ Chunksï¼š{chunk_count:,}")
    print(f"  â€¢ MENTIONS é€£æ¥ï¼š{mentions_count:,}")
    print(f"  â€¢ é—œä¿‚å¯†åº¦ï¼š{density:.3f} {'âœ… å„ªç§€' if density >= TARGET_DENSITY else 'âš ï¸ å¾…å„ªåŒ–'}")
    print(f"  â€¢ å¹³å‡åº¦æ•¸ï¼š{avg_degree:.2f} {'âœ… å„ªç§€' if avg_degree >= TARGET_AVG_DEGREE else 'âš ï¸ å¾…å„ªåŒ–'}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¬¬äºŒéƒ¨åˆ†ï¼šé€£æ¥è³ªé‡
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ”— äºŒã€é€£æ¥è³ªé‡åˆ†æ")
    print("-"*70)
    
    # 1. å­¤ç«‹å¯¦é«”æª¢æ¸¬
    isolated_entities = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
        RETURN count(e) AS cnt
    """).single()["cnt"]
    
    isolated_percent = (isolated_entities / entity_count * 100) if entity_count > 0 else 0
    print(f"  1. å­¤ç«‹å¯¦é«”ï¼š{isolated_entities:,} ({isolated_percent:.1f}%) {'âœ… å„ªç§€' if isolated_percent == 0 else 'âš ï¸ éœ€æ³¨æ„' if isolated_percent < 5 else 'âŒ éœ€æ”¹é€²'}")
    
    # 2. å¼±é€£æ¥å¯¦é«”ï¼ˆåº¦æ•¸ = 1ï¼‰
    weak_entities = session.run("""
        MATCH (e:Entity)-[r:RELATION]-()
        WITH e, count(r) AS degree
        WHERE degree = 1
        RETURN count(e) AS cnt
    """).single()["cnt"]
    
    weak_percent = (weak_entities / entity_count * 100) if entity_count > 0 else 0
    print(f"  2. å¼±é€£æ¥å¯¦é«”ï¼ˆåº¦æ•¸=1ï¼‰ï¼š{weak_entities:,} ({weak_percent:.1f}%) {'âœ… å„ªç§€' if weak_percent < 20 else 'âš ï¸ éœ€æ³¨æ„'}")
    
    # 3. å¼·é€£æ¥å¯¦é«”ï¼ˆåº¦æ•¸ â‰¥ 5ï¼‰
    strong_entities = session.run("""
        MATCH (e:Entity)-[r:RELATION]-()
        WITH e, count(r) AS degree
        WHERE degree >= 5
        RETURN count(e) AS cnt
    """).single()["cnt"]
    
    strong_percent = (strong_entities / entity_count * 100) if entity_count > 0 else 0
    print(f"  3. å¼·é€£æ¥å¯¦é«”ï¼ˆåº¦æ•¸â‰¥5ï¼‰ï¼š{strong_entities:,} ({strong_percent:.1f}%) {'âœ… å„ªç§€' if strong_percent >= 10 else 'âš ï¸ å¾…å„ªåŒ–'}")
    
    # 4. å¤šä¾†æºé—œä¿‚ï¼ˆè·¨ Chunk é—œä¿‚ï¼‰
    multi_source_relations = session.run("""
        MATCH ()-[r:RELATION]->()
        WHERE size(r.chunks) >= 2
        RETURN count(r) AS cnt
    """).single()["cnt"]
    
    multi_source_percent = (multi_source_relations / relation_count * 100) if relation_count > 0 else 0
    print(f"  4. å¤šä¾†æºé—œä¿‚ï¼ˆâ‰¥2 Chunksï¼‰ï¼š{multi_source_relations:,} ({multi_source_percent:.1f}%) {'âœ… å„ªç§€' if multi_source_percent >= 20 else 'âš ï¸ å¾…å„ªåŒ–'}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šé—œä¿‚å¼·åŒ–æ•ˆæœ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nâš¡ ä¸‰ã€é—œä¿‚å¼·åŒ–æ•ˆæœ")
    print("-"*70)
    
    enhanced_relations = session.run("""
        MATCH ()-[r:RELATION]->()
        WHERE r.enhanced = true
        RETURN count(r) AS cnt
    """).single()["cnt"]
    
    enhanced_percent = (enhanced_relations / relation_count * 100) if relation_count > 0 else 0
    print(f"  â€¢ å¼·åŒ–æ–°å¢é—œä¿‚ï¼š{enhanced_relations:,} ({enhanced_percent:.1f}%)")
    print(f"  â€¢ åŸå§‹é—œä¿‚ï¼š{relation_count - enhanced_relations:,} ({100-enhanced_percent:.1f}%)")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¬¬å››éƒ¨åˆ†ï¼šé—œä¿‚é¡å‹å¤šæ¨£æ€§
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ¨ å››ã€é—œä¿‚é¡å‹å¤šæ¨£æ€§")
    print("-"*70)
    
    relation_type_count = session.run("""
        MATCH ()-[r:RELATION]->()
        RETURN count(DISTINCT r.type) AS cnt
    """).single()["cnt"]
    
    print(f"  â€¢ é—œä¿‚é¡å‹ç¸½æ•¸ï¼š{relation_type_count}")
    
    relation_types = session.run("""
        MATCH ()-[r:RELATION]->()
        RETURN r.type AS relation_type, count(r) AS cnt
        ORDER BY cnt DESC
        LIMIT 10
    """).data()
    
    print(f"  â€¢ å‰ 10 ç¨®é—œä¿‚é¡å‹ï¼š")
    for idx, row in enumerate(relation_types, 1):
        percent = (row['cnt'] / relation_count * 100) if relation_count > 0 else 0
        print(f"    {idx:2d}. {row['relation_type']:<30s} {row['cnt']:>6,} ({percent:>5.1f}%)")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¬¬äº”éƒ¨åˆ†ï¼šæ ¸å¿ƒæ¨ç´ç¯€é»
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸŒŸ äº”ã€æ ¸å¿ƒæ¨ç´ç¯€é»ï¼ˆTop 10ï¼‰")
    print("-"*70)
    
    hub_entities = session.run("""
        MATCH (e:Entity)-[r:RELATION]-()
        WITH e, count(r) AS degree
        WHERE degree >= 5
        RETURN e.name AS entity_name, degree
        ORDER BY degree DESC
        LIMIT 10
    """).data()
    
    if hub_entities:
        for idx, row in enumerate(hub_entities, 1):
            print(f"  {idx:2d}. {row['entity_name']:<40s} {row['degree']:>3} å€‹é—œä¿‚")
    else:
        print("  âš ï¸ æœªç™¼ç¾åº¦æ•¸ â‰¥ 5 çš„æ ¸å¿ƒç¯€é»")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¬¬å…­éƒ¨åˆ†ï¼šè¦†è“‹ç‡åˆ†æ
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ“ å…­ã€æ–‡æœ¬è¦†è“‹ç‡åˆ†æ")
    print("-"*70)
    
    # æœ‰å¯¦é«”çš„ Chunks
    covered_chunks = session.run(f"""
        MATCH (c:Chunk {{dataset: '{DATASET_ID}'}})-[:MENTIONS]->()
        RETURN count(DISTINCT c) AS cnt
    """).single()["cnt"]
    
    coverage_percent = (covered_chunks / chunk_count * 100) if chunk_count > 0 else 0
    print(f"  â€¢ å·²è¦†è“‹ Chunksï¼š{covered_chunks} / {chunk_count} ({coverage_percent:.1f}%) {'âœ… å„ªç§€' if coverage_percent >= 95 else 'âš ï¸ å¾…å„ªåŒ–'}")
    
    # å¹³å‡æ¯å€‹ Chunk çš„å¯¦é«”æ•¸
    avg_entities_per_chunk = session.run(f"""
        MATCH (c:Chunk {{dataset: '{DATASET_ID}'}})-[:MENTIONS]->(e:Entity)
        WITH c, count(DISTINCT e) AS entity_count
        RETURN avg(entity_count) AS avg_cnt
    """).single()["avg_cnt"] or 0
    
    print(f"  â€¢ å¹³å‡æ¯ Chunk å¯¦é«”æ•¸ï¼š{avg_entities_per_chunk:.1f}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ç¬¬ä¸ƒéƒ¨åˆ†ï¼šè³ªé‡å•é¡Œæª¢æ¸¬
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nâš ï¸ ä¸ƒã€æ½›åœ¨è³ªé‡å•é¡Œæª¢æ¸¬")
    print("-"*70)
    
    issues_found = []
    
    # æª¢æ¸¬ 1ï¼šè‡ªç’°é—œä¿‚
    self_loops = session.run("""
        MATCH (e:Entity)-[r:RELATION]->(e)
        RETURN count(r) AS cnt
    """).single()["cnt"]
    if self_loops > 0:
        issues_found.append(f"ç™¼ç¾ {self_loops} å€‹è‡ªç’°é—œä¿‚")
    
    # æª¢æ¸¬ 2ï¼šç©ºå¯¦é«”åç¨±
    empty_entities = session.run("""
        MATCH (e:Entity)
        WHERE e.name IS NULL OR trim(e.name) = ''
        RETURN count(e) AS cnt
    """).single()["cnt"]
    if empty_entities > 0:
        issues_found.append(f"ç™¼ç¾ {empty_entities} å€‹ç©ºå¯¦é«”åç¨±")
    
    # æª¢æ¸¬ 3ï¼šé‡è¤‡é—œä¿‚ï¼ˆç›¸åŒé ­å°¾å’Œé¡å‹ï¼‰
    duplicate_relations = session.run("""
        MATCH (h:Entity)-[r:RELATION]->(t:Entity)
        WITH h, t, r.type AS rel_type, count(r) AS cnt
        WHERE cnt > 1
        RETURN count(*) AS dup_cnt
    """).single()["dup_cnt"]
    if duplicate_relations > 0:
        issues_found.append(f"ç™¼ç¾ {duplicate_relations} çµ„é‡è¤‡é—œä¿‚")
    
    # æª¢æ¸¬ 4ï¼šè¶…é•·å¯¦é«”åç¨±ï¼ˆå¯èƒ½æ˜¯å¥å­ç‰‡æ®µï¼‰
    long_entities = session.run("""
        MATCH (e:Entity)
        WHERE size(e.name) > 50
        RETURN count(e) AS cnt
    """).single()["cnt"]
    if long_entities > 0:
        issues_found.append(f"ç™¼ç¾ {long_entities} å€‹è¶…é•·å¯¦é«”åç¨±ï¼ˆ>50å­—å…ƒï¼‰")
    
    if issues_found:
        for issue in issues_found:
            print(f"  âš ï¸ {issue}")
    else:
        print("  âœ… æœªç™¼ç¾æ˜é¡¯è³ªé‡å•é¡Œ")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # æœ€çµ‚è©•ç´š
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\n{'='*70}")
    print(f"ğŸ† æœ€çµ‚è©•ç´š")
    print(f"{'='*70}")
    
    score = 0
    max_score = 7
    
    # è©•åˆ†é …ç›®
    if density >= TARGET_DENSITY:
        score += 1
        density_status = "âœ…"
    else:
        density_status = "âŒ"
    
    if avg_degree >= TARGET_AVG_DEGREE:
        score += 1
        degree_status = "âœ…"
    else:
        degree_status = "âŒ"
    
    if isolated_percent < 5:
        score += 1
        isolated_status = "âœ…"
    else:
        isolated_status = "âŒ"
    
    if weak_percent < 30:
        score += 1
        weak_status = "âœ…"
    else:
        weak_status = "âŒ"
    
    if strong_percent >= 10:
        score += 1
        strong_status = "âœ…"
    else:
        strong_status = "âŒ"
    
    if coverage_percent >= 95:
        score += 1
        coverage_status = "âœ…"
    else:
        coverage_status = "âŒ"
    
    if len(issues_found) == 0:
        score += 1
        quality_status = "âœ…"
    else:
        quality_status = "âŒ"
    
    print(f"  {density_status} é—œä¿‚å¯†åº¦ â‰¥ {TARGET_DENSITY}ï¼š{density:.3f}")
    print(f"  {degree_status} å¹³å‡åº¦æ•¸ â‰¥ {TARGET_AVG_DEGREE}ï¼š{avg_degree:.2f}")
    print(f"  {isolated_status} å­¤ç«‹å¯¦é«” < 5%ï¼š{isolated_percent:.1f}%")
    print(f"  {weak_status} å¼±é€£æ¥å¯¦é«” < 30%ï¼š{weak_percent:.1f}%")
    print(f"  {strong_status} å¼·é€£æ¥å¯¦é«” â‰¥ 10%ï¼š{strong_percent:.1f}%")
    print(f"  {coverage_status} æ–‡æœ¬è¦†è“‹ç‡ â‰¥ 95%ï¼š{coverage_percent:.1f}%")
    print(f"  {quality_status} ç„¡è³ªé‡å•é¡Œï¼š{'æ˜¯' if len(issues_found) == 0 else 'å¦'}")
    
    print(f"\n  ç¸½åˆ†ï¼š{score}/{max_score}")
    
    if score == max_score:
        grade = "A+ å“è¶Š"
    elif score >= 6:
        grade = "A å„ªç§€"
    elif score >= 5:
        grade = "B è‰¯å¥½"
    elif score >= 4:
        grade = "C åŠæ ¼"
    else:
        grade = "D å¾…æ”¹é€²"
    
    print(f"  ç­‰ç´šï¼š{grade}")
    print(f"{'='*70}")

print("\nâœ… åœ–è­œå®Œæ•´åº¦èˆ‡è³ªé‡æª¢é©—å®Œæˆï¼")
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# åœ–è­œè³ªé‡å•é¡Œè‡ªå‹•ä¿®æ­£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("ğŸ”§ é–‹å§‹è‡ªå‹•ä¿®æ­£åœ–è­œè³ªé‡å•é¡Œ...")
print("="*70)

fix_summary = {
    'self_loops_removed': 0,
    'long_entities_truncated': 0,
    'duplicate_relations_merged': 0,
    'empty_entities_removed': 0
}

with GRAPH_DRIVER.session() as session:
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¿®æ­£ 1ï¼šç§»é™¤è‡ªç’°é—œä¿‚ï¼ˆå¯¦é«”æŒ‡å‘è‡ªå·±ï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print("\nğŸ” ä¿®æ­£ 1ï¼šç§»é™¤è‡ªç’°é—œä¿‚")
    print("-"*70)
    
    self_loops_count = session.run("""
        MATCH (e:Entity)-[r:RELATION]->(e)
        RETURN count(r) AS cnt
    """).single()["cnt"]
    
    if self_loops_count > 0:
        print(f"  ç™¼ç¾ {self_loops_count} å€‹è‡ªç’°é—œä¿‚ï¼Œæ­£åœ¨ç§»é™¤...")
        result = session.run("""
            MATCH (e:Entity)-[r:RELATION]->(e)
            DELETE r
            RETURN count(r) AS deleted
        """).single()
        fix_summary['self_loops_removed'] = result['deleted']
        print(f"  âœ… å·²ç§»é™¤ {result['deleted']} å€‹è‡ªç’°é—œä¿‚")
    else:
        print("  âœ… æœªç™¼ç¾è‡ªç’°é—œä¿‚")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¿®æ­£ 2ï¼šè™•ç†è¶…é•·å¯¦é«”åç¨±ï¼ˆ>50å­—å…ƒï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ” ä¿®æ­£ 2ï¼šè™•ç†è¶…é•·å¯¦é«”åç¨±")
    print("-"*70)
    
    long_entities_count = session.run("""
        MATCH (e:Entity)
        WHERE size(e.name) > 50
        RETURN count(e) AS cnt
    """).single()["cnt"]
    
    if long_entities_count > 0:
        print(f"  ç™¼ç¾ {long_entities_count} å€‹è¶…é•·å¯¦é«”åç¨±")
        
        # ç²å–æ¨£æœ¬æª¢æŸ¥
        samples = session.run("""
            MATCH (e:Entity)
            WHERE size(e.name) > 50
            RETURN e.name AS name, size(e.name) AS length
            ORDER BY length DESC
            LIMIT 100
        """).data()
        
        print(f"  æ¨£æœ¬ï¼ˆå‰5å€‹ï¼‰ï¼š")
        for s in samples:
            display_name = s['name'][:60] + "..." if len(s['name']) > 60 else s['name']
            print(f"    â€¢ {display_name} (é•·åº¦: {s['length']})")
        
        # åˆ†æè¶…é•·å¯¦é«”çš„é€£æ¥åº¦
        connectivity_stats = session.run("""
            MATCH (e:Entity)
            WHERE size(e.name) > 50
            OPTIONAL MATCH (e)-[r:RELATION]-()
            WITH e, count(r) AS degree
            RETURN 
                count(e) AS total,
                sum(CASE WHEN degree = 0 THEN 1 ELSE 0 END) AS isolated,
                sum(CASE WHEN degree = 1 THEN 1 ELSE 0 END) AS weak,
                avg(degree) AS avg_degree
        """).single()
        
        print(f"\n  é€£æ¥åº¦åˆ†æï¼š")
        print(f"    â€¢ å­¤ç«‹ï¼ˆåº¦æ•¸=0ï¼‰ï¼š{connectivity_stats['isolated']}/{connectivity_stats['total']}")
        print(f"    â€¢ å¼±é€£æ¥ï¼ˆåº¦æ•¸=1ï¼‰ï¼š{connectivity_stats['weak']}/{connectivity_stats['total']}")
        print(f"    â€¢ å¹³å‡åº¦æ•¸ï¼š{connectivity_stats['avg_degree']:.2f}")
        
        # ç­–ç•¥ï¼šè‡ªå‹•ç§»é™¤é€™äº›å¥å­ç‰‡æ®µå¯¦é«”ï¼ˆå› ç‚ºå®ƒå€‘é€šå¸¸æ˜¯æŠ½å–éŒ¯èª¤ï¼‰
        print(f"\n  ğŸ’¡ å»ºè­°ï¼šé€™äº›è¶…é•·å¯¦é«”é€šå¸¸æ˜¯å¥å­ç‰‡æ®µï¼Œæœƒé™ä½åœ–è­œè³ªé‡")
        print(f"     â†’ è‡ªå‹•ç§»é™¤é€™äº›å¯¦é«”åŠå…¶é—œä¿‚...")
        
        result = session.run("""
            MATCH (e:Entity)
            WHERE size(e.name) > 50
            DETACH DELETE e
            RETURN count(e) AS deleted
        """).single()
        fix_summary['long_entities_truncated'] = result['deleted']
        print(f"  âœ… å·²ç§»é™¤ {result['deleted']} å€‹è¶…é•·å¯¦é«”åŠå…¶é—œä¿‚")
    else:
        print("  âœ… æœªç™¼ç¾è¶…é•·å¯¦é«”åç¨±")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¿®æ­£ 3ï¼šåˆä½µé‡è¤‡é—œä¿‚ï¼ˆç›¸åŒé ­å°¾å’Œé¡å‹ï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ” ä¿®æ­£ 3ï¼šåˆä½µé‡è¤‡é—œä¿‚")
    print("-"*70)
    
    duplicate_groups = session.run("""
        MATCH (h:Entity)-[r:RELATION]->(t:Entity)
        WITH h, t, r.type AS rel_type, collect(r) AS rels
        WHERE size(rels) > 1
        RETURN count(*) AS dup_groups, sum(size(rels) - 1) AS extra_rels
    """).single()
    
    dup_groups = duplicate_groups['dup_groups'] or 0
    extra_rels = duplicate_groups['extra_rels'] or 0
    
    if dup_groups > 0:
        print(f"  ç™¼ç¾ {dup_groups} çµ„é‡è¤‡é—œä¿‚ï¼ˆå…± {extra_rels} å€‹å¤šé¤˜é—œä¿‚ï¼‰")
        print(f"  æ­£åœ¨åˆä½µé‡è¤‡é—œä¿‚ï¼ˆä¿ç•™ç¬¬ä¸€å€‹ï¼Œåˆä½µ chunks å±¬æ€§ï¼‰...")
        
        # åˆä½µç­–ç•¥ï¼šä¿ç•™ç¬¬ä¸€å€‹é—œä¿‚ï¼Œå°‡å…¶ä»–é—œä¿‚çš„ chunks åˆä½µé€²å»
        result = session.run("""
            MATCH (h:Entity)-[r:RELATION]->(t:Entity)
            WITH h, t, r.type AS rel_type, collect(r) AS rels
            WHERE size(rels) > 1
            WITH h, t, rel_type, rels[0] AS keep, rels[1..] AS remove
            UNWIND remove AS del_rel
            WITH h, t, rel_type, keep, del_rel, 
                 COALESCE(keep.chunks, []) + COALESCE(del_rel.chunks, []) AS merged_chunks
            SET keep.chunks = merged_chunks
            DELETE del_rel
            RETURN count(del_rel) AS merged
        """).single()
        
        fix_summary['duplicate_relations_merged'] = result['merged']
        print(f"  âœ… å·²åˆä½µ {result['merged']} å€‹é‡è¤‡é—œä¿‚")
    else:
        print("  âœ… æœªç™¼ç¾é‡è¤‡é—œä¿‚")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ä¿®æ­£ 4ï¼šç§»é™¤ç©ºå¯¦é«”åç¨±
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ” ä¿®æ­£ 4ï¼šç§»é™¤ç©ºå¯¦é«”åç¨±")
    print("-"*70)
    
    empty_entities_count = session.run("""
        MATCH (e:Entity)
        WHERE e.name IS NULL OR trim(e.name) = ''
        RETURN count(e) AS cnt
    """).single()["cnt"]
    
    if empty_entities_count > 0:
        print(f"  ç™¼ç¾ {empty_entities_count} å€‹ç©ºå¯¦é«”åç¨±ï¼Œæ­£åœ¨ç§»é™¤...")
        result = session.run("""
            MATCH (e:Entity)
            WHERE e.name IS NULL OR trim(e.name) = ''
            DETACH DELETE e
            RETURN count(e) AS deleted
        """).single()
        fix_summary['empty_entities_removed'] = result['deleted']
        print(f"  âœ… å·²ç§»é™¤ {result['deleted']} å€‹ç©ºå¯¦é«”")
    else:
        print("  âœ… æœªç™¼ç¾ç©ºå¯¦é«”åç¨±")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¿®æ­£æ‘˜è¦
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print(f"\n{'='*70}")
print(f"ğŸ“‹ è³ªé‡ä¿®æ­£æ‘˜è¦")
print(f"{'='*70}")
print(f"  â€¢ ç§»é™¤è‡ªç’°é—œä¿‚ï¼š{fix_summary['self_loops_removed']}")
print(f"  â€¢ ç§»é™¤è¶…é•·å¯¦é«”ï¼š{fix_summary['long_entities_truncated']}")
print(f"  â€¢ åˆä½µé‡è¤‡é—œä¿‚ï¼š{fix_summary['duplicate_relations_merged']}")
print(f"  â€¢ ç§»é™¤ç©ºå¯¦é«”ï¼š{fix_summary['empty_entities_removed']}")

total_fixes = sum(fix_summary.values())
print(f"\n  ç¸½è¨ˆä¿®æ­£ï¼š{total_fixes} å€‹å•é¡Œ")
print(f"{'='*70}")

if total_fixes > 0:
    print("\nğŸ’¡ å»ºè­°ï¼šé‡æ–°åŸ·è¡Œåœ–è­œè³ªé‡æª¢é©—ä»¥ç¢ºèªä¿®æ­£æ•ˆæœ")
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ” å­¤ç«‹å¯¦é«”è¨ºæ–·åˆ†æ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("ğŸ” å­¤ç«‹å¯¦é«”æ·±åº¦è¨ºæ–·")
print("="*70)

with GRAPH_DRIVER.session() as session:
    # 1. ç²å–å­¤ç«‹å¯¦é«”æ¨£æœ¬åŠå…¶ä¾†æº
    isolated_samples = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
        OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(e)
        WITH e, collect(DISTINCT c.id) AS source_chunks
        RETURN 
            e.name AS entity,
            size(source_chunks) AS mention_count,
            source_chunks[0] AS sample_chunk
        ORDER BY mention_count DESC
        LIMIT 30
    """).data()
    
    print(f"\nğŸ“‹ å­¤ç«‹å¯¦é«”æ¨£æœ¬ï¼ˆå‰ 30 å€‹ï¼ŒæŒ‰æåŠæ¬¡æ•¸æ’åºï¼‰ï¼š")
    print("-"*70)
    
    for idx, item in enumerate(isolated_samples, 1):
        entity = item['entity']
        mentions = item['mention_count']
        chunk_id = item['sample_chunk'] or "æœªæ‰¾åˆ°ä¾†æº"
        
        # åˆ†é¡åˆ†æ
        entity_type = ""
        if len(entity) < 3:
            entity_type = "[éçŸ­]"
        elif entity.replace('_', '').replace('-', '').isdigit():
            entity_type = "[ç´”æ•¸å­—]"
        elif entity.lower() in ['it', 'this', 'that', 'they', 'these']:
            entity_type = "[ä»£è©]"
        elif any(char.isdigit() for char in entity) and any(char.isalpha() for char in entity):
            entity_type = "[æ•¸å€¼+å–®ä½]"
        else:
            entity_type = "[æ­£å¸¸]"
        
        print(f"  {idx:2d}. {entity_type:12s} {entity[:40]:40s} (æåŠ: {mentions}, Chunk: {chunk_id})")
    
    # 2. å­¤ç«‹å¯¦é«”é¡å‹çµ±è¨ˆ
    print(f"\nğŸ“Š å­¤ç«‹å¯¦é«”é¡å‹åˆ†æï¼š")
    print("-"*70)
    
    isolated_stats = session.run("""
        MATCH (e:Entity)
        WHERE NOT (e)-[:RELATION]-()
        OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(e)
        WITH e, count(DISTINCT c) AS mentions
        RETURN 
            count(e) AS total_isolated,
            sum(CASE WHEN mentions = 0 THEN 1 ELSE 0 END) AS no_mentions,
            sum(CASE WHEN mentions = 1 THEN 1 ELSE 0 END) AS single_mention,
            sum(CASE WHEN mentions >= 2 THEN 1 ELSE 0 END) AS multiple_mentions,
            sum(CASE WHEN size(e.name) < 3 THEN 1 ELSE 0 END) AS too_short,
            sum(CASE WHEN size(e.name) > 40 THEN 1 ELSE 0 END) AS too_long
    """).single()
    
    print(f"  â€¢ ç¸½å­¤ç«‹å¯¦é«”æ•¸ï¼š{isolated_stats['total_isolated']}")
    print(f"  â€¢ ç„¡ MENTIONS é€£æ¥ï¼š{isolated_stats['no_mentions']} ({isolated_stats['no_mentions']/isolated_stats['total_isolated']*100:.1f}%)")
    print(f"  â€¢ å–®ä¸€ Chunk æåŠï¼š{isolated_stats['single_mention']} ({isolated_stats['single_mention']/isolated_stats['total_isolated']*100:.1f}%)")
    print(f"  â€¢ å¤š Chunk æåŠï¼š{isolated_stats['multiple_mentions']} ({isolated_stats['multiple_mentions']/isolated_stats['total_isolated']*100:.1f}%)")
    print(f"  â€¢ åç¨±éçŸ­ï¼ˆ<3å­—å…ƒï¼‰ï¼š{isolated_stats['too_short']} ({isolated_stats['too_short']/isolated_stats['total_isolated']*100:.1f}%)")
    print(f"  â€¢ åç¨±éé•·ï¼ˆ>40å­—å…ƒï¼‰ï¼š{isolated_stats['too_long']} ({isolated_stats['too_long']/isolated_stats['total_isolated']*100:.1f}%)")
    
    # 3. æ½›åœ¨çš„åŒç¾©è©æª¢æ¸¬
    print(f"\nğŸ”— æ½›åœ¨åŒç¾©è©æª¢æ¸¬ï¼ˆç›¸ä¼¼å¯¦é«”åç¨±ï¼‰ï¼š")
    print("-"*70)
    
    potential_synonyms = session.run("""
        MATCH (e1:Entity)
        WHERE NOT (e1)-[:RELATION]-()
        MATCH (e2:Entity)
        WHERE e2 <> e1 AND (e2)-[:RELATION]-()
        AND (
            toLower(e1.name) CONTAINS toLower(e2.name) 
            OR toLower(e2.name) CONTAINS toLower(e1.name)
            OR toLower(replace(e1.name, '_', ' ')) = toLower(replace(e2.name, '_', ' '))
        )
        RETURN 
            e1.name AS isolated_entity,
            e2.name AS connected_entity,
            COUNT { (e2)-[:RELATION]-() } AS connected_degree
        ORDER BY connected_degree DESC
        LIMIT 15
    """).data()
    
    if potential_synonyms:
        print(f"  ç™¼ç¾ {len(potential_synonyms)} å°æ½›åœ¨åŒç¾©è©ï¼š")
        for syn in potential_synonyms:
            print(f"    â€¢ å­¤ç«‹: '{syn['isolated_entity'][:30]}' â†” å·²é€£æ¥: '{syn['connected_entity'][:30]}' (åº¦æ•¸: {syn['connected_degree']})")
    else:
        print("  æœªç™¼ç¾æ˜é¡¯çš„åŒç¾©è©æ¨¡å¼")

print(f"\n{'='*70}")
print("ğŸ’¡ è¨ºæ–·å»ºè­°ï¼š")
print("  1. éçŸ­å¯¦é«”ã€ç´”æ•¸å­—å¯¦é«” â†’ å»ºè­°åˆªé™¤ï¼ˆå¯èƒ½æ˜¯æå–éŒ¯èª¤ï¼‰")
print("  2. å–®ä¸€æåŠä¸”åç¨±ä¸å¸¸è¦‹ â†’ å¯èƒ½æ˜¯ä½è³ªé‡å¯¦é«”ï¼Œè€ƒæ…®åˆªé™¤")
print("  3. å¤šæ¬¡æåŠä½†å­¤ç«‹ â†’ é—œä¿‚æå–å¤±æ•—ï¼Œéœ€è¦é‡æ–°æå–é—œä¿‚")
print("  4. ç™¼ç¾åŒç¾©è© â†’ éœ€è¦å¯¦é«”æ­£è¦åŒ–èˆ‡åˆä½µ")
print("="*70)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ä¿®æ­£é—œä¿‚ä¾†æºæ¨™è¨˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("ğŸ”§ é–‹å§‹ä¿®æ­£é—œä¿‚ä¾†æºæ¨™è¨˜...")
print("="*70)

with GRAPH_DRIVER.session() as session:
    # 1. æª¢æŸ¥å•é¡Œè¦æ¨¡
    print("\nğŸ“Š æª¢æŸ¥å•é¡Œè¦æ¨¡...")
    
    missing_source = session.run("""
        MATCH ()-[r:RELATION]->()
        WHERE r.chunks IS NULL OR size(r.chunks) = 0
        RETURN 
            count(r) AS missing_count,
            sum(CASE WHEN r.inferred = true THEN 1 ELSE 0 END) AS inferred_missing,
            sum(CASE WHEN r.densified = true THEN 1 ELSE 0 END) AS densified_missing,
            sum(CASE WHEN r.enhanced = true THEN 1 ELSE 0 END) AS enhanced_missing
    """).single()
    
    print(f"  â€¢ ç¼ºå°‘ä¾†æºæ¨™è¨˜çš„é—œä¿‚ç¸½æ•¸ï¼š{missing_source['missing_count']}")
    print(f"    - inferred é—œä¿‚ï¼š{missing_source['inferred_missing']}")
    print(f"    - densified é—œä¿‚ï¼š{missing_source['densified_missing']}")
    print(f"    - enhanced é—œä¿‚ï¼š{missing_source['enhanced_missing']}")
    
    if missing_source['missing_count'] == 0:
        print("\nâœ… æ‰€æœ‰é—œä¿‚éƒ½å·²æ­£ç¢ºæ¨™è¨˜ä¾†æºï¼")
    else:
        # 2. ä¿®æ­£ inferred é—œä¿‚çš„ä¾†æº
        print(f"\nğŸ”„ ä¿®æ­£ inferred é—œä¿‚çš„ä¾†æºæ¨™è¨˜...")
        
        result_inferred = session.run("""
            MATCH (h:Entity)-[r:RELATION]->(t:Entity)
            WHERE r.inferred = true AND (r.chunks IS NULL OR size(r.chunks) = 0)
            
            // æ‰¾åˆ°é ­å°¾å¯¦é«”å…±åŒå‡ºç¾çš„ Chunks
            OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(h)
            OPTIONAL MATCH (c)-[:MENTIONS]->(t)
            WITH r, collect(DISTINCT c.id) AS common_chunks
            
            // å¦‚æœæœ‰å…±åŒ Chunkï¼Œä½¿ç”¨å…±åŒ Chunkï¼›å¦å‰‡ä½¿ç”¨é ­å¯¦é«”çš„ Chunk
            SET r.chunks = CASE 
                WHEN size(common_chunks) > 0 THEN common_chunks
                ELSE []
            END
            
            RETURN count(r) AS fixed_count
        """).single()
        
        print(f"  âœ… ä¿®æ­£ {result_inferred['fixed_count']} å€‹ inferred é—œä¿‚")
        
        # 3. ä¿®æ­£ densified é—œä¿‚çš„ä¾†æºï¼ˆæ‡‰è©²åœ¨å¯«å…¥æ™‚å°±æœ‰ï¼Œé€™æ˜¯å‚™ç”¨ï¼‰
        print(f"\nğŸ”„ æª¢æŸ¥ densified é—œä¿‚...")
        
        result_densified = session.run("""
            MATCH (h:Entity)-[r:RELATION]->(t:Entity)
            WHERE r.densified = true AND (r.chunks IS NULL OR size(r.chunks) = 0)
            
            // æ‰¾åˆ°é ­å°¾å¯¦é«”å…±åŒå‡ºç¾çš„ Chunks
            OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(h)
            OPTIONAL MATCH (c)-[:MENTIONS]->(t)
            WITH r, collect(DISTINCT c.id) AS common_chunks
            
            SET r.chunks = common_chunks
            
            RETURN count(r) AS fixed_count
        """).single()
        
        print(f"  âœ… ä¿®æ­£ {result_densified['fixed_count']} å€‹ densified é—œä¿‚")
        
        # 4. ä¿®æ­£ enhanced é—œä¿‚çš„ä¾†æº
        print(f"\nğŸ”„ æª¢æŸ¥ enhanced é—œä¿‚...")
        
        result_enhanced = session.run("""
            MATCH (h:Entity)-[r:RELATION]->(t:Entity)
            WHERE r.enhanced = true AND (r.chunks IS NULL OR size(r.chunks) = 0)
            
            // æ‰¾åˆ°é ­å°¾å¯¦é«”å…±åŒå‡ºç¾çš„ Chunks
            OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(h)
            OPTIONAL MATCH (c)-[:MENTIONS]->(t)
            WITH r, collect(DISTINCT c.id) AS common_chunks
            
            SET r.chunks = common_chunks
            
            RETURN count(r) AS fixed_count
        """).single()
        
        print(f"  âœ… ä¿®æ­£ {result_enhanced['fixed_count']} å€‹ enhanced é—œä¿‚")
        
        # 5. æœ€çµ‚é©—è­‰
        print(f"\nğŸ“Š æœ€çµ‚é©—è­‰...")
        
        final_check = session.run("""
            MATCH ()-[r:RELATION]->()
            RETURN 
                count(r) AS total_relations,
                sum(CASE WHEN r.chunks IS NULL OR size(r.chunks) = 0 THEN 1 ELSE 0 END) AS still_missing,
                sum(CASE WHEN size(r.chunks) >= 1 THEN 1 ELSE 0 END) AS has_source,
                sum(CASE WHEN size(r.chunks) >= 2 THEN 1 ELSE 0 END) AS multi_source
        """).single()
        
        print(f"  â€¢ é—œä¿‚ç¸½æ•¸ï¼š{final_check['total_relations']}")
        print(f"  â€¢ æœ‰ä¾†æºæ¨™è¨˜ï¼š{final_check['has_source']} ({final_check['has_source']/final_check['total_relations']*100:.1f}%)")
        print(f"  â€¢ å¤šä¾†æºæ”¯æŒï¼š{final_check['multi_source']} ({final_check['multi_source']/final_check['total_relations']*100:.1f}%)")
        print(f"  â€¢ ä»ç¼ºå°‘ä¾†æºï¼š{final_check['still_missing']} ({final_check['still_missing']/final_check['total_relations']*100:.1f}%)")

print("\n" + "="*70)
print("âœ… é—œä¿‚ä¾†æºæ¨™è¨˜ä¿®æ­£å®Œæˆï¼")
print("="*70)
# åœ–è­œè³ªé‡æª¢é©—èˆ‡å®Œæ•´åº¦é©—è­‰ï¼ˆå­¸è¡“ç´šå°ˆæ¥­ç‰ˆï¼‰

def ValidateGraphIntegrity(
    driver,
    original_chunks: List[Dict[str, str]],
    dataset_id: str,
    sample_size: int = 10,
    verbose: bool = True,
) -> Dict[str, Any]:
    """
    åŸ·è¡Œ Graph RAG çŸ¥è­˜åœ–è­œçš„å®Œæ•´åº¦èˆ‡è³ªé‡æª¢é©—ï¼ˆåŸºæ–¼å­¸è¡“èˆ‡å¯¦å‹™æ¨™æº–ï¼‰ã€‚
    
    æª¢é©—æ¶æ§‹ï¼š
    ã€ç¬¬ä¸€çµ„ã€‘çµæ§‹èˆ‡å®Œæ•´åº¦æª¢é©— (Completeness & Structural Quality)
       - ç¯€é»è¦†è“‹ç‡ (Node Coverage)
       - é—œä¿‚å¯†åº¦ (Relationship Density)
       - å±¬æ€§å¡«å……ç‡ (Property Fill Rate)
       - å­¤ç«‹ç¯€é»æ¯”ä¾‹ (Isolated Nodes Ratio)
    
    ã€ç¬¬äºŒçµ„ã€‘ä¸€è‡´æ€§èˆ‡é¡å‹æª¢æŸ¥ (Consistency & Schema Adherence)
       - é¡å‹éµå®ˆç‡ (Schema Adherence)
       - é‡è¤‡å¯¦é«”æª¢æ¸¬ (Duplication Check)
       - å±¬æ€§åˆæ³•æ€§æª¢æŸ¥ (Attribute Validity)
    
    ã€ç¬¬ä¸‰çµ„ã€‘æ ¸å¿ƒæ•¸æ“šè³ªé‡å ±å‘Š (Accuracy & Provenance)
       - äººå·¥æŠ½æ¨£é©—è­‰ (Manual Sampling) - 10 å€‹ä¸‰å…ƒçµ„
       - å‡ºè™•æ¨™è¨»ç‡ (Provenance Rate)
    
    Args:
        driver: Neo4j GraphDatabase driver
        original_chunks: åŸå§‹çŸ¥è­˜åº«çš„æ–‡æœ¬å€å¡Šåˆ—è¡¨
        dataset_id: è³‡æ–™é›†è­˜åˆ¥ç¬¦
        sample_size: äººå·¥æŠ½æ¨£ä¸‰å…ƒçµ„æ•¸é‡ï¼ˆé è¨­ 10ï¼‰
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°å ±å‘Š
    
    Returns:
        åŒ…å«æ‰€æœ‰æª¢é©—çµæœèˆ‡å°ˆå®¶çµè«–çš„å­—å…¸
    
    åƒè€ƒæ–‡ç»ï¼š
        - Completeness metrics: Paulheim (2017), "Knowledge Graph Refinement"
        - Quality dimensions: Zaveri et al. (2016), "Quality Assessment for Linked Data"
    """
    validation_results = {
        "completeness_structural": {},
        "consistency_schema": {},
        "accuracy_provenance": {},
        "overall_pass": False,
        "quality_grade": "",
        "expert_conclusion": "",
    }
    
    if verbose:
        print("=" * 100)
        print("ğŸ”¬ çŸ¥è­˜åœ–è­œè³ªé‡èˆ‡å®Œæ•´åº¦å°ˆæ¥­æª¢é©—å ±å‘Š (Academic-Grade KG Quality Assessment)")
        print("=" * 100)
        print("ğŸ“š æª¢é©—æ¨™æº–ï¼šPaulheim (2017) + Zaveri et al. (2016)")
        print("=" * 100)
    
    with driver.session() as session:
        # ==========================================
        # ã€ç¬¬ä¸€çµ„ã€‘çµæ§‹èˆ‡å®Œæ•´åº¦æª¢é©—
        # ==========================================
        if verbose:
            print("\n" + "=" * 100)
            print("ã€ç¬¬ä¸€çµ„ã€‘çµæ§‹èˆ‡å®Œæ•´åº¦æª¢é©— (Completeness & Structural Quality)")
            print("=" * 100)
        
        # 1.1 ç¯€é»è¦†è“‹ç‡ (Node Coverage)
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 1.1 | ç¯€é»è¦†è“‹ç‡ (Node Coverage)")
            print("-" * 100)
        
        expected_chunk_count = len(original_chunks)
        db_chunk_count = session.run(
            "MATCH (c:Chunk {dataset: $dataset}) RETURN count(c) AS cnt",
            dataset=dataset_id,
        ).single()["cnt"]
        
        total_chunk_count = session.run("MATCH (c:Chunk) RETURN count(c) AS cnt").single()["cnt"]
        other_chunks = total_chunk_count - db_chunk_count
        
        node_coverage = (db_chunk_count / expected_chunk_count * 100) if expected_chunk_count > 0 else 0
        
        if verbose:
            print(f"  â€¢ åŸå§‹æ–‡æœ¬ Chunk ç¸½æ•¸ (Expected)ï¼š{expected_chunk_count}")
            print(f"  â€¢ Neo4j ç•¶å‰ Dataset Chunk æ•¸ (Actual)ï¼š{db_chunk_count}")
            print(f"  â€¢ ç¯€é»è¦†è“‹ç‡ (Coverage Rate)ï¼š{node_coverage:.2f}%")
            if other_chunks > 0:
                print(f"  âš ï¸ è­¦å‘Šï¼šè³‡æ–™åº«ä¸­æœ‰å…¶ä»– dataset çš„ {other_chunks} å€‹èˆŠ Chunk")
            if node_coverage >= 100:
                print(f"  âœ… è©•ä¼°ï¼šç¯€é»è¦†è“‹ç‡é”æ¨™ (â‰¥100%)")
            elif node_coverage >= 95:
                print(f"  âš ï¸ è©•ä¼°ï¼šç¯€é»è¦†è“‹ç‡å¯æ¥å— (95-100%)")
            else:
                print(f"  âŒ è©•ä¼°ï¼šç¯€é»è¦†è“‹ç‡ä¸è¶³ (<95%)")
        
        # 1.2 é—œä¿‚å¯†åº¦ (Relationship Density)
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 1.2 | é—œä¿‚å¯†åº¦ (Relationship Density)")
            print("-" * 100)
        
        entity_count = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)
            RETURN count(DISTINCT e) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        relation_count = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(h:Entity)-[r:RELATION]->(t:Entity)
            RETURN count(DISTINCT r) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        relationship_density = (relation_count / entity_count) if entity_count > 0 else 0
        
        # çµ±è¨ˆå¯¦é«”çš„é—œä¿‚åˆ†ä½ˆ
        # æ³¨æ„ï¼šé€™è£¡çµ±è¨ˆçš„æ˜¯æ¯å€‹å¯¦é«”åƒèˆ‡çš„é—œä¿‚æ•¸ï¼ˆä½œç‚º head æˆ– tailï¼‰
        # ç”±æ–¼é—œä¿‚æ˜¯æœ‰å‘çš„ï¼Œæ¯æ¢é—œä¿‚æœƒè¢«è¨ˆå…¥ head å’Œ tail å„ä¸€æ¬¡
        # æ‰€ä»¥ avg_relations â‰ˆ 2 * relationship_densityï¼ˆç†è«–å€¼ï¼‰
        entity_relation_stats = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)
            OPTIONAL MATCH (e)-[r:RELATION]-()
            WITH e, count(DISTINCT r) AS rel_count
            RETURN 
                count(CASE WHEN rel_count = 0 THEN 1 END) AS isolated_entities,
                count(CASE WHEN rel_count = 1 THEN 1 END) AS single_rel_entities,
                count(CASE WHEN rel_count >= 2 AND rel_count < 5 THEN 1 END) AS moderate_rel_entities,
                count(CASE WHEN rel_count >= 5 THEN 1 END) AS high_rel_entities,
                avg(rel_count) AS avg_relations,
                max(rel_count) AS max_relations
            """,
            dataset=dataset_id,
        ).single()
        
        isolated_count = entity_relation_stats["isolated_entities"] or 0
        single_rel_count = entity_relation_stats["single_rel_entities"] or 0
        moderate_rel_count = entity_relation_stats["moderate_rel_entities"] or 0
        high_rel_count = entity_relation_stats["high_rel_entities"] or 0
        avg_rels = entity_relation_stats["avg_relations"] or 0
        max_rels = entity_relation_stats["max_relations"] or 0
        
        if verbose:
            print(f"  â€¢ å¯¦é«”ç¯€é»ç¸½æ•¸ (Entity Nodes)ï¼š{entity_count}")
            print(f"  â€¢ é—œä¿‚ç¸½æ•¸ (Relations)ï¼š{relation_count}")
            print(f"  â€¢ é—œä¿‚å¯†åº¦ (Density = Relations/Entities)ï¼š{relationship_density:.4f}")
            print()
            print(f"  ğŸ“Š å¯¦é«”é€£æ¥åº¦åˆ†ä½ˆï¼š")
            print(f"    â€¢ å­¤ç«‹å¯¦é«”ï¼ˆ0 å€‹é—œä¿‚ï¼‰ï¼š{isolated_count} ({isolated_count/entity_count*100:.1f}%)")
            print(f"    â€¢ å¼±é€£æ¥å¯¦é«”ï¼ˆ1 å€‹é—œä¿‚ï¼‰ï¼š{single_rel_count} ({single_rel_count/entity_count*100:.1f}%)")
            print(f"    â€¢ ä¸­åº¦é€£æ¥å¯¦é«”ï¼ˆ2-4 å€‹é—œä¿‚ï¼‰ï¼š{moderate_rel_count} ({moderate_rel_count/entity_count*100:.1f}%)")
            print(f"    â€¢ é«˜åº¦é€£æ¥å¯¦é«”ï¼ˆâ‰¥5 å€‹é—œä¿‚ï¼‰ï¼š{high_rel_count} ({high_rel_count/entity_count*100:.1f}%)")
            print()
            print(f"  ğŸ“ˆ å¯¦é«”é€£æ¥åº¦çµ±è¨ˆï¼š")
            print(f"    â€¢ å¹³å‡æ¯å¯¦é«”é—œä¿‚æ•¸ï¼ˆé›™å‘è¨ˆæ•¸ï¼‰ï¼š{avg_rels:.2f}")
            print(f"      â””â”€ èªªæ˜ï¼šçµ±è¨ˆæ™‚æ¯æ¢é—œä¿‚è¢«è¨ˆå…¥ head å’Œ tail å„ 1 æ¬¡")
            print(f"      â””â”€ ç†è«–é—œä¿‚ï¼šå¹³å‡é—œä¿‚æ•¸ â‰ˆ 2 Ã— é—œä¿‚å¯†åº¦ = {relationship_density*2:.2f}")
            print(f"      â””â”€ å¯¦éš›æ¯”å€¼ï¼š{avg_rels/relationship_density if relationship_density > 0 else 0:.2f}x")
            print(f"    â€¢ æœ€å¤§é€£æ¥åº¦ï¼š{max_rels}")
            print()
            
            # å°ˆå®¶ç´šè©•ä¼°
            print(f"  ğŸ”¬ å°ˆå®¶è©•ä¼°ï¼š")
            if relationship_density >= 2.0:
                print(f"    âœ… é—œä¿‚å¯†åº¦å„ªç§€ï¼ˆâ‰¥2.0ï¼‰")
                print(f"       â””â”€ åœ–è­œå…·å‚™è±å¯Œçš„èªç¾©é€£é€šæ€§ï¼Œé©åˆè¤‡é›œæ¨ç†ä»»å‹™")
            elif relationship_density >= 1.5:
                print(f"    âœ… é—œä¿‚å¯†åº¦è‰¯å¥½ï¼ˆ1.5-2.0ï¼‰")
                print(f"       â””â”€ åœ–è­œé€£é€šæ€§å……è¶³ï¼Œæ”¯æŒå¤šè·³æŸ¥è©¢")
            elif relationship_density >= 1.0:
                print(f"    âš ï¸ é—œä¿‚å¯†åº¦ä¸­ç­‰ï¼ˆ1.0-1.5ï¼‰")
                print(f"       â””â”€ åŸºæœ¬æ»¿è¶³éœ€æ±‚ï¼Œä½†ä»æœ‰æ”¹é€²ç©ºé–“")
            elif relationship_density >= 0.5:
                print(f"    âš ï¸ é—œä¿‚å¯†åº¦åä½ï¼ˆ0.5-1.0ï¼‰")
                print(f"       â””â”€ é€£é€šæ€§ä¸è¶³ï¼Œå¤šè·³æ¨ç†èƒ½åŠ›å—é™")
                print(f"       â””â”€ å»ºè­°ï¼šå¢å¼·é—œä¿‚æŠ½å–æ·±åº¦å’Œå»£åº¦")
            else:
                print(f"    âŒ é—œä¿‚å¯†åº¦åš´é‡ä¸è¶³ï¼ˆ<0.5ï¼‰")
                print(f"       â””â”€ åœ–è­œå¹¾ä¹å‘ˆå­¤ç«‹ç‹€æ…‹ï¼Œç„¡æ³•æœ‰æ•ˆæ”¯æŒæ¨ç†")
                print(f"       â””â”€ ç·Šæ€¥éœ€æ±‚ï¼šå…¨é¢å„ªåŒ–ä¸‰å…ƒçµ„æŠ½å–ç­–ç•¥")
            
            print()
            if isolated_count / entity_count > 0.3:
                print(f"    âš ï¸ å­¤ç«‹å¯¦é«”æ¯”ä¾‹éé«˜ï¼ˆ{isolated_count/entity_count*100:.1f}%ï¼‰")
                print(f"       å»ºè­°ï¼šæª¢æŸ¥å¯¦é«”æŠ½å–æ˜¯å¦éæ–¼å¯¬æ³›ï¼Œæˆ–é—œä¿‚æŠ½å–éæ–¼ä¿å®ˆ")
            
            if single_rel_count / entity_count > 0.4:
                print(f"    âš ï¸ å¼±é€£æ¥å¯¦é«”ä½”æ¯”éå¤§ï¼ˆ{single_rel_count/entity_count*100:.1f}%ï¼‰")
                print(f"       å»ºè­°ï¼šå¢åŠ å±¬æ€§é—œä¿‚ã€æ™‚é–“é—œä¿‚ã€å› æœéˆç­‰å¤šç¶­åº¦é—œä¿‚")
            
            if high_rel_count / entity_count < 0.1:
                print(f"    ğŸ’¡ ç¼ºä¹æ ¸å¿ƒæ¨ç´ç¯€é»ï¼ˆé«˜é€£æ¥åº¦å¯¦é«” < 10%ï¼‰")
                print(f"       å»ºè­°ï¼šè­˜åˆ¥ä¸¦å¼·åŒ–é ˜åŸŸæ ¸å¿ƒæ¦‚å¿µçš„é—œä¿‚ç¶²çµ¡")
        
        # 1.3 å±¬æ€§å¡«å……ç‡ (Property Fill Rate)
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 1.3 | å±¬æ€§å¡«å……ç‡ (Property Fill Rate)")
            print("-" * 100)
        
        # æª¢æŸ¥ Entity çš„ name å±¬æ€§å¡«å……ç‡
        entity_with_name = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)
            WHERE e.name IS NOT NULL AND e.name <> ''
            RETURN count(DISTINCT e) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        name_fill_rate = (entity_with_name / entity_count * 100) if entity_count > 0 else 0
        
        # æª¢æŸ¥ Chunk çš„ text å±¬æ€§å¡«å……ç‡
        chunk_with_text = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})
            WHERE c.text IS NOT NULL AND c.text <> ''
            RETURN count(c) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        text_fill_rate = (chunk_with_text / db_chunk_count * 100) if db_chunk_count > 0 else 0
        
        if verbose:
            print(f"  â€¢ Entity.name å¡«å……ç‡ï¼š{name_fill_rate:.2f}% ({entity_with_name}/{entity_count})")
            print(f"  â€¢ Chunk.text å¡«å……ç‡ï¼š{text_fill_rate:.2f}% ({chunk_with_text}/{db_chunk_count})")
            avg_fill_rate = (name_fill_rate + text_fill_rate) / 2
            print(f"  â€¢ å¹³å‡å±¬æ€§å¡«å……ç‡ï¼š{avg_fill_rate:.2f}%")
            if avg_fill_rate >= 95:
                print(f"  âœ… è©•ä¼°ï¼šå±¬æ€§å¡«å……ç‡å„ªç§€ (â‰¥95%)")
            elif avg_fill_rate >= 80:
                print(f"  âš ï¸ è©•ä¼°ï¼šå±¬æ€§å¡«å……ç‡è‰¯å¥½ (80-95%)")
            else:
                print(f"  âŒ è©•ä¼°ï¼šå±¬æ€§å¡«å……ç‡ä¸è¶³ (<80%)")
        
        # 1.4 å­¤ç«‹ç¯€é»æ¯”ä¾‹ (Isolated Nodes Ratio)
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 1.4 | å­¤ç«‹ç¯€é»æ¯”ä¾‹ (Isolated Nodes Ratio)")
            print("-" * 100)
        
        isolated_chunks = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})
            WHERE NOT (c)-[:MENTIONS]->(:Entity)
            RETURN count(c) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        isolated_entities = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)
            WHERE NOT (e)-[:RELATION]-()
            RETURN count(DISTINCT e) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        isolated_chunk_ratio = (isolated_chunks / db_chunk_count * 100) if db_chunk_count > 0 else 0
        isolated_entity_ratio = (isolated_entities / entity_count * 100) if entity_count > 0 else 0
        
        if verbose:
            print(f"  â€¢ å­¤ç«‹ Chunk æ•¸ (ç„¡ MENTIONS é€£æ¥)ï¼š{isolated_chunks} ({isolated_chunk_ratio:.2f}%)")
            print(f"  â€¢ å­¤ç«‹ Entity æ•¸ (ç„¡ RELATION é€£æ¥)ï¼š{isolated_entities} ({isolated_entity_ratio:.2f}%)")
            if isolated_chunk_ratio <= 5 and isolated_entity_ratio <= 15:
                print(f"  âœ… è©•ä¼°ï¼šå­¤ç«‹ç¯€é»æ¯”ä¾‹ä½ï¼Œçµæ§‹å“è³ªè‰¯å¥½")
            elif isolated_chunk_ratio <= 10 and isolated_entity_ratio <= 30:
                print(f"  âš ï¸ è©•ä¼°ï¼šå­¤ç«‹ç¯€é»æ¯”ä¾‹ä¸­ç­‰ï¼Œå»ºè­°å„ªåŒ–ä¸‰å…ƒçµ„æŠ½å–")
            else:
                print(f"  âŒ è©•ä¼°ï¼šå­¤ç«‹ç¯€é»æ¯”ä¾‹åé«˜ï¼Œå¯èƒ½å½±éŸ¿çŸ¥è­˜æ¨ç†èƒ½åŠ›")
        
        # å„²å­˜ç¬¬ä¸€çµ„æª¢é©—çµæœ
        validation_results["completeness_structural"] = {
            "node_coverage": node_coverage,
            "relationship_density": relationship_density,
            "avg_relations_per_entity": avg_rels,
            "max_relations_per_entity": max_rels,
            "property_fill_rate": (name_fill_rate + text_fill_rate) / 2,
            "isolated_chunk_ratio": isolated_chunk_ratio,
            "isolated_entity_ratio": isolated_entity_ratio,
            "entity_connection_distribution": {
                "isolated": isolated_count,
                "single_relation": single_rel_count,
                "moderate_relations": moderate_rel_count,
                "high_relations": high_rel_count,
            },
            "metrics": {
                "expected_chunks": expected_chunk_count,
                "db_chunks": db_chunk_count,
                "entity_count": entity_count,
                "relation_count": relation_count,
                "isolated_chunks": isolated_chunks,
                "isolated_entities": isolated_entities,
            }
        }
        
        # ==========================================
        # ã€ç¬¬äºŒçµ„ã€‘ä¸€è‡´æ€§èˆ‡é¡å‹æª¢æŸ¥
        # ==========================================
        if verbose:
            print("\n" + "=" * 100)
            print("ã€ç¬¬äºŒçµ„ã€‘ä¸€è‡´æ€§èˆ‡é¡å‹æª¢æŸ¥ (Consistency & Schema Adherence)")
            print("=" * 100)
        
        # 2.1 é¡å‹éµå®ˆç‡ (Schema Adherence)
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 2.1 | é¡å‹éµå®ˆç‡ (Schema Adherence)")
            print("-" * 100)
        
        node_labels_result = session.run(
            """
            CALL db.labels() YIELD label
            RETURN collect(label) AS labels
            """
        ).single()
        node_labels = node_labels_result["labels"] if node_labels_result else []
        
        relationship_types_result = session.run(
            """
            CALL db.relationshipTypes() YIELD relationshipType
            RETURN collect(relationshipType) AS types
            """
        ).single()
        relationship_types = relationship_types_result["types"] if relationship_types_result else []
        
        # æª¢æŸ¥é—œä¿‚çš„èªç¾©æœ‰æ•ˆæ€§ï¼ˆæª¢æŸ¥ RELATION.type å±¬æ€§ï¼Œè€Œéé—œä¿‚é¡å‹åç¨±ï¼‰
        # çµ±è¨ˆä¸åŒçš„èªç¾©é—œä¿‚é¡å‹ï¼ˆå¾ r.type å±¬æ€§ï¼‰
        semantic_relations_result = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()
            WHERE r.type IS NOT NULL AND r.type <> ''
            RETURN DISTINCT r.type AS relation_type
            ORDER BY relation_type
            """,
            dataset=dataset_id,
        ).data()
        semantic_relations = [row["relation_type"] for row in semantic_relations_result]
        
        # æª¢æ¸¬éæ–¼å¯¬æ³›çš„èªç¾©é—œä¿‚ï¼ˆåœ¨ r.type å±¬æ€§ä¸­ï¼‰
        generic_semantic_relations = [
            rt for rt in semantic_relations 
            if rt.upper() in ['RELATION', 'RELATES_TO', 'CONNECTED_TO', 'ASSOCIATED_WITH', 'é—œè¯', 'ç›¸é—œ', 'é€£æ¥']
        ]
        
        # çµ±è¨ˆä½¿ç”¨é€šç”¨é—œä¿‚çš„æ•¸é‡
        generic_relation_count = 0
        if generic_semantic_relations:
            generic_relation_count = session.run(
                """
                MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()
                WHERE r.type IN $generic_types
                RETURN count(r) AS cnt
                """,
                dataset=dataset_id,
                generic_types=generic_semantic_relations,
            ).single()["cnt"]
        
        if verbose:
            print(f"  â€¢ ç¯€é»æ¨™ç±¤ (Node Labels)ï¼š{', '.join(node_labels)}")
            print(f"    â””â”€ å…± {len(node_labels)} ç¨®ç¯€é»é¡å‹")
            print(f"  â€¢ Neo4j é—œä¿‚é¡å‹ (Relationship Types)ï¼š{', '.join(relationship_types)}")
            print(f"    â””â”€ å…± {len(relationship_types)} ç¨® Neo4j é—œä¿‚é¡å‹")
            print(f"  â€¢ èªç¾©é—œä¿‚é¡å‹ (RELATION.type å±¬æ€§å€¼)ï¼š{', '.join(semantic_relations[:20])}")
            if len(semantic_relations) > 20:
                print(f"    ... ä»¥åŠå…¶ä»– {len(semantic_relations) - 20} ç¨®èªç¾©é—œä¿‚")
            print(f"    â””â”€ å…± {len(semantic_relations)} ç¨®èªç¾©é—œä¿‚é¡å‹")
            
            if generic_semantic_relations:
                print(f"  âš ï¸ è­¦å‘Šï¼šæª¢æ¸¬åˆ°éæ–¼å¯¬æ³›çš„èªç¾©é—œä¿‚é¡å‹ï¼š{', '.join(generic_semantic_relations)}")
                print(f"     å…±æœ‰ {generic_relation_count} å€‹é—œä¿‚ä½¿ç”¨äº†é€šç”¨èªç¾©")
                print(f"     å»ºè­°ï¼šå„ªåŒ–æç¤ºè©ä»¥ç”¢ç”Ÿæ›´å…·é«”çš„èªç¾©é—œä¿‚é¡å‹")
            
            if len(node_labels) >= 2 and len(semantic_relations) >= 2:
                print(f"  âœ… è©•ä¼°ï¼šåœ–è­œé¡å‹çµæ§‹å®Œæ•´ï¼Œèªç¾©é—œä¿‚è±å¯Œ")
            else:
                print(f"  âŒ è©•ä¼°ï¼šåœ–è­œé¡å‹çµæ§‹ä¸å®Œæ•´æˆ–èªç¾©é—œä¿‚éæ–¼å–®ä¸€")
        
        # 2.2 é‡è¤‡å¯¦é«”æª¢æ¸¬ (Duplication Check)
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 2.2 | é‡è¤‡å¯¦é«”æª¢æ¸¬ (Duplication Check)")
            print("-" * 100)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰åç¨±å®Œå…¨ç›¸åŒçš„å¯¦é«”ï¼ˆå¯èƒ½è¡¨ç¤ºé‡è¤‡ï¼‰
        duplicate_entities = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)
            WITH e.name AS name, collect(DISTINCT e) AS entities
            WHERE size(entities) > 1
            RETURN count(*) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        # æŠ½æ¨£æ½›åœ¨é‡è¤‡å¯¦é«”
        duplicate_samples = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)
            WITH e.name AS name, collect(DISTINCT id(e)) AS entity_ids
            WHERE size(entity_ids) > 1
            RETURN name, size(entity_ids) AS count
            LIMIT 5
            """,
            dataset=dataset_id,
        ).data()
        
        if verbose:
            print(f"  â€¢ æª¢æ¸¬åˆ°çš„é‡è¤‡å¯¦é«”åç¨±æ•¸ï¼š{duplicate_entities}")
            if duplicate_samples:
                print(f"  â€¢ æŠ½æ¨£ç¯„ä¾‹ï¼š")
                for sample in duplicate_samples:
                    print(f"    - ã€Œ{sample['name']}ã€: {sample['count']} å€‹ç¯€é»")
                print(f"  âš ï¸ è©•ä¼°ï¼šå­˜åœ¨é‡è¤‡å¯¦é«”ï¼Œå¯èƒ½éœ€è¦å¯¦é«”æ¶ˆæ­§ï¼ˆEntity Disambiguationï¼‰")
            else:
                print(f"  âœ… è©•ä¼°ï¼šæœªæª¢æ¸¬åˆ°æ˜é¡¯é‡è¤‡å¯¦é«”")
        
        # 2.3 å±¬æ€§åˆæ³•æ€§æª¢æŸ¥ (Attribute Validity)
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 2.3 | å±¬æ€§åˆæ³•æ€§æª¢æŸ¥ (Attribute Validity)")
            print("-" * 100)
        
        # æª¢æŸ¥ç©ºå€¼æˆ–ç„¡æ•ˆå€¼
        invalid_relations = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()
            WHERE r.type IS NULL OR r.type = ''
            RETURN count(r) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        if verbose:
            print(f"  â€¢ ç„¡æ•ˆé—œä¿‚æ•¸ï¼ˆtype å±¬æ€§ç‚ºç©ºï¼‰ï¼š{invalid_relations}")
            if invalid_relations == 0:
                print(f"  âœ… è©•ä¼°ï¼šæ‰€æœ‰é—œä¿‚å±¬æ€§åˆæ³•")
            else:
                print(f"  âŒ è©•ä¼°ï¼šå­˜åœ¨ {invalid_relations} å€‹ç„¡æ•ˆé—œä¿‚ï¼Œéœ€è¦ä¿®æ­£")
        
        # å„²å­˜ç¬¬äºŒçµ„æª¢é©—çµæœ
        validation_results["consistency_schema"] = {
            "node_label_count": len(node_labels),
            "relationship_type_count": len(relationship_types),
            "semantic_relation_count": len(semantic_relations),
            "node_labels": node_labels,
            "relationship_types": relationship_types,
            "semantic_relations": semantic_relations,
            "generic_semantic_relations": generic_semantic_relations,
            "generic_relation_count": generic_relation_count,
            "duplicate_entities": duplicate_entities,
            "duplicate_samples": duplicate_samples,
            "invalid_relations": invalid_relations,
        }
        
        # ==========================================
        # ã€ç¬¬ä¸‰çµ„ã€‘æ ¸å¿ƒæ•¸æ“šè³ªé‡å ±å‘Š
        # ==========================================
        if verbose:
            print("\n" + "=" * 100)
            print("ã€ç¬¬ä¸‰çµ„ã€‘æ ¸å¿ƒæ•¸æ“šè³ªé‡å ±å‘Š (Accuracy & Provenance)")
            print("=" * 100)
        
        # 3.1 äººå·¥æŠ½æ¨£é©—è­‰ (Manual Sampling) - 10 å€‹ä¸‰å…ƒçµ„
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 3.1 | äººå·¥æŠ½æ¨£é©—è­‰ (Manual Sampling for Accuracy)")
            print("-" * 100)
            print(f"  éš¨æ©ŸæŠ½å– {sample_size} å€‹ä¸‰å…ƒçµ„ï¼Œè«‹é€²è¡Œäººå·¥èªç¾©æ­£ç¢ºæ€§æª¢æŸ¥ï¼š")
            print()
        
        sampled_triples = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(h:Entity)-[r:RELATION]->(t:Entity)
            WITH h, r, t, rand() AS random
            ORDER BY random
            LIMIT $limit
            RETURN h.name AS head, r.type AS relation, t.name AS tail
            """,
            dataset=dataset_id,
            limit=sample_size,
        ).data()
        
        if verbose:
            if sampled_triples:
                for idx, triple in enumerate(sampled_triples, start=1):
                    head = triple.get("head", "N/A")
                    relation = triple.get("relation", "N/A")
                    tail = triple.get("tail", "N/A")
                    print(f"  [{idx:2d}] ({head}) --[{relation}]--> ({tail})")
                print()
                print("  " + "=" * 96)
                print("  ğŸ’¡ äººå·¥æª¢æŸ¥æŒ‡å¼• (Manual Verification Guidelines)")
                print("  " + "=" * 96)
                print("  è«‹é‡å°ä»¥ä¸Šä¸‰å…ƒçµ„é€ä¸€è©•ä¼°ä»¥ä¸‹ä¸‰å€‹ç¶­åº¦ï¼š")
                print()
                print("  âœ“ èªç¾©æ­£ç¢ºæ€§ (Semantic Correctness)")
                print("    â””â”€ å¯¦é«”åç¨±æ˜¯å¦æ­£ç¢ºä¸”æœ‰æ„ç¾©ï¼Ÿ")
                print("    â””â”€ é—œä¿‚é¡å‹æ˜¯å¦æº–ç¢ºæè¿°å…©å¯¦é«”é–“çš„èªç¾©é—œè¯ï¼Ÿ")
                print()
                print("  âœ“ é‚è¼¯ä¸€è‡´æ€§ (Logical Consistency)")
                print("    â””â”€ ä¸‰å…ƒçµ„çš„é‚è¼¯æ˜¯å¦ç¬¦åˆçœŸå¯¦ä¸–ç•Œæˆ–åŸå§‹çŸ¥è­˜åº«å…§å®¹ï¼Ÿ")
                print("    â””â”€ Head å’Œ Tail çš„å¯¦é«”é¡å‹æ˜¯å¦èˆ‡ Relation ç›¸å®¹ï¼Ÿ")
                print()
                print("  âœ“ è³‡è¨Šå®Œæ•´æ€§ (Information Completeness)")
                print("    â””â”€ ä¸‰å…ƒçµ„æ˜¯å¦åŒ…å«è¶³å¤ çš„ä¸Šä¸‹æ–‡è³‡è¨Šï¼Ÿ")
                print("    â””â”€ æ˜¯å¦æœ‰æ˜é¡¯çš„è³‡è¨Šç¼ºå¤±æˆ–æ­§ç¾©ï¼Ÿ")
                print()
                print("  ğŸ“ å»ºè­°ï¼šè¨˜éŒ„æœ‰å•é¡Œçš„ä¸‰å…ƒçµ„ç·¨è™Ÿï¼Œç”¨æ–¼å¾ŒçºŒå„ªåŒ–æç¤ºè©æˆ–çŸ¥è­˜æŠ½å–æµç¨‹ã€‚")
                print("  " + "=" * 96)
            else:
                print("  âŒ è­¦å‘Šï¼šç„¡æ³•æŠ½å–ä¸‰å…ƒçµ„ï¼Œåœ–è­œä¸­å¯èƒ½æ²’æœ‰æœ‰æ•ˆçš„ RELATION é—œä¿‚")
        
        # 3.2 å‡ºè™•æ¨™è¨»ç‡ (Provenance Rate)
        if verbose:
            print("\nğŸ“Š æŒ‡æ¨™ 3.2 | å‡ºè™•æ¨™è¨»ç‡ (Provenance Rate)")
            print("-" * 100)
        
        # æª¢æŸ¥ Chunk çš„ source å±¬æ€§å¡«å……ç‡
        chunks_with_source = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})
            WHERE c.source IS NOT NULL AND c.source <> ''
            RETURN count(c) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        provenance_rate = (chunks_with_source / db_chunk_count * 100) if db_chunk_count > 0 else 0
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ RELATION åŒ…å«ä¾†æº chunks è³‡è¨Š
        relations_with_chunks = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()
            WHERE r.chunks IS NOT NULL AND size(r.chunks) > 0
            RETURN count(DISTINCT r) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        
        relation_provenance_rate = (relations_with_chunks / relation_count * 100) if relation_count > 0 else 0
        
        if verbose:
            print(f"  â€¢ Chunk å‡ºè™•æ¨™è¨»ç‡ (source å±¬æ€§)ï¼š{provenance_rate:.2f}% ({chunks_with_source}/{db_chunk_count})")
            print(f"  â€¢ Relation ä¾†æºè¿½æº¯ç‡ (chunks å±¬æ€§)ï¼š{relation_provenance_rate:.2f}% ({relations_with_chunks}/{relation_count})")
            avg_provenance = (provenance_rate + relation_provenance_rate) / 2
            print(f"  â€¢ å¹³å‡å‡ºè™•æ¨™è¨»ç‡ï¼š{avg_provenance:.2f}%")
            if avg_provenance >= 90:
                print(f"  âœ… è©•ä¼°ï¼šå‡ºè™•æ¨™è¨»ç‡å„ªç§€ï¼ŒçŸ¥è­˜æº¯æºæ€§é«˜")
            elif avg_provenance >= 70:
                print(f"  âš ï¸ è©•ä¼°ï¼šå‡ºè™•æ¨™è¨»ç‡è‰¯å¥½ï¼Œå»ºè­°é€²ä¸€æ­¥æå‡")
            else:
                print(f"  âŒ è©•ä¼°ï¼šå‡ºè™•æ¨™è¨»ç‡ä¸è¶³ï¼Œå¯èƒ½å½±éŸ¿çŸ¥è­˜å¯ä¿¡åº¦")
        
        # å„²å­˜ç¬¬ä¸‰çµ„æª¢é©—çµæœ
        validation_results["accuracy_provenance"] = {
            "sampled_triples": sampled_triples,
            "sample_size": len(sampled_triples),
            "provenance_rate": provenance_rate,
            "relation_provenance_rate": relation_provenance_rate,
            "avg_provenance": (provenance_rate + relation_provenance_rate) / 2,
        }
    
    # ==========================================
    # ã€æœ€çµ‚å°ˆå®¶çµè«–ã€‘ç¶œåˆè³ªé‡è©•ä¼°
    # ==========================================
    if verbose:
        print("\n" + "=" * 100)
        print("ã€æœ€çµ‚å°ˆå®¶çµè«–ã€‘ç¶œåˆè³ªé‡è©•ä¼° (Overall Quality Grade & Expert Conclusion)")
        print("=" * 100)
    
    # è¨ˆç®—å„çµ„æŒ‡æ¨™çš„åˆ†æ•¸
    comp_struct = validation_results["completeness_structural"]
    consist_schema = validation_results["consistency_schema"]
    acc_prov = validation_results["accuracy_provenance"]
    
    # è©•åˆ†é‚è¼¯ï¼ˆåŸºæ–¼å­¸è¡“æ¨™æº–ï¼‰
    score_completeness = 0
    if comp_struct["node_coverage"] >= 100:
        score_completeness += 25
    elif comp_struct["node_coverage"] >= 95:
        score_completeness += 20
    
    if comp_struct["relationship_density"] >= 0.5:
        score_completeness += 25
    elif comp_struct["relationship_density"] >= 0.2:
        score_completeness += 15
    
    if comp_struct["property_fill_rate"] >= 95:
        score_completeness += 25
    elif comp_struct["property_fill_rate"] >= 80:
        score_completeness += 20
    
    if comp_struct["isolated_chunk_ratio"] <= 5 and comp_struct["isolated_entity_ratio"] <= 15:
        score_completeness += 25
    elif comp_struct["isolated_chunk_ratio"] <= 10 and comp_struct["isolated_entity_ratio"] <= 30:
        score_completeness += 15
    
    score_consistency = 0
    if consist_schema["node_label_count"] >= 2 and consist_schema["semantic_relation_count"] >= 5:
        score_consistency += 30
    elif consist_schema["semantic_relation_count"] >= 2:
        score_consistency += 15
    
    # æª¢æŸ¥èªç¾©é—œä¿‚çš„è³ªé‡ï¼ˆç„¡é€šç”¨é—œä¿‚ = æ»¿åˆ†ï¼Œå¦å‰‡æŒ‰æ¯”ä¾‹æ‰£åˆ†ï¼‰
    if consist_schema["generic_relation_count"] == 0:
        score_consistency += 30
    else:
        # æŒ‰ç…§é€šç”¨é—œä¿‚æ¯”ä¾‹æ‰£åˆ†
        total_relations = session.run(
            """
            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()
            RETURN count(r) AS cnt
            """,
            dataset=dataset_id,
        ).single()["cnt"]
        if total_relations > 0:
            generic_ratio = consist_schema["generic_relation_count"] / total_relations
            if generic_ratio < 0.1:  # å°æ–¼ 10% çš„é€šç”¨é—œä¿‚
                score_consistency += 25
            elif generic_ratio < 0.3:  # å°æ–¼ 30% çš„é€šç”¨é—œä¿‚
                score_consistency += 15
            elif generic_ratio < 0.5:  # å°æ–¼ 50% çš„é€šç”¨é—œä¿‚
                score_consistency += 5
    
    if consist_schema["duplicate_entities"] == 0:
        score_consistency += 20
    if consist_schema["invalid_relations"] == 0:
        score_consistency += 20
    
    score_accuracy = 0
    if acc_prov["sample_size"] >= sample_size:
        score_accuracy += 50
    if acc_prov["avg_provenance"] >= 90:
        score_accuracy += 50
    elif acc_prov["avg_provenance"] >= 70:
        score_accuracy += 35
    
    total_score = (score_completeness + score_consistency + score_accuracy) / 3
    
    # è³ªé‡ç­‰ç´šåˆ¤å®š
    if total_score >= 85:
        quality_grade = "å„ªç§€ (Excellent)"
        grade_emoji = "ğŸ†"
        fitness_status = "é«˜è³ªé‡"
    elif total_score >= 70:
        quality_grade = "è‰¯å¥½ (Good)"
        grade_emoji = "âœ…"
        fitness_status = "ä¸­é«˜è³ªé‡"
    elif total_score >= 55:
        quality_grade = "ä¸­ç­‰ (Fair)"
        grade_emoji = "âš ï¸"
        fitness_status = "ä¸­ç­‰è³ªé‡"
    else:
        quality_grade = "éœ€æ”¹é€² (Poor)"
        grade_emoji = "âŒ"
        fitness_status = "ä½è³ªé‡"
    
    # æ‰¾å‡ºæœ€å¼±æŒ‡æ¨™
    weakest_metrics = []
    if comp_struct["relationship_density"] < 0.2:
        weakest_metrics.append("é—œä¿‚å¯†åº¦åä½ (å½±éŸ¿å¤šè·³æ¨ç†)")
    if comp_struct["isolated_entity_ratio"] > 30:
        weakest_metrics.append("å­¤ç«‹ç¯€é»æ¯”ä¾‹éé«˜ (å½±éŸ¿çŸ¥è­˜é€£é€šæ€§)")
    if comp_struct["property_fill_rate"] < 80:
        weakest_metrics.append("å±¬æ€§å¡«å……ç‡ä¸è¶³ (å½±éŸ¿è³‡è¨Šå®Œæ•´æ€§)")
    if consist_schema["generic_semantic_relations"]:
        weakest_metrics.append(f"å­˜åœ¨éæ–¼å¯¬æ³›çš„èªç¾©é—œä¿‚ ({', '.join(consist_schema['generic_semantic_relations'][:5])})")
    if acc_prov["avg_provenance"] < 70:
        weakest_metrics.append("å‡ºè™•æ¨™è¨»ç‡åä½ (å½±éŸ¿çŸ¥è­˜æº¯æºæ€§)")
    
    if verbose:
        print(f"\n  {grade_emoji} è³ªé‡ç­‰ç´šï¼š{quality_grade}")
        print(f"  ğŸ“Š ç¶œåˆè©•åˆ†ï¼š{total_score:.1f}/100")
        print()
        print(f"  åˆ†é …è©•åˆ†ï¼š")
        print(f"    â€¢ å®Œæ•´åº¦èˆ‡çµæ§‹ (Completeness & Structure)ï¼š{score_completeness:.1f}/100")
        print(f"    â€¢ ä¸€è‡´æ€§èˆ‡é¡å‹ (Consistency & Schema)ï¼š{score_consistency:.1f}/100")
        print(f"    â€¢ æº–ç¢ºæ€§èˆ‡æº¯æº (Accuracy & Provenance)ï¼š{score_accuracy:.1f}/100")
        print()
        print("  " + "=" * 96)
        print("  ğŸ“‹ å°ˆå®¶çµè«– (Expert Conclusion)")
        print("  " + "=" * 96)
        print()
        print(f"  æ ¹æ“š (i) é—œä¿‚å¯†åº¦ ({comp_struct['relationship_density']:.4f})ã€")
        print(f"       (ii) å­¤ç«‹ç¯€é»æ¯”ç‡ (Chunks: {comp_struct['isolated_chunk_ratio']:.2f}%, Entities: {comp_struct['isolated_entity_ratio']:.2f}%)ã€")
        print(f"       (iii) å±¬æ€§å¡«å……ç‡ ({comp_struct['property_fill_rate']:.2f}%)ã€")
        print(f"       (iv) èªç¾©é—œä¿‚è±å¯Œåº¦ ({consist_schema['semantic_relation_count']} ç¨®)ã€")
        print(f"       (v) å¹³å‡é€£æ¥åº¦ (æ¯å¯¦é«” {comp_struct['avg_relations_per_entity']:.2f} å€‹é—œä¿‚) çš„æ•¸æ“šï¼Œ")
        print()
        print(f"  æœ¬åœ–è­œå·²é”åˆ°ã€{fitness_status}ã€‘æ¨™æº–ï¼Œ{'å¯' if total_score >= 70 else 'æš«ä¸å»ºè­°'}æŠ•å…¥ Graph RAG ç³»çµ±ä½¿ç”¨ã€‚")
        print()
        if weakest_metrics:
            print(f"  âš ï¸ éœ€æ³¨æ„çš„æŒ‡æ¨™ï¼š")
            for metric in weakest_metrics:
                print(f"     â€¢ {metric}")
            print()
            print(f"  ğŸ’¡ æ”¹é€²å»ºè­°ï¼š")
            
            # é—œä¿‚å¯†åº¦å°ˆé …å»ºè­°
            if comp_struct["relationship_density"] < 1.5:
                print(f"     ğŸ“Š é—œä¿‚å¯†åº¦æå‡ç­–ç•¥ï¼ˆç•¶å‰ï¼š{comp_struct['relationship_density']:.2f}ï¼Œç›®æ¨™ï¼šâ‰¥1.5ï¼‰ï¼š")
                print(f"        â”œâ”€ ğŸ”§ å¢å¼·æŠ½å–æ·±åº¦ï¼š")
                print(f"        â”‚  â€¢ æ“´å±•é—œä¿‚é¡å‹ï¼šå±¬æ€§é—œä¿‚ï¼ˆæ•¸å€¼ç‚ºã€æ¿ƒåº¦ç‚ºï¼‰ã€æ™‚é–“é—œä¿‚ï¼ˆç™¼ç”Ÿæ–¼ã€æŒçºŒï¼‰")
                print(f"        â”‚  â€¢ æŒ–æ˜éš±å¼é—œä¿‚ï¼šå› æœéˆï¼ˆAâ†’Bâ†’Cï¼‰ã€å…±ç¾é—œä¿‚ã€å±¤ç´šé—œä¿‚")
                print(f"        â”‚  â€¢ å¯¦æ–½å…±æŒ‡æ¶ˆè§£ï¼šå°‡ã€Œå®ƒã€ã€Œè©²ç‰©è³ªã€é‚„åŸç‚ºå…·é«”å¯¦é«”åï¼Œå¢åŠ å¯¦é«”è¤‡ç”¨")
                print(f"        â”œâ”€ ğŸ¯ å„ªåŒ–æç¤ºè©ï¼š")
                print(f"        â”‚  â€¢ æ˜ç¢ºè¦æ±‚ã€Œæ¯å€‹å¯¦é«”è‡³å°‘ 2 å€‹é—œä¿‚ã€")
                print(f"        â”‚  â€¢ æä¾›å¤šç¶­åº¦é—œä¿‚ç¯„ä¾‹ï¼ˆå› æœã€å±¬æ€§ã€åŠŸèƒ½ã€æ™‚é–“ï¼‰")
                print(f"        â”‚  â€¢ å¢åŠ ã€Œå¾ä¸åŒè§’åº¦æè¿°å¯¦é«”ã€çš„æŒ‡ä»¤")
                print(f"        â””â”€ ğŸ§ª å¾Œè™•ç†å¢å¼·ï¼š")
                print(f"           â€¢ çŸ¥è­˜åœ–è­œè£œå…¨ï¼ˆLink Predictionï¼‰ï¼šTransE/RotatE é æ¸¬ç¼ºå¤±é—œä¿‚")
                print(f"           â€¢ å¯¦é«”åˆä½µï¼šè­˜åˆ¥åŒç¾©å¯¦é«”ï¼ˆå¦‚ã€Œç¶­ç”Ÿç´ Aã€vsã€Œè¦–é»ƒé†‡ã€ï¼‰")
                print(f"           â€¢ é—œä¿‚æ¨ç†ï¼šåŸºæ–¼è¦å‰‡çš„å‚³éé–‰åŒ…ï¼ˆå¦‚ AåŒ…å«B, Bå«æœ‰C â†’ Aé–“æ¥å«æœ‰Cï¼‰")
                print()
            
            if comp_struct["isolated_entity_ratio"] > 30:
                print(f"     â€¢ å­¤ç«‹å¯¦é«”éå¤šï¼ˆ{comp_struct['isolated_entity_ratio']:.1f}%ï¼‰ï¼š")
                print(f"       â””â”€ å¯èƒ½åŸå› ï¼šå¯¦é«”ç²’åº¦éç´°ã€é—œä¿‚æŠ½å–éæ–¼ä¿å®ˆ")
                print(f"       â””â”€ å»ºè­°ï¼šæé«˜å¯¦é«”æŠ½è±¡å±¤ç´šï¼Œæˆ–å¢åŠ å¯¦é«”é–“çš„å¼±é—œä¿‚ï¼ˆå¦‚å…±ç¾ã€ä¸Šä¸‹ä½ï¼‰")
                print()
            
            if consist_schema["generic_semantic_relations"]:
                print(f"     â€¢ å­˜åœ¨é€šç”¨èªç¾©é—œä¿‚ï¼šè«‹ä½¿ç”¨å…·é«”å‹•è©ï¼ˆå°è‡´ã€å«æœ‰ã€å½±éŸ¿ï¼‰æ›¿ä»£æ¨¡ç³Šè©ï¼ˆé—œè¯ã€ç›¸é—œï¼‰")
                print()
            
            if acc_prov["avg_provenance"] < 70:
                print(f"     â€¢ å‡ºè™•æ¨™è¨»ç‡åä½ï¼šç¢ºä¿æ‰€æœ‰é—œä¿‚åŒ…å«ä¾†æºè¿½æº¯è³‡è¨Šï¼ˆchunks å±¬æ€§ï¼‰")
        else:
            print(f"  âœ… æ‰€æœ‰æ ¸å¿ƒæŒ‡æ¨™å‡é”åˆ°å„ªè‰¯æ¨™æº–ï¼Œåœ–è­œè³ªé‡å“è¶Šï¼")
        print()
        print("  " + "=" * 96)
        print(f"  ğŸ“š åƒè€ƒæ–‡ç»èˆ‡é€²éšæŠ€è¡“ï¼š")
        print(f"     â€¢ Paulheim, H. (2017). Knowledge graph refinement: A survey of approaches.")
        print(f"     â€¢ Zaveri, A., et al. (2016). Quality assessment for linked data.")
        print(f"     â€¢ TransE/RotatE: çŸ¥è­˜åœ–è­œåµŒå…¥æ¨¡å‹ï¼Œç”¨æ–¼éˆæ¥é æ¸¬èˆ‡è£œå…¨")
        print(f"     â€¢ Coreference Resolution: å…±æŒ‡æ¶ˆè§£æŠ€è¡“ï¼Œæå‡å¯¦é«”è¤‡ç”¨ç‡")
        print("  " + "=" * 96)
    
    validation_results["overall_pass"] = (total_score >= 70)
    validation_results["quality_grade"] = quality_grade
    validation_results["total_score"] = total_score
    validation_results["score_breakdown"] = {
        "completeness_structural": score_completeness,
        "consistency_schema": score_consistency,
        "accuracy_provenance": score_accuracy,
    }
    validation_results["weakest_metrics"] = weakest_metrics
    validation_results["expert_conclusion"] = f"æœ¬åœ–è­œé”åˆ°ã€{fitness_status}ã€‘æ¨™æº–ï¼ˆè©•åˆ†ï¼š{total_score:.1f}/100ï¼‰"
    
    return validation_results


print("âœ… ValidateGraphIntegrity() å‡½å¼å·²è¼‰å…¥ï¼ˆå­¸è¡“ç´šå°ˆæ¥­ç‰ˆï¼‰")