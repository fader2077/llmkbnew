# Structure augmentation (Phase 3) (placeholder)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Relation Enhancement Prompt: Focus on Existing Entity Relations
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RELATION_ENHANCEMENT_PROMPT = """
You are an expert in knowledge graph relation extraction. Your task is to extract **relationships between entities** from the text, but with a strict constraint:

âš ï¸ **CRITICAL CONSTRAINT**: You can ONLY use entity names from the following list to construct triples.

## ğŸ“‹ Available Entity List (MUST STRICTLY FOLLOW)

{entity_list}

## ğŸ¯ Extraction Rules

1. **Entity Matching**:
   - The 'head' and 'tail' of each triple MUST **exactly match** an entity name from the above list
   - If the text mentions a concept NOT in the list, **DO NOT extract** that triple
   - Perform synonym matching (e.g., "goat" = "goats" = "caprine")

2. **Relation Type Normalization**:
   - Use specific, precise verbs (e.g., "causes", "contains", "requires", "belongs_to")
   - Avoid vague verbs (e.g., "relates_to", "associated_with", "affects")

3. **Deep Mining**:
   - **Explicit relations**: Directly extracted from text statements
   - **Implicit relations**: Causal, classification, and compositional relations based on logical reasoning
   - **Attribute relations**: Numerical, state, and feature-based descriptive relations

4. **Quality First**:
   - Each triple must be semantically clear and logically rigorous
   - Prioritize relations between core concepts
   - Avoid overly granular relations (e.g., "goat"-"weight"-"45" can be simplified to "goat"-"weighs"-"45kg")

## ğŸ“¤ Output Format

Output ONLY a JSON array, with each triple containing head, relation, and tail fields:

```json
[
  {{"head":"goat", "relation":"deficient_in", "tail":"vitamin_A"}},
  {{"head":"vitamin_A_deficiency", "relation":"causes", "tail":"growth_retardation"}},
  {{"head":"goat", "relation":"belongs_to", "tail":"ruminant"}}
]
```

## ğŸ“ Text to Extract From

{chunk_text}

Begin extraction. Remember: **ONLY use entity names from the available entity list**!
"""


def format_entity_list_for_prompt(entities: List[str], max_entities: int = 10000) -> str:
    """
    Format entity list for prompt readability
    
    Args:
        entities: List of entity names
        max_entities: Maximum number of entities to display (avoid excessively long prompts)
    
    Returns:
        Formatted entity list string
    """
    if len(entities) <= max_entities:
        entity_str = "\n".join([f"  â€¢ {entity}" for entity in entities])
        return f"(Total: {len(entities)} entities)\n\n{entity_str}"
    else:
        # If too many entities, show first N + total count
        sample_entities = entities[:max_entities]
        entity_str = "\n".join([f"  â€¢ {entity}" for entity in sample_entities])
        return f"(Total: {len(entities)} entities, showing first {max_entities})\n\n{entity_str}\n\n... and {len(entities) - max_entities} more entities"


print("âœ… Relation enhancement prompt loaded")
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ ¸å¿ƒå‡½æ•¸ï¼šEnhanceGraphConnectivity()
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def EnhanceGraphConnectivity(
    driver,
    client: Client,
    model: str,
    dataset_id: str = "goat_kb_v1",
    max_entities_per_prompt: int = 200,
    temperature: float = 0.2,
    batch_size: int = 5,
) -> Dict[str, Any]:
    """
    é—œä¿‚å¼·åŒ–ä¸»å‡½æ•¸ï¼šåŸºæ–¼ç¾æœ‰ Chunk å’Œ Entity é€²è¡ŒäºŒæ¬¡é—œä¿‚æŠ½å–
    
    æ ¸å¿ƒåŸå‰‡ï¼š
    1. åªé€£æ¥ã€ä¸å‰µå»ºï¼ˆNo CREATE, Only MATCH + MERGEï¼‰
    2. åªå°ç¾æœ‰å¯¦é«”å»ºç«‹é—œä¿‚
    3. å¢é‡å¯«å…¥ï¼Œé¿å…é‡è¤‡
    
    Args:
        driver: Neo4j driver
        client: Ollama client
        model: LLM model name
        dataset_id: Dataset ID
        max_entities_per_prompt: æ¯æ¬¡æç¤ºè©ä¸­åŒ…å«çš„æœ€å¤§å¯¦é«”æ•¸
        temperature: LLM temperature
        batch_size: æ‰¹æ¬¡è™•ç†å¤§å°
    
    Returns:
        {
            'new_relations': int,  # æ–°å¢é—œä¿‚æ•¸é‡
            'processed_chunks': int,  # è™•ç†çš„ Chunk æ•¸é‡
            'density_before': float,  # å¼·åŒ–å‰çš„å¯†åº¦
            'density_after': float,  # å¼·åŒ–å¾Œçš„å¯†åº¦
            'avg_degree_before': float,  # å¼·åŒ–å‰çš„å¹³å‡åº¦æ•¸
            'avg_degree_after': float,  # å¼·åŒ–å¾Œçš„å¹³å‡åº¦æ•¸
        }
    """
    
    print("="*70)
    print("ğŸ”— é—œä¿‚å¼·åŒ–æµç¨‹é–‹å§‹")
    print("="*70)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # éšæ®µé›¶ï¼šè¨˜éŒ„å¼·åŒ–å‰çš„ç‹€æ…‹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with driver.session() as session:
        entity_count = session.run("MATCH (e:Entity) RETURN count(e) AS cnt").single()["cnt"]
        relation_count_before = session.run("MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt").single()["cnt"]
        
        # è¨ˆç®—å¹³å‡åº¦æ•¸
        avg_degree_before = session.run("""
            MATCH (e:Entity)
            OPTIONAL MATCH (e)-[r:RELATION]-()
            WITH e, count(r) AS degree
            RETURN avg(degree) AS avg_degree
        """).single()["avg_degree"] or 0.0
        
    density_before = relation_count_before / entity_count if entity_count > 0 else 0.0
    
    print(f"\nğŸ“Š å¼·åŒ–å‰ç‹€æ…‹ï¼š")
    print(f"  â€¢ å¯¦é«”ç¯€é»ï¼š{entity_count:,}")
    print(f"  â€¢ èªç¾©é—œä¿‚ï¼š{relation_count_before:,}")
    print(f"  â€¢ é—œä¿‚å¯†åº¦ï¼š{density_before:.3f}")
    print(f"  â€¢ å¹³å‡åº¦æ•¸ï¼š{avg_degree_before:.2f}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # éšæ®µä¸€ï¼šæª¢ç´¢ç¾æœ‰ Chunk å’Œ Entity
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ” éšæ®µä¸€ï¼šæª¢ç´¢ç¾æœ‰æ•¸æ“š...")
    
    with driver.session() as session:
        # æª¢ç´¢æ‰€æœ‰ Chunk
        chunks = session.run("""
            MATCH (c:Chunk {dataset: $dataset})
            RETURN c.id AS chunk_id, c.text AS chunk_text
            ORDER BY c.id
        """, dataset=dataset_id).data()
        
        # æª¢ç´¢æ‰€æœ‰ Entity
        entities = session.run("""
            MATCH (e:Entity)
            RETURN e.name AS entity_name
            ORDER BY e.name
        """).data()
        
    entity_list = [e["entity_name"] for e in entities]
    
    print(f"  âœ… æª¢ç´¢åˆ° {len(chunks)} å€‹ Chunks")
    print(f"  âœ… æª¢ç´¢åˆ° {len(entity_list)} å€‹ Entities")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # éšæ®µäºŒï¼šæ‰¹æ¬¡é—œä¿‚æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ¤– éšæ®µäºŒï¼šLLM é—œä¿‚æŠ½å–ï¼ˆæ‰¹æ¬¡è™•ç†ï¼Œbatch_size={batch_size}ï¼‰...")
    
    all_extracted_triples = []
    processed_count = 0
    
    # æ ¼å¼åŒ–å¯¦é«”åˆ—è¡¨ï¼ˆåªåšä¸€æ¬¡ï¼‰
    formatted_entity_list = format_entity_list_for_prompt(entity_list, max_entities_per_prompt)
    
    for i in range(0, len(chunks), batch_size):
        batch_chunks = chunks[i:i+batch_size]
        
        for chunk in batch_chunks:
            chunk_id = chunk["chunk_id"]
            chunk_text = chunk["chunk_text"]
            
            # æ§‹å»ºæç¤ºè©
            prompt = RELATION_ENHANCEMENT_PROMPT.format(
                entity_list=formatted_entity_list,
                chunk_text=chunk_text
            )
            
            # èª¿ç”¨ LLM
            try:
                response = client.chat(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    options={"temperature": temperature, "top_p": 0.9},
                )
                content = response.get("message", {}).get("content", "")
                
                # è§£æä¸‰å…ƒçµ„
                triples = parse_triples(content)
                
                # æ·»åŠ ä¾†æºä¿¡æ¯
                for triple in triples:
                    triple["source_chunk"] = chunk_id
                
                all_extracted_triples.extend(triples)
                processed_count += 1
                
                if processed_count % 5 == 0:
                    print(f"  â†³ å·²è™•ç† {processed_count}/{len(chunks)} Chunksï¼Œç´¯è¨ˆæå– {len(all_extracted_triples)} å€‹ä¸‰å…ƒçµ„")
                    
            except Exception as e:
                print(f"  âš ï¸  Chunk {chunk_id} æŠ½å–å¤±æ•—ï¼š{e}")
                continue
    
    print(f"\n  âœ… æŠ½å–å®Œæˆï¼šå…± {len(all_extracted_triples)} å€‹å€™é¸ä¸‰å…ƒçµ„")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # éšæ®µä¸‰ï¼šå¢é‡å¯«å…¥æ–°é—œä¿‚ï¼ˆMATCH + MERGEï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    print(f"\nğŸ’¾ éšæ®µä¸‰ï¼šå¢é‡å¯«å…¥æ–°é—œä¿‚ï¼ˆåƒ…é€£æ¥ç¾æœ‰å¯¦é«”ï¼‰...")
    
    new_relations_count = 0
    skipped_count = 0
    
    with driver.session() as session:
        for triple in all_extracted_triples:
            head = triple.get("head")
            relation = triple.get("relation")
            tail = triple.get("tail")
            source_chunk = triple.get("source_chunk")
            
            if not all([head, relation, tail]):
                skipped_count += 1
                continue
            
            # é—œéµï¼šä½¿ç”¨ MATCH + MERGEï¼ˆä¸å‰µå»ºæ–°å¯¦é«”ï¼‰
            result = session.run("""
                // 1. åŒ¹é…ç¾æœ‰çš„é ­å¯¦é«”å’Œå°¾å¯¦é«”
                MATCH (h:Entity {name: $head})
                MATCH (t:Entity {name: $tail})
                
                // 2. å¢é‡åˆä½µé—œä¿‚ï¼ˆåŸºæ–¼ head + type + tail å”¯ä¸€æ€§ï¼‰
                MERGE (h)-[r:RELATION {type: $relation}]->(t)
                ON CREATE SET 
                    r.chunks = [$source_chunk],
                    r.created_at = timestamp(),
                    r.confidence = 0.95,
                    r.enhanced = true
                ON MATCH SET 
                    r.chunks = CASE 
                        WHEN $source_chunk IN r.chunks THEN r.chunks 
                        ELSE r.chunks + $source_chunk 
                    END,
                    r.last_updated = timestamp()
                
                RETURN r.enhanced AS is_new
            """, head=head, tail=tail, relation=relation, source_chunk=source_chunk)
            
            record = result.single()
            if record and record.get("is_new"):
                new_relations_count += 1
    
    print(f"  âœ… æ–°å¢é—œä¿‚ï¼š{new_relations_count:,}")
    print(f"  âš ï¸  è·³éï¼ˆå¯¦é«”ä¸å­˜åœ¨æˆ–æ ¼å¼éŒ¯èª¤ï¼‰ï¼š{skipped_count:,}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # éšæ®µå››ï¼šè¨ˆç®—å¼·åŒ–å¾Œçš„ç‹€æ…‹
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with driver.session() as session:
        relation_count_after = session.run("MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt").single()["cnt"]
        
        avg_degree_after = session.run("""
            MATCH (e:Entity)
            OPTIONAL MATCH (e)-[r:RELATION]-()
            WITH e, count(r) AS degree
            RETURN avg(degree) AS avg_degree
        """).single()["avg_degree"] or 0.0
    
    density_after = relation_count_after / entity_count if entity_count > 0 else 0.0
    
    print(f"\nğŸ“Š å¼·åŒ–å¾Œç‹€æ…‹ï¼š")
    print(f"  â€¢ å¯¦é«”ç¯€é»ï¼š{entity_count:,} ï¼ˆç„¡è®ŠåŒ– âœ…ï¼‰")
    print(f"  â€¢ èªç¾©é—œä¿‚ï¼š{relation_count_after:,} ï¼ˆ+{relation_count_after - relation_count_before:,} âœ…ï¼‰")
    print(f"  â€¢ é—œä¿‚å¯†åº¦ï¼š{density_after:.3f} ï¼ˆå¾ {density_before:.3f} æå‡ {((density_after/density_before - 1) * 100):.1f}% âœ…ï¼‰")
    print(f"  â€¢ å¹³å‡åº¦æ•¸ï¼š{avg_degree_after:.2f} ï¼ˆå¾ {avg_degree_before:.2f} æå‡ {((avg_degree_after/avg_degree_before - 1) * 100):.1f}% âœ…ï¼‰")
    
    print("\n" + "="*70)
    print("âœ… é—œä¿‚å¼·åŒ–æµç¨‹å®Œæˆï¼")
    print("="*70)
    
    return {
        "new_relations": new_relations_count,
        "processed_chunks": processed_count,
        "density_before": density_before,
        "density_after": density_after,
        "avg_degree_before": avg_degree_before,
        "avg_degree_after": avg_degree_after,
        "entity_count": entity_count,
        "relation_count_before": relation_count_before,
        "relation_count_after": relation_count_after,
    }



# âœ… éœ€è¦ä» builder å¯¼å…¥ parse_triples å‡½æ•°
from typing import List, Dict, Any
from ollama import Client
from src.builder import parse_triples
