# src/optimizer.py
"""
Structure augmentation (Phase 3)
åœ–è­œå„ªåŒ–å™¨ï¼šè² è²¬å¢å¼·é€£é€šæ€§ã€å¯¦é«”å°é½Šèˆ‡åœ–è­œæ¸…ç†
"""

import json
import logging
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from ollama import Client
from src.utils import parse_triples

# è¨­å®š Logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ==============================================================================
# Prompt å®šç¾©
# ==============================================================================

RELATION_ENHANCEMENT_PROMPT = """
You are an expert knowledge graph engineer.
Task: Extract **implicit relationships** between the provided entities based on the context.

âš ï¸ **CRITICAL CONSTRAINT**: You can ONLY use entity names from the following list to construct triples.

## ğŸ“‹ Available Entity List
{entity_list}

## ğŸ“ Context
{chunk_text}

## ğŸ“¤ Output Format
Output ONLY a JSON array of triples:
[
  {{"head": "EntityA", "relation": "CAUSES", "tail": "EntityB"}}
]
"""

ENTITY_RESOLUTION_PROMPT = """
You are a data cleaning expert.
Task: Identify distinct entities that refer to the same concept (Synonyms) from the list below.
Focus on:
1. Plural forms (e.g., "Goat" and "Goats")
2. Abbreviations (e.g., "Vit A" and "Vitamin A")
3. Case sensitivity issues.

List: {entity_list}

Output ONLY a JSON list of pairs to merge. The 'primary' should be the most standard/complete name.
Example:
[
    {{"primary": "Vitamin A", "duplicate": "Vit A"}},
    {{"primary": "Goat", "duplicate": "goats"}}
]
If no duplicates found, return [].
"""

WEAK_ENTITY_INFERENCE_PROMPT = """
You are an expert knowledge graph reasoning agent.

Task: Infer potential relationships for a weakly-connected entity based on its existing connections and global context.

âš ï¸ **CRITICAL CONSTRAINT**: You can ONLY create relationships between entities from the PROVIDED ENTITY LIST.

## ğŸ“Œ Target Weak Entity
Name: {weak_entity}
Current Connections: {current_connections}

## ğŸ“‹ Available Entities for New Relations
{entity_list}

## ğŸ§  Reasoning Guidelines
1. **Based on current connections**: What other relationships can be inferred from the entity's existing neighbors?
2. **Domain knowledge**: Apply your domain expertise to suggest semantically valid relationships
3. **Transitive inference**: If A relates to B and B relates to C, might A relate to C?
4. **Avoid creating duplicate relationships** that already exist

## ğŸ“¤ Output Format
Output ONLY a JSON array (max 5 relationships):
[
  {{"head": "weak_entity_name", "relation": "RELATIONSHIP_TYPE", "tail": "EntityFromList"}},
  {{"head": "EntityFromList", "relation": "RELATIONSHIP_TYPE", "tail": "weak_entity_name"}}
]

Focus on HIGH-CONFIDENCE inferences only. If uncertain, return fewer relationships or [].
"""

# ğŸš€ æ–°å¢ï¼šå„ªåŒ–éçš„æ‰¹æ¬¡è™•ç† Promptï¼ˆä»¥ Chunk ç‚ºä¸­å¿ƒï¼‰
WEAK_LINK_BATCH_PROMPT = """
You are a Knowledge Graph Expert.
Task: Connect the following "Isolated Entities" to the rest of the concepts in the text.

## ğŸ“„ Context Text:
{text}

## ğŸ¯ Target Isolated Entities (Connect these!):
{entities}

## âš¡ Instructions:
1. For each Target Entity, find **explicit or implied** relationships connecting it to ANY other entity in the text.
2. The output must be valid JSON triples.
3. Use precise predicates (e.g., 'PART_OF', 'CAUSES', 'LOCATED_AT', 'HAS_SYMPTOM', 'TREATED_BY').
4. Focus on creating meaningful connections that integrate isolated entities into the knowledge graph.

## ğŸ“¤ Output JSON format:
[
  {{"head": "IsolatedEntity", "relation": "RELATION", "tail": "OtherEntity"}},
  {{"head": "OtherEntity", "relation": "RELATION", "tail": "IsolatedEntity"}}
]

Extract as many valid relationships as possible to maximize connectivity.
"""

HYPOTHETICAL_QUESTIONS_PROMPT = """
You are an expert in knowledge graph relation extraction.

Task: Re-extract relationships between entities in this text using **hypothetical question-driven reasoning**.

## ğŸ¯ Hypothetical Question Types
1. **Causal Questions**: Does X cause/lead to/result in Y? Does Y prevent/inhibit X?
2. **Compositional Questions**: Does X contain/include/consist of Y? Is Y a part/component of X?
3. **Functional Questions**: Does X use/require/depend on Y? Does X produce/generate Y?
4. **Hierarchical Questions**: Is X a type of Y? Does X belong to category Y?
5. **Comparative Questions**: How does X compare to Y? Is X similar to/different from Y?
6. **Temporal Questions**: Does X happen before/after/during Y?
7. **Spatial Questions**: Where is X located relative to Y?
8. **Attribute Questions**: What properties/characteristics does X have?

## ğŸ“‹ Entities in this Chunk
{entity_list}

## ğŸ“ Text Context
{chunk_text}

## ğŸ“¤ Output Format
Output ONLY a JSON array of triples:
[
  {{"head": "EntityA", "relation": "CAUSES", "tail": "EntityB"}},
  {{"head": "EntityA", "relation": "CONTAINS", "tail": "EntityC"}}
]

Rules:
1. Only extract relationships explicitly supported by the text
2. Use specific relationship types (not generic like "related" or "associated")
3. Extract as many valid relationships as possible between entities
4. Focus on maximizing the number of valid connections to create a dense knowledge network
"""


# ==============================================================================
# å„ªåŒ–å™¨é¡åˆ¥
# ==============================================================================

class GraphOptimizer:
    """
    åœ–è­œå„ªåŒ–æ§åˆ¶å™¨
    åŒ…å«ï¼šå¯¦é«”å°é½Šã€é—œä¿‚å¼·åŒ–ã€å­¤ç«‹é»æ¸…ç†
    
    ğŸš€ å„ªåŒ–ç‰ˆæœ¬ç‰¹æ€§ï¼š
    - æ‰¹æ¬¡è™•ç†ï¼šä»¥ Chunk ç‚ºå–®ä½æ‰¹é‡è™•ç†å¼±å¯¦é«”
    - ä¸¦è¡ŒåŸ·è¡Œï¼šä½¿ç”¨å¤šç·šç¨‹åŠ é€Ÿ LLM æ¨ç†
    - åŠŸèƒ½æ•´åˆï¼šåŒæ™‚å®Œæˆå¼±é€£æ¥ä¿®å¾©å’Œéš±æ€§é—œä¿‚æŒ–æ˜
    """
    def __init__(self, driver, client: Client, model: str, max_workers: int = 2):
        self.driver = driver
        self.client = client
        self.model = model
        # ä¸¦è¡Œåº¦è¨­å®šï¼ˆæ ¹æ“šæ‚¨çš„ç¡¬é«”èª¿æ•´ï¼‰
        # GPU æœ¬åœ°é‹è¡Œå»ºè­° 2-4ï¼ŒAPI æœå‹™å¯è¨­æ›´é«˜ï¼ˆå¦‚ 8-10ï¼‰
        self.max_workers = max_workers
        logging.info(f"GraphOptimizer initialized with {max_workers} workers")

    def run_optimization_pipeline(self, max_iterations: int = 1, dataset_id: str = "goat_kb_v1", use_accelerated: bool = True):
        """
        åŸ·è¡Œå®Œæ•´çš„ Phase 3 å„ªåŒ–æµç¨‹
        
        Args:
            max_iterations: å„ªåŒ–è¿­ä»£æ¬¡æ•¸
            dataset_id: è³‡æ–™é›†ID
            use_accelerated: æ˜¯å¦ä½¿ç”¨åŠ é€Ÿç‰ˆå¼±é€£æ¥æ¨ç†ï¼ˆé è¨­Trueï¼‰
        """
        print(f"\nâš¡ é–‹å§‹ Phase 3 åœ–è­œå„ªåŒ– (Max Iterations: {max_iterations})")
        print(f"   æ¨¡å¼ï¼š{'ğŸš€ åŠ é€Ÿç‰ˆ' if use_accelerated else 'æ¨™æº–ç‰ˆ'}")
        
        for i in range(max_iterations):
            print(f"\nğŸ”„ Iteration {i+1}/{max_iterations}")
            
            # 1. å¯¦é«”å°é½Š (å…ˆæ¸…ç†ï¼Œå†é€£æ¥)
            self.merge_synonym_entities()
            
            # 2. é—œä¿‚å¼·åŒ– (å¢åŠ é€£æ¥)
            self.enhance_connectivity(dataset_id)
            
            # 3. ğŸš€ å¼±é€£æ¥æ¨ç†ï¼ˆä½¿ç”¨åŠ é€Ÿç‰ˆæˆ–æ¨™æº–ç‰ˆï¼‰
            if use_accelerated:
                self.infer_weak_links_accelerated(degree_threshold=2)
            else:
                # å¦‚æœæ‚¨ä¿ç•™äº†èˆŠç‰ˆæ–¹æ³•ï¼Œå¯ä»¥åœ¨é€™è£¡èª¿ç”¨
                print("  âš ï¸  æ¨™æº–ç‰ˆå¼±é€£æ¥æ¨ç†å·²è¢«åŠ é€Ÿç‰ˆå–ä»£")
            
            # 4. æ¸…ç†å­¤ç«‹é» (æ‰“æƒæˆ°å ´)
            self.prune_isolated_nodes()
            
        print("\nâœ… Phase 3 å„ªåŒ–æµç¨‹å®Œæˆï¼")

    # --------------------------------------------------------------------------
    # 1. å¯¦é«”å°é½Š (Entity Resolution)
    # --------------------------------------------------------------------------
    def merge_synonym_entities(self):
        """
        ä½¿ç”¨ LLM è­˜åˆ¥ç›¸ä¼¼å¯¦é«”ä¸¦åœ¨ Neo4j ä¸­åˆä½µ
        """
        print("  ğŸ§© åŸ·è¡Œå¯¦é«”å°é½Š (Entity Resolution)...")
        with self.driver.session() as session:
            # æŠ“å–æ‰€æœ‰å¯¦é«”åç¨±
            entities = [r["name"] for r in session.run("MATCH (e:Entity) RETURN e.name AS name")]
        
        if not entities:
            print("    âš ï¸ ç„¡å¯¦é«”ï¼Œè·³é")
            return

        # ç°¡å–®åˆ†æ‰¹è™•ç†
        batch_size = 200
        merged_count = 0
        
        for i in range(0, len(entities), batch_size):
            batch = entities[i : i + batch_size]
            prompt = ENTITY_RESOLUTION_PROMPT.format(entity_list=batch)
            
            try:
                response = self.client.chat(
                    model=self.model, 
                    messages=[{"role": "user", "content": prompt}],
                    options={"temperature": 0.0}
                )
                content = response['message']['content'] if isinstance(response, dict) else ''
                
                # è§£æ JSON
                pairs = []
                try:
                    json_str = content[content.find('['):content.rfind(']')+1]
                    pairs = json.loads(json_str)
                except Exception:
                    pairs = []
                
                if not pairs:
                    continue

                with self.driver.session() as session:
                    for p in pairs:
                        primary = p.get("primary")
                        duplicate = p.get("duplicate")
                        
                        if primary and duplicate and primary != duplicate:
                            # æª¢æŸ¥å…©è€…æ˜¯å¦éƒ½å­˜åœ¨ï¼ˆå„ªåŒ–ï¼šé¿å…ç¬›å¡çˆ¾ç©ï¼‰
                            check_result = session.run("""
                                MATCH (p:Entity {name: $primary})
                                MATCH (d:Entity {name: $duplicate})
                                RETURN count(p) + count(d) as cnt
                            """, primary=primary, duplicate=duplicate).single()
                            
                            if not check_result or check_result["cnt"] < 2:
                                continue

                            # ä½¿ç”¨æ¨™æº– Cypher æ‰‹å‹•åˆä½µ (æ‹†åˆ†ç‚ºå¤šæ­¥é©Ÿï¼Œé¿å… NULL å•é¡Œ)
                            try:
                                # æ­¥é©Ÿ 1: è½‰ç§»å‡ºé‚Š (RELATION é—œä¿‚)
                                session.run("""
                                    MATCH (p:Entity {name: $primary})
                                    MATCH (d:Entity {name: $duplicate})
                                    MATCH (d)-[r:RELATION]->(target)
                                    WITH p, d, r, target, type(r) as rel_type, properties(r) as props
                                    MERGE (p)-[new_r:RELATION {type: rel_type}]->(target)
                                    ON CREATE SET new_r = props
                                """, primary=primary, duplicate=duplicate)
                                
                                # æ­¥é©Ÿ 2: è½‰ç§»å…¥é‚Š (RELATION é—œä¿‚)
                                session.run("""
                                    MATCH (p:Entity {name: $primary})
                                    MATCH (d:Entity {name: $duplicate})
                                    MATCH (source)-[r:RELATION]->(d)
                                    WITH p, d, r, source, type(r) as rel_type, properties(r) as props
                                    MERGE (source)-[new_r:RELATION {type: rel_type}]->(p)
                                    ON CREATE SET new_r = props
                                """, primary=primary, duplicate=duplicate)
                                
                                # æ­¥é©Ÿ 3: è½‰ç§» MENTIONS é—œä¿‚
                                session.run("""
                                    MATCH (p:Entity {name: $primary})
                                    MATCH (d:Entity {name: $duplicate})
                                    MATCH (c:Chunk)-[m:MENTIONS]->(d)
                                    MERGE (c)-[:MENTIONS]->(p)
                                """, primary=primary, duplicate=duplicate)
                                
                                # æ­¥é©Ÿ 4: åˆªé™¤èˆŠç¯€é»ï¼ˆæœƒè‡ªå‹•åˆªé™¤æ‰€æœ‰é—œä¿‚ï¼‰
                                session.run("""
                                    MATCH (d:Entity {name: $duplicate})
                                    DETACH DELETE d
                                """, duplicate=duplicate)
                                
                                merged_count += 1
                                print(f"    ğŸ”„ Merged: {duplicate} -> {primary}")
                            except Exception as e:
                                print(f"    âš ï¸ Merge error: {e}")

            except Exception as e:
                print(f"    âš ï¸ æ‰¹æ¬¡è™•ç†éŒ¯èª¤: {e}")
                continue
        
        print(f"    âœ… å·²åˆä½µ {merged_count} çµ„é‡è¤‡å¯¦é«”")

    # --------------------------------------------------------------------------
    # 2. é—œä¿‚å¼·åŒ– (Connectivity Enhancement)
    # --------------------------------------------------------------------------
    def enhance_connectivity(self, dataset_id: str):
        """é‡å°ç¾æœ‰ Chunk é€²è¡ŒäºŒæ¬¡é—œä¿‚æ¨ç†"""
        print("  ğŸ”— åŸ·è¡Œé—œä¿‚å¼·åŒ– (Connectivity Enhancement)...")
        
        with self.driver.session() as session:
            # ç²å–å¯¦é«”åˆ—è¡¨ä¾› Prompt ä½¿ç”¨
            entities_data = session.run("MATCH (e:Entity) RETURN e.name as name").data()
            entity_list = [e['name'] for e in entities_data]
            
            # ç²å– chunks
            chunks = session.run("""
                MATCH (c:Chunk {dataset: $dataset}) 
                RETURN c.id as id, c.text as text 
            """, dataset=dataset_id).data()

        if not chunks:
            print("    âš ï¸ ç„¡ Chunksï¼Œè·³é")
            return

        added_count = 0
        formatted_entities = str(entity_list[:500])  # æˆªæ–·ä»¥é˜² Prompt éé•·
        
        for chunk in chunks:
            prompt = RELATION_ENHANCEMENT_PROMPT.format(
                entity_list=formatted_entities,
                chunk_text=chunk['text']
            )
            
            try:
                response = self.client.chat(
                    model=self.model, 
                    messages=[{"role": "user", "content": prompt}],
                    options={"temperature": 0.0}
                )
                content = response['message']['content'] if isinstance(response, dict) else ''
                triples = parse_triples(content)
                
                with self.driver.session() as session:
                    for t in triples:
                        # åªé€£æ¥ç¾æœ‰å¯¦é«”ï¼ˆä¿®å¾©ï¼šåˆ†é–‹ MATCH é¿å…ç¬›å¡çˆ¾ç©ï¼‰
                        result = session.run("""
                            MATCH (h:Entity {name: $head})
                            MATCH (t:Entity {name: $tail})
                            MERGE (h)-[r:RELATION {type: $rel}]->(t)
                            ON CREATE SET r.enhanced = true, r.confidence = 0.8
                            RETURN r
                        """, head=t['head'], rel=t['relation'], tail=t['tail'])
                        
                        if result.single():
                            added_count += 1
            except Exception:
                continue
                
        print(f"    âœ… æ¨ç†ä¸¦æ–°å¢äº† {added_count} æ¢é—œä¿‚")

    # --------------------------------------------------------------------------
    # 3. å­¤ç«‹é»æ¸…ç† (Pruning)
    # --------------------------------------------------------------------------
    def prune_isolated_nodes(self):
        """åˆªé™¤æ²’æœ‰ä»»ä½•é—œä¿‚çš„å­¤ç«‹ Entity ç¯€é»"""
        print("  âœ‚ï¸  åŸ·è¡Œå­¤ç«‹é»æ¸…ç† (Pruning)...")
        with self.driver.session() as session:
            # åˆªé™¤æ²’æœ‰ RELATION ä¸”æ²’æœ‰ MENTIONS çš„å¯¦é«” (å®Œå…¨å­¤ç«‹)
            result = session.run("""
                MATCH (e:Entity)
                WHERE NOT (e)--()
                DELETE e
                RETURN count(e) as cnt
            """)
            record = result.single()
            cnt = record["cnt"] if record else 0
            
            print(f"    âœ… å·²åˆªé™¤ {cnt} å€‹å®Œå…¨å­¤ç«‹å¯¦é«”")

    # --------------------------------------------------------------------------
    # ğŸš€ æ–°å¢ï¼šåŠ é€Ÿç‰ˆå¼±é€£æ¥æ¨ç† (Context-Aware Batching + Parallel Execution)
    # --------------------------------------------------------------------------
    def infer_weak_links_accelerated(self, degree_threshold: int = 2):
        """
        ğŸš€ åŠ é€Ÿç‰ˆï¼šå¼±é€£æ¥æ¨ç† (æ•´åˆäº†ä¸Šä¸‹æ–‡æ‰¹æ¬¡è™•ç†èˆ‡ä¸¦è¡ŒåŸ·è¡Œ)
        
        æ ¸å¿ƒå„ªåŒ–ï¼š
        1. æ‰¹æ¬¡è™•ç†ï¼šä»¥ Chunk ç‚ºå–®ä½ï¼Œä¸€æ¬¡è™•ç†å¤šå€‹å¼±å¯¦é«”
        2. ä¸¦è¡ŒåŸ·è¡Œï¼šä½¿ç”¨ ThreadPoolExecutor åŒæ™‚è™•ç†å¤šå€‹ Chunks
        3. åŠŸèƒ½æ•´åˆï¼šåŒæ™‚å®Œæˆå¼±é€£æ¥ä¿®å¾©å’Œéš±æ€§é—œä¿‚æŒ–æ˜
        
        Args:
            degree_threshold: é€£æ¥æ•¸é–¾å€¼ï¼Œä½æ–¼æ­¤å€¼è¦–ç‚ºå¼±å¯¦é«”ï¼ˆé è¨­ 2ï¼‰
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ å•Ÿå‹•åŠ é€Ÿç‰ˆåœ–è­œæ“´å¢ (Target: Weak Entities < {degree_threshold} links)")
        print(f"   ç­–ç•¥ï¼šContext-Aware Batching + Parallel Execution")
        print(f"   ä¸¦è¡Œåº¦ï¼š{self.max_workers} workers")
        print(f"{'='*60}")

        # 1. æŠ“å–è³‡æ–™ï¼šæ‰¾å‡ºã€ŒåŒ…å«å¼±å¯¦é«”ã€çš„ Chunksï¼Œä¸¦å°‡å¼±å¯¦é«”æŒ‰ Chunk åˆ†çµ„
        # é€™å¥ Cypher éå¸¸é—œéµï¼Œå®ƒç›´æ¥æŠŠå·¥ä½œé‡æŒ‰ Chunk åˆ†å¥½äº†
        fetch_query = """
        MATCH (e:Entity)
        WHERE size((e)--()) < $threshold
        MATCH (e)<-[:MENTIONS]-(c:Chunk)
        WITH c, collect(DISTINCT e.name) AS weak_entities
        WHERE size(weak_entities) > 0
        RETURN c.id AS chunk_id, c.text AS text, weak_entities
        """
        
        with self.driver.session() as session:
            result = session.run(fetch_query, threshold=degree_threshold)
            tasks = [record.data() for record in result]

        if not tasks:
            print("ğŸ“Š æœªç™¼ç¾éœ€è¦è™•ç†çš„å¼±å¯¦é«”ï¼Œè·³éå„ªåŒ–")
            return

        print(f"ğŸ“Š æƒæå®Œæˆï¼šå…± {len(tasks)} å€‹ Chunks åŒ…å«å¼±é€£æ¥å¯¦é«”ï¼Œæº–å‚™ä¸¦è¡Œè™•ç†...")
        logging.info(f"Found {len(tasks)} chunks with weak entities")

        total_new_relations = 0
        
        # 2. å®šç¾©å–®å€‹ä»»å‹™çš„è™•ç†å‡½æ•¸ (çµ¦åŸ·è¡Œç·’ç”¨)
        def process_chunk_task(task):
            chunk_id = task['chunk_id']
            text = task['text']
            weak_entities = task['weak_entities']
            
            # å¦‚æœè©² Chunk çš„å¼±å¯¦é«”å¤ªå¤šï¼Œå¯ä»¥æˆªæ–·ä»¥å… Prompt å¤ªé•·
            # å»ºè­°æœ€å¤šè™•ç† 20 å€‹å¯¦é«”/Chunk
            if len(weak_entities) > 20:
                weak_entities = weak_entities[:20]
                logging.warning(f"Chunk {chunk_id} has too many weak entities, truncated to 20")
            
            target_list_str = ", ".join(weak_entities)
            
            prompt = WEAK_LINK_BATCH_PROMPT.format(text=text, entities=target_list_str)
            
            try:
                response = self.client.chat(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    options={"temperature": 0.1}  # ä½éš¨æ©Ÿæ€§ï¼Œæ±‚ç©©
                )
                content = response['message']['content'] if isinstance(response, dict) else ''
                triples = parse_triples(content)
                logging.debug(f"Chunk {chunk_id}: extracted {len(triples)} triples")
                return triples
            except Exception as e:
                # éœé»˜å¤±æ•—æˆ–è¨˜éŒ„ Logï¼Œä¸è¦å¡ä½ä¸»æµç¨‹
                logging.error(f"Error processing chunk {chunk_id}: {e}")
                return []

        # 3. ä½¿ç”¨ ThreadPoolExecutor ä¸¦è¡ŒåŸ·è¡Œ
        new_triples_batch = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_chunk = {executor.submit(process_chunk_task, task): task for task in tasks}
            
            # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
            for future in tqdm(as_completed(future_to_chunk), total=len(tasks), desc="ğŸ”„ Processing Chunks"):
                triples = future.result()
                if triples:
                    new_triples_batch.extend(triples)

        # 4. æ‰¹æ¬¡å¯«å…¥è³‡æ–™åº« (æ¸›å°‘ DB I/O)
        if new_triples_batch:
            print(f"\nğŸ’¾ æ­£åœ¨å°‡ {len(new_triples_batch)} æ¢æ–°é—œä¿‚å¯«å…¥ Neo4j...")
            total_new_relations = self._batch_insert_relations(new_triples_batch)
            print(f"   âœ… æˆåŠŸå¯«å…¥ {total_new_relations} æ¢é—œä¿‚")
        else:
            print("\nâš ï¸  æœªç”¢ç”Ÿæ–°é—œä¿‚")

        print(f"\nâœ… å„ªåŒ–å®Œæˆï¼æ–°å¢äº† {total_new_relations} æ¢é—œä¿‚ï¼Œå¼·åŒ–äº†å¼±å¯¦é«”é€£æ¥ã€‚")
        logging.info(f"Weak link inference completed: {total_new_relations} new relations added")

    def _batch_insert_relations(self, triples: List[Dict], batch_size: int = 1000) -> int:
        """
        è¼”åŠ©å‡½æ•¸ï¼šåˆ†æ‰¹å¯«å…¥é—œä¿‚ï¼Œé¿å…è¨˜æ†¶é«”æº¢å‡º
        
        Args:
            triples: ä¸‰å…ƒçµ„åˆ—è¡¨
            batch_size: æ¯æ‰¹å¯«å…¥çš„æ•¸é‡
            
        Returns:
            æˆåŠŸå¯«å…¥çš„é—œä¿‚æ•¸é‡
        """
        inserted_count = 0
        
        with self.driver.session() as session:
            for i in range(0, len(triples), batch_size):
                batch = triples[i:i+batch_size]
                
                for item in batch:
                    try:
                        # æ¸…ç†é—œä¿‚åç¨±ï¼ˆè½‰ç‚ºå¤§å¯«ä¸¦æ›¿æ›ç©ºæ ¼ï¼‰
                        rel_type = item.get('relation', 'RELATED_TO').upper().replace(" ", "_").replace("-", "_")
                        if not rel_type or rel_type == "":
                            rel_type = "RELATED_TO"
                        
                        # å‹•æ…‹å‰µå»ºé—œä¿‚ï¼ˆä½¿ç”¨ Cypher å­—ä¸²æ’å€¼ï¼Œéœ€è¬¹æ…è™•ç†ï¼‰
                        # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨åƒæ•¸åŒ–æŸ¥è©¢ä¿è­‰å®‰å…¨æ€§
                        cypher = f"""
                        MATCH (h:Entity {{name: $head}})
                        MATCH (t:Entity {{name: $tail}})
                        WHERE h <> t
                        MERGE (h)-[r:`{rel_type}`]->(t)
                        ON CREATE SET r.source = 'ai_inference', r.confidence = 0.8
                        RETURN r
                        """
                        result = session.run(cypher, head=item['head'], tail=item['tail'])
                        
                        if result.single():
                            inserted_count += 1
                            
                    except Exception as e:
                        # è·³éå¤±æ•—çš„é—œä¿‚ï¼Œç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹
                        logging.debug(f"Failed to insert relation {item}: {e}")
                        continue
        
        return inserted_count

    # --------------------------------------------------------------------------
    # 6. è³ªé‡å•é¡Œä¿®å¾© (Quality Issue Fixes)
    # --------------------------------------------------------------------------
    def fix_quality_issues(self) -> Dict[str, int]:
        """
        ä¿®å¾©åœ–è­œä¸­çš„è³ªé‡å•é¡Œ
        
        ä¿®å¾©é …ç›®ï¼š
        1. è‡ªç’°é—œä¿‚ï¼ˆå¯¦é«”æŒ‡å‘è‡ªå·±ï¼‰
        2. é‡è¤‡é—œä¿‚ï¼ˆç›¸åŒ head + type + tailï¼‰
        3. ç¼ºå°‘ä¾†æºæ¨™è¨˜çš„é—œä¿‚ï¼ˆr.chunks ç‚ºç©ºï¼‰
        
        Returns:
            åŒ…å«ä¿®å¾©çµ±è¨ˆçš„å­—å…¸
        """
        print("\nğŸ”§ é–‹å§‹è³ªé‡å•é¡Œä¿®å¾©...")
        print("="*70)
        
        results = {
            'self_loops_removed': 0,
            'duplicate_relations_merged': 0,
            'empty_chunks_fixed': 0
        }
        
        with self.driver.session() as session:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ä¿®å¾© 1ï¼šç§»é™¤è‡ªç’°é—œä¿‚
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            print("\nğŸ” ä¿®å¾© 1ï¼šç§»é™¤è‡ªç’°é—œä¿‚")
            print("-"*70)
            
            self_loops_count = session.run("""
                MATCH (e:Entity)-[r:RELATION]->(e)
                RETURN count(r) AS cnt
            """).single()["cnt"]
            
            if self_loops_count > 0:
                print(f"  ç™¼ç¾ {self_loops_count} å€‹è‡ªç’°é—œä¿‚ï¼Œæ­£åœ¨ç§»é™¤...")
                result = session.run("""
                    MATCH (e:Entity)-[r:RELATION]->(e)
                    DELETE r
                    RETURN count(r) AS deleted
                """)
                record = result.single()
                deleted = record["deleted"] if record else 0
                results['self_loops_removed'] = deleted
                print(f"  âœ… å·²ç§»é™¤ {deleted} å€‹è‡ªç’°é—œä¿‚")
            else:
                print("  âœ… æœªç™¼ç¾è‡ªç’°é—œä¿‚")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ä¿®å¾© 2ï¼šåˆä½µé‡è¤‡é—œä¿‚
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            print(f"\nğŸ” ä¿®å¾© 2ï¼šåˆä½µé‡è¤‡é—œä¿‚")
            print("-"*70)
            
            # æ‰¾å‡ºé‡è¤‡é—œä¿‚çµ„
            duplicate_groups = session.run("""
                MATCH (h:Entity)-[r:RELATION]->(t:Entity)
                WITH h, t, r.type AS rel_type, collect(r) AS rels
                WHERE size(rels) > 1
                RETURN h.name AS head, t.name AS tail, rel_type, size(rels) AS dup_count
            """).data()
            
            if duplicate_groups:
                print(f"  ç™¼ç¾ {len(duplicate_groups)} çµ„é‡è¤‡é—œä¿‚ï¼Œæ­£åœ¨åˆä½µ...")
                
                for group in duplicate_groups:
                    head = group['head']
                    tail = group['tail']
                    rel_type = group['rel_type']
                    
                    # åˆä½µç­–ç•¥ï¼šä¿ç•™ç¬¬ä¸€å€‹é—œä¿‚ï¼Œåˆä½µ chunks å±¬æ€§ï¼Œåˆªé™¤å…¶é¤˜é—œä¿‚
                    session.run("""
                        MATCH (h:Entity {name: $head})-[r:RELATION {type: $rel_type}]->(t:Entity {name: $tail})
                        WITH h, t, $rel_type AS rel_type, collect(r) AS rels
                        WHERE size(rels) > 1
                        
                        // æ”¶é›†æ‰€æœ‰ chunks
                        WITH h, t, rels, 
                             [rel IN rels | COALESCE(rel.chunks, [])] AS all_chunks_list
                        WITH h, t, rels, 
                             reduce(acc = [], chunks IN all_chunks_list | acc + chunks) AS merged_chunks
                        
                        // ä¿ç•™ç¬¬ä¸€å€‹é—œä¿‚ï¼Œæ›´æ–°å…¶ chunks
                        WITH rels[0] AS keep_rel, rels[1..] AS delete_rels, merged_chunks
                        SET keep_rel.chunks = merged_chunks
                        
                        // åˆªé™¤å…¶é¤˜é—œä¿‚
                        FOREACH (r IN delete_rels | DELETE r)
                        
                        RETURN size(delete_rels) AS deleted
                    """, head=head, tail=tail, rel_type=rel_type)
                
                results['duplicate_relations_merged'] = len(duplicate_groups)
                print(f"  âœ… å·²åˆä½µ {len(duplicate_groups)} çµ„é‡è¤‡é—œä¿‚")
            else:
                print("  âœ… æœªç™¼ç¾é‡è¤‡é—œä¿‚")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ä¿®å¾© 3ï¼šä¿®å¾©ç¼ºå°‘ä¾†æºæ¨™è¨˜çš„é—œä¿‚ï¼ˆå¼·åŠ›é›™ç­–ç•¥æ¨¡å¼ï¼‰
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            print("\nğŸ” ä¿®å¾© 3ï¼šä¿®å¾©ç¼ºå°‘ä¾†æºæ¨™è¨˜çš„é—œä¿‚ï¼ˆå¼·åŠ›æ¨¡å¼ï¼‰")
            print("-" * 70)
            
            # å…ˆæª¢æŸ¥æœ‰å¤šå°‘éœ€è¦ä¿®å¾©
            total_empty = session.run("""
                MATCH ()-[r:RELATION]->() 
                WHERE r.chunks IS NULL OR size(r.chunks) = 0 
                RETURN count(r) as cnt
            """).single()["cnt"]
            print(f"  ç™¼ç¾ {total_empty:,} å€‹é—œä¿‚ç¼ºå°‘ä¾†æºæ¨™è¨˜")
            
            if total_empty > 0:
                # ç­–ç•¥ Aï¼šå„ªå…ˆæ‰¾ã€Œå…±åŒã€Chunksï¼ˆç²¾æº–æ¨¡å¼ï¼‰
                print(f"\n  ğŸ”¹ ç­–ç•¥ Aï¼šæŸ¥æ‰¾é ­å°¾å¯¦é«”å…±åŒå‡ºç¾çš„ Chunksï¼ˆç²¾æº–æ¨¡å¼ï¼‰...")
                strategy_a = session.run("""
                    MATCH (h:Entity)-[r:RELATION]->(t:Entity)
                    WHERE r.chunks IS NULL OR size(r.chunks) = 0
                    MATCH (c:Chunk)-[:MENTIONS]->(h)
                    MATCH (c)-[:MENTIONS]->(t)
                    WITH r, collect(DISTINCT c.id) AS common_chunks
                    WHERE size(common_chunks) > 0
                    SET r.chunks = common_chunks
                    RETURN count(r) AS cnt
                """).single()["cnt"]
                print(f"     âœ… ç­–ç•¥ A ä¿®å¾©äº†: {strategy_a:,} å€‹")
                
                # ç­–ç•¥ Bï¼šç¹¼æ‰¿é ­å°¾å¯¦é«”çš„æ‰€æœ‰ä¾†æºï¼ˆå¯¬é¬†æ¨¡å¼ï¼Œç¢ºä¿ä¸æ–·éˆï¼‰
                print(f"\n  ğŸ”¹ ç­–ç•¥ Bï¼šç¹¼æ‰¿é ­å°¾å¯¦é«”çš„æ‰€æœ‰ä¾†æºï¼ˆå¯¬é¬†æ¨¡å¼ï¼‰...")
                strategy_b = session.run("""
                    MATCH (h:Entity)-[r:RELATION]->(t:Entity)
                    WHERE r.chunks IS NULL OR size(r.chunks) = 0
                    
                    // æ”¶é›†é ­å¯¦é«”çš„ä¾†æº
                    OPTIONAL MATCH (c1:Chunk)-[:MENTIONS]->(h)
                    WITH r, t, collect(DISTINCT c1.id) AS h_chunks
                    
                    // æ”¶é›†å°¾å¯¦é«”çš„ä¾†æº
                    OPTIONAL MATCH (c2:Chunk)-[:MENTIONS]->(t)
                    WITH r, h_chunks, collect(DISTINCT c2.id) AS t_chunks
                    
                    // åˆä½µå…©è€…ä¸¦å»é‡
                    WITH r, h_chunks + t_chunks AS all_chunks
                    WHERE size(all_chunks) > 0
                    
                    // æ‰‹å‹•å»é‡ï¼ˆä¸ä¾è³´ APOCï¼‰
                    WITH r, [x IN all_chunks WHERE x IS NOT NULL] AS filtered_chunks
                    WITH r, reduce(s = [], x IN filtered_chunks | 
                        CASE WHEN x IN s THEN s ELSE s + [x] END
                    ) AS unique_chunks
                    
                    SET r.chunks = unique_chunks
                    RETURN count(r) AS cnt
                """).single()["cnt"]
                print(f"     âœ… ç­–ç•¥ B ä¿®å¾©äº†: {strategy_b:,} å€‹")
                
                results['empty_chunks_fixed'] = strategy_a + strategy_b
                
                # å†æ¬¡æª¢æŸ¥æ˜¯å¦é‚„æœ‰ç„¡æ³•ä¿®å¾©çš„
                remaining = session.run("""
                    MATCH ()-[r:RELATION]->() 
                    WHERE r.chunks IS NULL OR size(r.chunks) = 0 
                    RETURN count(r) AS cnt
                """).single()["cnt"]
                
                print(f"\n  ğŸ“Š ä¿®å¾©çµ±è¨ˆï¼š")
                print(f"     â€¢ ä¿®å¾©å‰ï¼š{total_empty:,} å€‹")
                print(f"     â€¢ ä¿®å¾©å¾Œï¼š{remaining:,} å€‹")
                print(f"     â€¢ æˆåŠŸä¿®å¾©ï¼š{total_empty - remaining:,} å€‹ ({(total_empty - remaining) / total_empty * 100:.1f}%)")
                
                if remaining > 0:
                    print(f"\n  âš ï¸  ä»æœ‰ {remaining:,} å€‹é—œä¿‚ç„¡æ³•ä¿®å¾©")
                    print(f"     ï¼ˆå¯èƒ½æ˜¯æ¨ç†é—œä¿‚ï¼Œä¸”é ­å°¾å¯¦é«”éƒ½æ˜¯å­¤å…’å¯¦é«”ï¼‰")
                else:
                    print(f"\n  âœ… æ‰€æœ‰é—œä¿‚éƒ½å·²æˆåŠŸè£œå……ä¾†æºæ¨™è¨˜ï¼")
            else:
                print("  âœ… æ‰€æœ‰é—œä¿‚éƒ½æœ‰ä¾†æºæ¨™è¨˜ï¼Œç„¡éœ€ä¿®å¾©")
        
        print(f"\n{'='*70}")
        print(f"âœ… è³ªé‡å•é¡Œä¿®å¾©å®Œæˆ")
        print(f"  â€¢ ç§»é™¤è‡ªç’°é—œä¿‚ï¼š{results['self_loops_removed']}")
        print(f"  â€¢ åˆä½µé‡è¤‡é—œä¿‚ï¼š{results['duplicate_relations_merged']}")
        print(f"  â€¢ ä¿®å¾©ä¾†æºæ¨™è¨˜ï¼š{results['empty_chunks_fixed']}")
        print(f"{'='*70}")
        
        return results

    # --------------------------------------------------------------------------
    # 4. å¼±é€£æ¥å¯¦é«”å…¨å±€é—œä¿‚æ¨ç† (Weak Entity Augmentation)
    # --------------------------------------------------------------------------
    def infer_global_relations(
        self, 
        min_degree: int = 1, 
        max_degree: int = 3, 
        max_inferences_per_entity: int = 5,
        batch_size: int = 10
    ) -> Dict[str, Any]:
        """
        é‡å°å¼±é€£æ¥å¯¦é«”ï¼ˆåº¦æ•¸ 1-3ï¼‰é€²è¡Œå…¨å±€é—œä¿‚æ¨ç†
        
        æ ¸å¿ƒåŸå‰‡ï¼š
        1. åªé‡å°ç¾æœ‰çš„å¼±é€£æ¥å¯¦é«”é€²è¡Œæ“´å¢
        2. åªåœ¨ç¾æœ‰å¯¦é«”ä¹‹é–“å»ºç«‹æ–°é—œä¿‚ï¼ˆMATCH + MERGEï¼Œä¸ CREATEï¼‰
        3. åŸºæ–¼å¯¦é«”çš„é„°å±…ä¸Šä¸‹æ–‡é€²è¡Œæ¨ç†
        
        Args:
            min_degree: æœ€å°åº¦æ•¸ï¼ˆåŒ…å«ï¼‰
            max_degree: æœ€å¤§åº¦æ•¸ï¼ˆåŒ…å«ï¼‰
            max_inferences_per_entity: æ¯å€‹å¯¦é«”æœ€å¤šæ¨ç†å¹¾æ¢é—œä¿‚
            batch_size: æ‰¹æ¬¡è™•ç†å¤§å°
            
        Returns:
            åŒ…å«çµ±è¨ˆä¿¡æ¯çš„å­—å…¸
        """
        print(f"\nğŸ§  é–‹å§‹å¼±é€£æ¥å¯¦é«”å…¨å±€é—œä¿‚æ¨ç†")
        print(f"  ç›®æ¨™ï¼šé‡å°å¼±é€£æ¥å¯¦é«”ï¼ˆåº¦æ•¸ {min_degree}-{max_degree}ï¼‰æ¨ç†æ–°é—œä¿‚")
        print("="*70)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # éšæ®µ 1ï¼šè­˜åˆ¥å¼±é€£æ¥å¯¦é«”
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\nğŸ“Š éšæ®µ 1ï¼šè­˜åˆ¥å¼±é€£æ¥å¯¦é«”...")
        
        with self.driver.session() as session:
            # çµ±è¨ˆå¼·åŒ–å‰ç‹€æ…‹
            stats_before = session.run("""
                MATCH (e:Entity)
                WITH count(e) AS total_entities
                MATCH ()-[r:RELATION]->()
                RETURN total_entities, count(r) AS total_relations,
                       toFloat(count(r)) / total_entities AS density
            """).single()
            
            density_before = stats_before['density'] if stats_before else 0.0
            total_entities = stats_before['total_entities'] if stats_before else 0
            
            # è­˜åˆ¥å¼±é€£æ¥å¯¦é«”
            weak_entities = session.run(f"""
                MATCH (e:Entity)
                OPTIONAL MATCH (e)-[r:RELATION]-()
                WITH e, count(DISTINCT r) AS degree
                WHERE degree >= {min_degree} AND degree <= {max_degree}
                RETURN e.name AS entity_name, degree
                ORDER BY degree ASC
            """).data()
            
            print(f"  æ‰¾åˆ° {len(weak_entities)} å€‹å¼±é€£æ¥å¯¦é«”ï¼ˆåº¦æ•¸ {min_degree}-{max_degree}ï¼‰")
            
            if not weak_entities:
                print("  âš ï¸  ç„¡ç¬¦åˆæ¢ä»¶çš„å¼±é€£æ¥å¯¦é«”")
                return {
                    'processed_entities': 0,
                    'inferred_relations': 0,
                    'density_before': density_before,
                    'density_after': density_before
                }
            
            # ç²å–æ‰€æœ‰å¯¦é«”åˆ—è¡¨ï¼ˆç”¨æ–¼ Promptï¼‰
            all_entities = [e["entity_name"] for e in session.run(
                "MATCH (e:Entity) RETURN e.name AS entity_name"
            ).data()]
            
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # éšæ®µ 2ï¼šå°å¼±é€£æ¥å¯¦é«”é€²è¡Œé—œä¿‚æ¨ç†
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\nğŸ”„ éšæ®µ 2ï¼šå°å¼±é€£æ¥å¯¦é«”é€²è¡Œé—œä¿‚æ¨ç†...")
        
        processed_count = 0
        total_inferred = 0
        
        for idx, entity_data in enumerate(weak_entities):
            entity_name = entity_data['entity_name']
            current_degree = entity_data['degree']
            
            # ç²å–è©²å¯¦é«”çš„ç¾æœ‰é€£æ¥
            with self.driver.session() as session:
                current_connections = session.run("""
                    MATCH (e:Entity {name: $name})-[r:RELATION]-(neighbor:Entity)
                    RETURN type(r) AS rel_type, neighbor.name AS neighbor_name
                    LIMIT 20
                """, name=entity_name).data()
            
            # æ ¼å¼åŒ–ç•¶å‰é€£æ¥ä¿¡æ¯
            connections_str = "\n".join([
                f"  - {conn['rel_type']} -> {conn['neighbor_name']}" 
                for conn in current_connections
            ]) if current_connections else "  (No current connections)"
            
            # æ ¼å¼åŒ–å¯¦é«”åˆ—è¡¨ï¼ˆé™åˆ¶é•·åº¦ï¼‰
            entity_list_str = ", ".join(all_entities[:300])
            
            # æ§‹å»ºæç¤ºè©
            prompt = WEAK_ENTITY_INFERENCE_PROMPT.format(
                weak_entity=entity_name,
                current_connections=connections_str,
                entity_list=entity_list_str
            )
            
            # èª¿ç”¨ LLM æ¨ç†
            try:
                response = self.client.chat(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    options={"temperature": 0.2, "top_p": 0.9}  # ç¨é«˜æº«åº¦å…è¨±æ¨ç†
                )
                content = response.get("message", {}).get("content", "") if isinstance(response, dict) else ""
                
                # è§£æä¸‰å…ƒçµ„
                inferred_triples = parse_triples(content)
                
                # é™åˆ¶æ¨ç†æ•¸é‡
                inferred_triples = inferred_triples[:max_inferences_per_entity]
                
                # å¯«å…¥æ–°é—œä¿‚ï¼ˆåªé€£æ¥ç¾æœ‰å¯¦é«”ï¼‰
                with self.driver.session() as session:
                    for triple in inferred_triples:
                        head = triple.get("head")
                        relation = triple.get("relation")
                        tail = triple.get("tail")
                        
                        if not all([head, relation, tail]):
                            continue
                        
                        # ä½¿ç”¨ MATCH + MERGE ç¢ºä¿ä¸å‰µå»ºæ–°å¯¦é«”
                        result = session.run("""
                            MATCH (h:Entity {name: $head})
                            MATCH (t:Entity {name: $tail})
                            MERGE (h)-[r:RELATION {type: $relation}]->(t)
                            ON CREATE SET r.inferred = true, r.confidence = 0.75
                            RETURN count(r) AS created
                        """, head=head, relation=relation, tail=tail)
                        
                        record = result.single()
                        if record and record['created'] > 0:
                            total_inferred += 1
                
                processed_count += 1
                
                if (idx + 1) % batch_size == 0:
                    print(f"  â†³ å·²è™•ç† {processed_count}/{len(weak_entities)} å€‹å¯¦é«”ï¼Œæ¨ç† {total_inferred} æ¢é—œä¿‚")
                    
            except Exception as e:
                print(f"  âš ï¸  Entity {entity_name} æ¨ç†å¤±æ•—ï¼š{e}")
                continue
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # éšæ®µ 3ï¼šçµ±è¨ˆçµæœ
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        with self.driver.session() as session:
            stats_after = session.run("""
                MATCH (e:Entity)
                WITH count(e) AS total_entities
                MATCH ()-[r:RELATION]->()
                RETURN total_entities, count(r) AS total_relations,
                       toFloat(count(r)) / total_entities AS density
            """).single()
            
            density_after = stats_after['density'] if stats_after else 0.0
        
        print(f"\n{'='*70}")
        print(f"âœ… å¼±é€£æ¥å¯¦é«”å…¨å±€é—œä¿‚æ¨ç†å®Œæˆ")
        print(f"  â€¢ è™•ç†å¯¦é«”æ•¸ï¼š{processed_count}")
        print(f"  â€¢ æ¨ç†é—œä¿‚æ•¸ï¼š{total_inferred}")
        print(f"  â€¢ å¯†åº¦è®ŠåŒ–ï¼š{density_before:.3f} â†’ {density_after:.3f} (+{density_after-density_before:.3f})")
        print(f"{'='*70}")
        
        return {
            'processed_entities': processed_count,
            'inferred_relations': total_inferred,
            'density_before': density_before,
            'density_after': density_after
        }

    # --------------------------------------------------------------------------
    # 5. å‡è¨­æ€§å•é¡Œé—œä¿‚å¯†é›†åŒ– (Densification)
    # --------------------------------------------------------------------------
    def densify_relations_with_questions(
        self,
        dataset_id: str,
        target_chunks: int = 10000,
        temperature: float = 0.0
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å‡è¨­æ€§å•é¡Œæ³•å°ä½å¯†åº¦ Chunks é‡æ–°æŠ½å–é—œä¿‚
        
        ç­–ç•¥ï¼š
        1. è­˜åˆ¥é—œä¿‚å¯†åº¦è¼ƒä½çš„ Chunksï¼ˆå¯¦é«”é–“é€£æ¥ä¸è¶³ï¼‰
        2. ä½¿ç”¨å‡è¨­æ€§å•é¡Œå¼•å° LLM ç™¼ç¾æ›´å¤šéš±å«é—œä¿‚
        3. åªé€£æ¥å·²å­˜åœ¨çš„å¯¦é«”ï¼ˆMATCH + MERGEï¼‰
        
        Args:
            dataset_id: æ•¸æ“šé›† ID
            target_chunks: è¦è™•ç†çš„ Chunk æ•¸é‡
            temperature: LLM æº«åº¦
            
        Returns:
            åŒ…å«çµ±è¨ˆä¿¡æ¯çš„å­—å…¸
        """
        print(f"\nğŸ’¡ é–‹å§‹å‡è¨­æ€§å•é¡Œé—œä¿‚å¯†é›†åŒ–")
        print(f"  ç›®æ¨™ Chunks æ•¸ï¼š{target_chunks}")
        print(f"  æŠ½å–æ¨¡å‹ï¼š{self.model}")
        print("="*70)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # éšæ®µ 1ï¼šé¸æ“‡ä½å¯†åº¦ Chunks
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\nğŸ“Š éšæ®µ 1ï¼šé¸æ“‡ä½å¯†åº¦ Chunks...")
        
        with self.driver.session() as session:
            # è¨˜éŒ„åˆå§‹ç‹€æ…‹
            initial_stats = session.run("""
                MATCH (e:Entity)
                WITH count(e) AS entities
                MATCH ()-[r:RELATION]->()
                RETURN entities, count(r) AS relations,
                       toFloat(count(r)) / entities AS density
            """).single()
            
            density_before = initial_stats['density'] if initial_stats else 0.0
            
            # é¸æ“‡ä½å¯†åº¦ Chunksï¼ˆå¯¦é«”æ•¸ >= 3ï¼Œä½†é€£æ¥åº¦ < 30%ï¼‰
            low_density_chunks = session.run(f"""
                MATCH (c:Chunk {{dataset: $dataset}})-[:MENTIONS]->(e:Entity)
                WITH c, collect(DISTINCT e.name) AS entities, count(DISTINCT e) AS entity_count
                WHERE entity_count >= 3
                
                // è¨ˆç®—è©² Chunk ä¸­å¯¦é«”é–“çš„é—œä¿‚æ•¸
                MATCH (c)-[:MENTIONS]->(e1:Entity)
                MATCH (c)-[:MENTIONS]->(e2:Entity)
                WHERE e1 <> e2
                OPTIONAL MATCH (e1)-[r:RELATION]-(e2)
                WITH c, entities, entity_count,
                     count(DISTINCT r) AS relation_count,
                     toFloat(count(DISTINCT r)) / (entity_count * (entity_count - 1) / 2) AS chunk_density
                
                WHERE chunk_density < 0.3
                
                RETURN c.id AS chunk_id, c.text AS chunk_text,
                       entities, entity_count, relation_count, chunk_density
                ORDER BY entity_count DESC, chunk_density ASC
                LIMIT {target_chunks}
            """, dataset=dataset_id).data()
            
            print(f"  æ‰¾åˆ° {len(low_density_chunks)} å€‹ä½å¯†åº¦ Chunks")
            
            if not low_density_chunks:
                print("  âœ… æ‰€æœ‰ Chunks å¯†åº¦å·²é”æ¨™")
                return {
                    'processed_chunks': 0,
                    'new_relations': 0,
                    'density_before': density_before,
                    'density_after': density_before
                }
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # éšæ®µ 2ï¼šå°æ¯å€‹ Chunk é€²è¡Œå¯†é›†åŒ–æŠ½å–
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"\nğŸ”„ éšæ®µ 2ï¼šä½¿ç”¨å‡è¨­æ€§å•é¡Œæ³•é‡æ–°æŠ½å–é—œä¿‚...")
        
        total_new_relations = 0
        processed_count = 0
        
        for idx, chunk_data in enumerate(low_density_chunks):
            chunk_id = chunk_data['chunk_id']
            chunk_text = chunk_data['chunk_text']
            entities = chunk_data['entities']
            
            # æ ¼å¼åŒ–å¯¦é«”åˆ—è¡¨
            entity_list_text = ", ".join(entities)
            
            # æ§‹å»ºæç¤ºè©
            prompt = HYPOTHETICAL_QUESTIONS_PROMPT.format(
                chunk_text=chunk_text[:2000],  # é™åˆ¶é•·åº¦
                entity_list=entity_list_text
            )
            
            # èª¿ç”¨ LLM
            try:
                response = self.client.chat(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    options={"temperature": temperature, "top_p": 0.9}
                )
                content = response.get("message", {}).get("content", "") if isinstance(response, dict) else ""
                
                # è§£æä¸‰å…ƒçµ„
                triples = parse_triples(content)
                
                # å¯«å…¥æ–°é—œä¿‚
                with self.driver.session() as session:
                    for triple in triples:
                        head = triple.get("head")
                        relation = triple.get("relation")
                        tail = triple.get("tail")
                        
                        if not all([head, relation, tail]):
                            continue
                        
                        # åªé€£æ¥ç¾æœ‰å¯¦é«”
                        result = session.run("""
                            MATCH (h:Entity {name: $head})
                            MATCH (t:Entity {name: $tail})
                            MERGE (h)-[r:RELATION {type: $relation}]->(t)
                            ON CREATE SET r.chunks = [$chunk_id], r.densified = true
                            ON MATCH SET r.chunks = CASE
                                WHEN NOT $chunk_id IN r.chunks
                                THEN r.chunks + [$chunk_id]
                                ELSE r.chunks
                            END
                            RETURN count(r) AS created
                        """, head=head, relation=relation, tail=tail, chunk_id=chunk_id)
                        
                        record = result.single()
                        if record:
                            total_new_relations += record['created']
                
                processed_count += 1
                
                if (idx + 1) % 10 == 0:
                    print(f"  â†³ å·²è™•ç† {processed_count}/{len(low_density_chunks)} Chunksï¼Œæ–°å¢ {total_new_relations} å€‹é—œä¿‚")
                    
            except Exception as e:
                print(f"  âš ï¸  Chunk {chunk_id} è™•ç†å¤±æ•—ï¼š{e}")
                continue
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # éšæ®µ 3ï¼šçµ±è¨ˆçµæœ
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        with self.driver.session() as session:
            final_stats = session.run("""
                MATCH (e:Entity)
                WITH count(e) AS entities
                MATCH ()-[r:RELATION]->()
                RETURN entities, count(r) AS relations,
                       toFloat(count(r)) / entities AS density
            """).single()
            
            density_after = final_stats['density'] if final_stats else 0.0
        
        print(f"\n{'='*70}")
        print(f"âœ… å‡è¨­æ€§å•é¡Œé—œä¿‚å¯†é›†åŒ–å®Œæˆ")
        print(f"  â€¢ è™•ç† Chunksï¼š{processed_count}")
        print(f"  â€¢ æ–°å¢é—œä¿‚ï¼š{total_new_relations}")
        print(f"  â€¢ å¯†åº¦è®ŠåŒ–ï¼š{density_before:.3f} â†’ {density_after:.3f} (+{density_after-density_before:.3f})")
        print(f"{'='*70}")
        
        return {
            'processed_chunks': processed_count,
            'new_relations': total_new_relations,
            'density_before': density_before,
            'density_after': density_after
        }

