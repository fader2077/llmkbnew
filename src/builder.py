import json
import re
import hashlib
from typing import List, Dict, Tuple, Any, Iterable
from pathlib import Path
from ollama import Client
from config import CONFIG, TRIPLE_PROMPT_TEMPLATE
from src.models import OllamaVectorEmbedder
from src.database import ensure_vector_index, ensure_fulltext_index, ensure_entity_index
# âœ… å¾ utils.py åŒ¯å…¥é€šç”¨å·¥å…·å‡½æ•¸
from src.utils import chunk_text, parse_triples, deduplicate_triples, normalize_text

# âœ… é è¨­å€¼ä»å¾ CONFIG è®€å–ï¼Œä½†å…è¨±è¦†è“‹
DEFAULT_CHUNK_SIZE = CONFIG["optimal_indexing"]["chunk_size"]
DEFAULT_CHUNK_OVERLAP = CONFIG["optimal_indexing"]["overlap"]
DATASET_ID = CONFIG["infrastructure"]["dataset_id"]


def load_chunks(path: Path, chunk_size: int = None, overlap: int = None) -> List[Dict[str, str]]:
    """
    è¼‰å…¥ä¸¦åˆ‡åˆ†æ–‡æœ¬
    âœ… ä¿®æ­£ï¼šåŠ å…¥ chunk_size èˆ‡ overlap åƒæ•¸ï¼Œæ”¯æ´æ¶ˆèå¯¦é©—å‹•æ…‹èª¿æ•´
    """
    if not path.exists():
        raise FileNotFoundError(f"Knowledge base not found: {path}")
    
    # æ±ºå®šä½¿ç”¨å‚³å…¥åƒæ•¸æˆ–é è¨­å€¼
    size = chunk_size if chunk_size is not None else DEFAULT_CHUNK_SIZE
    ovlp = overlap if overlap is not None else DEFAULT_CHUNK_OVERLAP
    
    print(f"    ğŸ“„ Chunking strategy: Size={size}, Overlap={ovlp}")
    
    raw_text = path.read_text(encoding="utf-8")
    segments = chunk_text(raw_text, size, ovlp)
    
    docs: List[Dict[str, str]] = []
    for idx, segment in enumerate(segments):
        text = segment.strip()
        doc_id = f"{DATASET_ID}_chunk_{idx:05d}"
        docs.append(
            {
                "id": doc_id,
                "text": text,
                "source": path.name,
                "hash": hashlib.sha256(text.encode("utf-8")).hexdigest(),
            }
        )
    return docs
def upsert_chunks(driver, embedder: OllamaVectorEmbedder, docs: List[Dict[str, str]]) -> Tuple[int, int]:
    inserted = 0
    skipped = 0
    with driver.session() as session:
        for doc in docs:
            existing = session.run(
                "MATCH (c:Chunk {id:$id}) RETURN c.text_hash AS hash",
                id=doc["id"],
            ).single()
            if existing and existing.get("hash") == doc["hash"]:
                skipped += 1
                continue
            embedding = embedder.embed_query(doc["text"])
            session.run(
                """
                MERGE (c:Chunk {id:$id})
                SET c.text = $text,
                    c.source = $source,
                    c.dataset = $dataset,
                    c.embedding = $embedding,
                    c.text_hash = $hash
                """,
                id=doc["id"],
                text=doc["text"],
                source=doc["source"],
                dataset=DATASET_ID,
                embedding=embedding,
                hash=doc["hash"],
            )
            inserted += 1
    return inserted, skipped


def split_text_for_triples(text: str, max_length: int = 1024) -> List[str]:
    paragraphs = [p.strip() for p in re.split(r"\n{2,}", text) if p.strip()]
    if not paragraphs:
        paragraphs = [text]
    segments: List[str] = []
    for para in paragraphs:
        if len(para) <= max_length:
            segments.append(para)
            continue
        start = 0
        while start < len(para):
            end = min(len(para), start + max_length)
            segments.append(para[start:end])
            start = end
    return segments


def extract_triples(
    client: Client,
    text: str,
    model: str,
    language: str,
    retries: int = 2,
    allow_recursive: bool = True,
) -> List[Dict[str, str]]:
    prompt = TRIPLE_PROMPT_TEMPLATE.format(chunk=text, language=language)
    for attempt in range(retries + 1):
        response = client.chat(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            options={"temperature": 0.15 + attempt * 0.05, "top_p": 0.9},
        )
        content = response.get("message", {}).get("content", "")
        triples = parse_triples(content)
        if triples:
            return deduplicate_triples(triples)
    if allow_recursive and len(text) > 600:
        aggregated: List[Dict[str, str]] = []
        for segment in split_text_for_triples(text):
            partial = extract_triples(
                client,
                segment,
                model=model,
                language=language,
                retries=1,
                allow_recursive=False,
            )
            aggregated.extend(partial)
        return deduplicate_triples(aggregated)
    return []


def collect_triples_for_documents(
    client: Client, 
    docs: List[Dict[str, str]], 
    model: str, 
    language: str
) -> Tuple[Dict[str, List[Dict[str, str]]], List[str]]:
    """
    ä¸ºæ‰€æœ‰æ–‡æ¡£æ‰¹é‡æå–ä¸‰å…ƒç»„
    
    Args:
        client: Ollama client
        docs: æ–‡æ¡£åˆ—è¡¨
        model: LLM æ¨¡å‹åç§°
        language: ç›®æ ‡è¯­è¨€
    
    Returns:
        (triple_map, empty_chunks): ä¸‰å…ƒç»„æ˜ å°„å’Œæ— ä¸‰å…ƒç»„çš„chunkåˆ—è¡¨
    """
    triple_map = {}
    empty_chunks = []
    
    for i, doc in enumerate(docs):
        print(f"   Extracting {i+1}/{len(docs)}...", end="\r")
        triples = extract_triples(client, doc["text"], model, language)
        
        if not triples:
            empty_chunks.append(doc["id"])
        
        triple_map[doc["id"]] = triples
    
    print(f"   âœ… å·²å¤„ç† {len(docs)} ä¸ªæ–‡æ¡£ï¼Œ{len(empty_chunks)} ä¸ªæ— ä¸‰å…ƒç»„")
    return triple_map, empty_chunks


def ingest_triples(
    driver,
    docs: List[Dict[str, str]],
    client: Client,
    model: str,
    language: str,
) -> Tuple[int, int, List[str]]:
    """
    å¢é‡å¼çŸ¥è­˜åœ–è­œæ§‹å»º (Incremental Construction)
    
    æ ¸å¿ƒåŸå‰‡ï¼š
    1. ä½¿ç”¨ MERGE è€Œé CREATEï¼Œç¢ºä¿å¯¦é«”å’Œé—œä¿‚ä¸é‡è¤‡
    2. ä¸åˆªé™¤æ—¢æœ‰çš„ MENTIONS å’Œ RELATION
    3. åƒ…å¢é‡æ·»åŠ æ–°çš„çŸ¥è­˜ä¸‰å…ƒçµ„
    4. ä¿ç•™æ‰€æœ‰æ­·å²ä¾†æºè¿½æº¯ (r.chunks)
    
    éšæ®µåŠƒåˆ†ï¼š
    - éšæ®µä¸€ï¼šå¯¦é«”ç¯€é»å¢é‡å¯«å…¥ (Entity Nodes)
    - éšæ®µäºŒï¼šé—œä¿‚/ä¸‰å…ƒçµ„å¢é‡å¯«å…¥ (Relationships/Triples)
    - éšæ®µä¸‰ï¼šChunk èˆ‡å‡ºè™•å¢é‡é€£æ¥ (Provenance Linking)
    """
    triple_map, empty_chunks = collect_triples_for_documents(client, docs, model, language)
    updated = 0
    
    with driver.session() as session:
        for doc in docs:
            chunk_id = doc["id"]
            triples = triple_map.get(chunk_id, [])
            
            # âš ï¸ é‡è¦è®Šæ›´ï¼šç§»é™¤æ‰€æœ‰ DELETE æ“ä½œ
            # èˆŠé‚è¼¯ï¼ˆå·²å»¢é™¤ï¼‰ï¼š
            # - DELETE MENTIONS é—œä¿‚
            # - DELETE RELATION é—œä¿‚
            # æ–°é‚è¼¯ï¼šä¿ç•™æ‰€æœ‰æ—¢æœ‰è³‡æ–™ï¼Œåƒ…å¢é‡æ·»åŠ 
            
            if not triples:
                # å³ä½¿æ²’æœ‰æ–°ä¸‰å…ƒçµ„ï¼Œä¹Ÿä¸åˆªé™¤æ—¢æœ‰è³‡æ–™
                continue
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # éšæ®µä¸€ + éšæ®µäºŒ + éšæ®µä¸‰ï¼šåˆä½µåŸ·è¡Œï¼ˆæ€§èƒ½å„ªåŒ–ï¼‰
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            session.run(
            """
            // ===== éšæ®µä¸€ï¼šå¯¦é«”ç¯€é»å¢é‡å¯«å…¥ =====
            UNWIND $triples AS triple
            
            // å‰µå»ºæˆ–åŒ¹é…é ­å¯¦é«”ï¼ˆä½¿ç”¨ MERGE ç¢ºä¿å”¯ä¸€æ€§ï¼‰
            MERGE (h:Entity {name: triple.head})
            ON CREATE SET h.created_at = timestamp()
            
            // å‰µå»ºæˆ–åŒ¹é…å°¾å¯¦é«”ï¼ˆä½¿ç”¨ MERGE ç¢ºä¿å”¯ä¸€æ€§ï¼‰
            MERGE (t:Entity {name: triple.tail})
            ON CREATE SET t.created_at = timestamp()
            
            // ===== éšæ®µäºŒï¼šé—œä¿‚/ä¸‰å…ƒçµ„å¢é‡å¯«å…¥ =====
            // ä½¿ç”¨ MERGE ç¢ºä¿é—œä¿‚å”¯ä¸€æ€§ï¼ˆåŸºæ–¼ head + type + tailï¼‰
            MERGE (h)-[r:RELATION {type: triple.relation}]->(t)
            ON CREATE SET 
                r.chunks = [$cid],
                r.created_at = timestamp(),
                r.confidence = 0.9
            ON MATCH SET 
                // åƒ…åœ¨ chunks åˆ—è¡¨ä¸­ä¸å­˜åœ¨æ™‚æ‰æ·»åŠ ï¼ˆé¿å…é‡è¤‡ï¼‰
                r.chunks = CASE 
                    WHEN $cid IN r.chunks THEN r.chunks 
                    ELSE r.chunks + $cid 
                END,
                r.last_updated = timestamp()
            
            // ===== éšæ®µä¸‰ï¼šChunk èˆ‡å‡ºè™•å¢é‡é€£æ¥ =====
            WITH h, t
            
            // ç¢ºä¿ Chunk ç¯€é»å­˜åœ¨
            MERGE (c:Chunk {id: $cid})
            
            // å¢é‡é€£æ¥ Chunk -> Entity (MENTIONS)
            // ä½¿ç”¨ MERGE ç¢ºä¿é—œä¿‚ä¸é‡è¤‡
            MERGE (c)-[:MENTIONS]->(h)
            MERGE (c)-[:MENTIONS]->(t)
            """,
            triples=triples,
            cid=chunk_id,
        )
            updated += 1
    
    skipped = len(docs) - updated
    return updated, skipped, empty_chunks


class GraphBuilder:
    """
    å°è£…å›¾è°±æ„å»ºæµç¨‹
    """
    def __init__(self, driver, ollama_client: Client):
        self.driver = driver
        self.client = ollama_client
        self.embedder = OllamaVectorEmbedder(self.client, CONFIG["models"]["embed_model"])

    def build_graph(self, text_path: Path, chunk_size: int = None, overlap: int = None):
        """
        ç»Ÿä¸€çš„å›¾è°±æ„å»ºå…¥å£
        
        Args:
            text_path: çŸ¥è¯†åº“æ–‡æœ¬è·¯å¾„
            chunk_size: chunk å¤§å°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ CONFIGï¼‰
            overlap: é‡å å¤§å°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨ CONFIGï¼‰
        """
        print("ğŸ“š Loading and chunking...")
        # âœ… ä¿®æ­£ï¼šå°‡åƒæ•¸å‚³éçµ¦ load_chunks
        chunks = load_chunks(text_path, chunk_size, overlap)
        print(f"  âœ… å·²åŠ è½½ {len(chunks)} ä¸ª chunks")
        
        print("ğŸ§® Ensuring indexes...")
        # âœ… é—œéµæ€§èƒ½å„ªåŒ–ï¼šç‚º Entity å‰µå»ºç´¢å¼•
        ensure_entity_index(self.driver)
        
        ensure_vector_index(
            self.driver, 
            CONFIG["infrastructure"]["vector_index_name"], 
            "Chunk", 
            "embedding", 
            self.embedder.dimension
        )
        ensure_fulltext_index(
            self.driver,
            CONFIG["infrastructure"]["fulltext_index_name"],
            "Chunk",
            "text"
        )
        
        print("â¬†ï¸ Upserting chunks...")
        upserted, skipped = upsert_chunks(self.driver, self.embedder, chunks)
        print(f"  âœ… Upserted {upserted}, skipped {skipped}")
        
        print("ğŸ”— Extracting triples...")
        updated, skipped_triples, empty = ingest_triples(
            self.driver, 
            chunks, 
            self.client, 
            CONFIG["models"]["llm_model"], 
            language=CONFIG["models"]["answer_language"]
        )
        print(f"  âœ… Updated {updated} chunks, {len(empty)} empty")
        
        print("\nâœ… å›¾è°±æ„å»ºå®Œæˆï¼")

