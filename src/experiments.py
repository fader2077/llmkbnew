# src/experiments.py
"""
å®éªŒç®¡ç†æ¨¡å—

è´Ÿè´£è¿è¡Œæ¶ˆèå®éªŒï¼ˆAblation Studyï¼‰ï¼š
- Phase 1: ç´¢å¼•æ¶ˆèï¼ˆIndexing Ablationï¼‰
- Phase 4: æ£€ç´¢æ¶ˆèï¼ˆRetrieval Ablationï¼‰
"""

import time
import json
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from ollama import Client

from config import CONFIG, RESULT_DIR, KNOWLEDGE_BASE_PATH
from src.retrieval import RetrievalEngine
from src.models import OllamaVectorEmbedder
from src.builder import GraphBuilder
from src.database import clean_database
from src.metrics import calculate_f1_score, calculate_exact_match, calculate_cosine_similarity_score, is_effective_answer

# âœ… é…ç½®æ—¥èªŒç³»çµ±
LOG_FILE = RESULT_DIR / "experiment.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BaseExperimentRunner:
    """å¯¦é©—åŸºç¤é¡åˆ¥ï¼Œæä¾›é€šç”¨å·¥å…·"""
    
    def __init__(self, driver, ollama_client):
        self.driver = driver
        self.ollama_client = ollama_client
        self.embedder = OllamaVectorEmbedder(ollama_client, CONFIG["models"]["embed_model"])
        self.engine = RetrievalEngine(driver, ollama_client)

    def _save_results(self, results: List[Dict], prefix: str):
        """
        åŒæ™‚å„²å­˜ CSV å’Œ JSONL
        - CSV: æ–¹ä¾¿ Excel æŸ¥çœ‹
        - JSONL: ä¿ç•™å®Œæ•´æ ¼å¼ï¼ˆæ›è¡Œç¬¦ã€å¼•è™Ÿï¼‰ï¼Œæ–¹ä¾¿ç¨‹å¼è®€å–
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 1. å„²å­˜ CSV
        df = pd.DataFrame(results)
        csv_path = RESULT_DIR / f"{prefix}_{timestamp}.csv"
        # ä½¿ç”¨ utf-8-sig è®“ Excel é–‹å•Ÿä¸äº‚ç¢¼ï¼Œescapechar è™•ç†æ›è¡Œ
        df.to_csv(csv_path, index=False, encoding='utf-8-sig', escapechar='\\')
        
        # 2. å„²å­˜ JSONL
        jsonl_path = RESULT_DIR / f"{prefix}_{timestamp}.jsonl"
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for record in results:
                json.dump(record, f, ensure_ascii=False)
                f.write('\n')
        
        logger.info(f"âœ… çµæœå·²ä¿å­˜ï¼š")
        logger.info(f"   ğŸ“‚ CSV: {csv_path}")
        logger.info(f"   ğŸ“‚ JSONL: {jsonl_path}")
        
        print(f"\nâœ… çµæœå·²ä¿å­˜ï¼š")
        print(f"   ğŸ“‚ CSV: {csv_path}")
        print(f"   ğŸ“‚ JSONL: {jsonl_path}")
        
        return df


class RetrievalAblationRunner(BaseExperimentRunner):
    """Phase 4: æª¢ç´¢æ¶ˆèå¯¦é©— (Hop Count / Top-K)"""
    
    def __init__(self, driver, ollama_client):
        super().__init__(driver, ollama_client)
        logger.info("="*70)
        logger.info("ğŸš€ åˆå§‹åŒ–æª¢ç´¢æ¶ˆèå¯¦é©—ç®¡ç†å™¨")
        logger.info(f"ğŸ“Š Embedding æ¨¡å‹: {CONFIG['models']['embed_model']}")
        logger.info(f"ğŸ“Š LLM æ¨¡å‹: {CONFIG['models']['llm_model']}")
        logger.info("="*70)
    
    def run_experiment(
        self,
        questions_path: Path,
        hop_values: Optional[List[int]] = None,
        top_k_values: Optional[List[int]] = None,
        max_questions: Optional[int] = None
    ) -> pd.DataFrame:
        
        if hop_values is None: 
            hop_values = CONFIG["retrieval_grid"]["hop_counts"]
        if top_k_values is None: 
            top_k_values = CONFIG["retrieval_grid"]["top_k_values"]
        if max_questions is None: 
            max_questions = CONFIG["retrieval_grid"]["max_questions"]
        
        logger.info(f"ğŸ“š åŠ è¼‰å•é¡Œæ•¸æ“šé›†: {questions_path}")
        print(f"ğŸ“š åŠ è¼‰å•é¡Œæ•¸æ“šé›†: {questions_path}")
        
        df_questions = pd.read_csv(questions_path)
        if len(df_questions) > max_questions:
            df_questions = df_questions.head(max_questions)
            logger.warning(f"âš ï¸  é™åˆ¶åˆ°å‰ {max_questions} å€‹å•é¡Œ")
            print(f"  âš ï¸  é™åˆ¶åˆ°å‰ {max_questions} å€‹å•é¡Œ")
        
        logger.info(f"âœ… åŠ è¼‰ {len(df_questions)} å€‹å•é¡Œ")
        print(f"  âœ… åŠ è¼‰ {len(df_questions)} å€‹å•é¡Œ")
        
        all_results = []
        total_experiments = len(hop_values) * len(top_k_values) * len(df_questions)
        completed = 0
        
        print(f"\nğŸ§ª é–‹å§‹ Phase 4 å¯¦é©—: {total_experiments} æ¬¡æ¸¬è©¦\n")
        logger.info(f"ğŸ§ª é–‹å§‹å¯¦é©—: {total_experiments} æ¬¡æ¸¬è©¦")
        
        for hop in hop_values:
            for top_k in top_k_values:
                exp_name = f"Hop-{hop}_TopK-{top_k}"
                logger.info(f"\n{'='*70}")
                logger.info(f"ğŸ¯ å¯¦é©—é…ç½®: {exp_name}")
                logger.info(f"   Hop={hop} {'(Baseline - Vector Only)' if hop == 0 else ''}, Top-K={top_k}")
                logger.info("="*70)
                
                print(f"{'='*70}")
                print(f"ğŸ¯ å¯¦é©—é…ç½®: {exp_name}")
                print(f"   Hop={hop} {'(Baseline - Vector Only)' if hop == 0 else ''}, Top-K={top_k}")
                print("="*70)
                
                exp_start_time = time.time()
                
                for idx, row in df_questions.iterrows():
                    question = row.get('question', row.get('Question', ''))
                    reference_answer = row.get('answer', row.get('Answer', None))
                    
                    try:
                        result = self.engine.run_qa(
                            question=question,
                            hop=hop,
                            top_k=top_k,
                            reference_answer=reference_answer,
                            verbose=False
                        )
                        
                        # âœ… æ–°å¢ï¼šè¨ˆç®—è©•ä¼°æŒ‡æ¨™
                        f1_score = 0.0
                        exact_match = 0
                        cosine_sim = 0.0
                        is_effective = 0
                        
                        if reference_answer:
                            f1_score = calculate_f1_score(result.predicted_answer, reference_answer)
                            exact_match = calculate_exact_match(result.predicted_answer, reference_answer)
                            cosine_sim = calculate_cosine_similarity_score(result.predicted_answer, reference_answer, self.embedder)
                        
                        is_effective = 1 if is_effective_answer(result.predicted_answer) else 0
                        
                        # è®°å½•ç»“æœ
                        all_results.append({
                            "experiment": exp_name,
                            "hop": hop,
                            "top_k": top_k,
                            "question_id": idx,
                            "question": question,
                            "reference_answer": reference_answer,
                            "predicted_answer": result.predicted_answer,
                            "num_chunks": result.num_chunks,
                            "latency_ms": result.inference_latency_ms,
                            # âœ… æ–°å¢æŒ‡æ¨™æ¬„ä½
                            "f1_score": f1_score,
                            "exact_match": exact_match,
                            "cosine_similarity": cosine_sim,
                            "is_effective": is_effective
                        })
                        
                        completed += 1
                        
                        # æ—¥èªŒè¨˜éŒ„æ¯å€‹å•é¡Œ
                        logger.info(f"âœ… Q{idx} | F1={f1_score:.3f} | Cos={cosine_sim:.3f} | Latency={result.inference_latency_ms:.1f}ms | Effective={is_effective}")
                        
                        # è¿›åº¦æ˜¾ç¤ºï¼ˆåŒ…å«æŒ‡æ¨™ï¼‰
                        if completed % 10 == 0:
                            progress = (completed / total_experiments) * 100
                            logger.info(f"ğŸ“Š é€²åº¦: {completed}/{total_experiments} ({progress:.1f}%)")
                            print(f"  â†³ è¿›åº¦: {completed}/{total_experiments} ({progress:.1f}%) | æœ€è¿‘: F1={f1_score:.2f} Cos={cosine_sim:.2f}")
                    
                    except Exception as e:
                        logger.error(f"âŒ Q{idx} å¤±æ•—: {str(e)}")
                        print(f"  âš ï¸  é—®é¢˜ #{idx} å¤±è´¥: {e}")
                        all_results.append({
                            "experiment": exp_name,
                            "hop": hop,
                            "top_k": top_k,
                            "question_id": idx,
                            "question": question,
                            "reference_answer": reference_answer,
                            "predicted_answer": f"[Error: {e}]",
                            "num_chunks": 0,
                            "latency_ms": 0.0,
                            # âœ… éŒ¯èª¤æ™‚å¡«å…¥ 0 åˆ†
                            "f1_score": 0.0,
                            "exact_match": 0,
                            "cosine_similarity": 0.0,
                            "is_effective": 0
                        })
                        completed += 1
                
                exp_duration = time.time() - exp_start_time
                logger.info(f"âœ… {exp_name} å®Œæˆï¼ˆè€—æ™‚ {exp_duration:.1f}sï¼‰")
                print(f"  âœ… {exp_name} å®Œæˆ ({exp_duration:.1f}s)\n")
        
        # ä¿å­˜ç»“æœ
        df_results = self._save_results(all_results, "retrieval_ablation")
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(df_results)
        
        return df_results
    
    def _print_summary(self, df_results: pd.DataFrame):
        """âœ… ä¿®å¾©ç‰ˆæ‘˜è¦æ‰“å°"""
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ“Š å¯¦é©—æ‘˜è¦ (Average Metrics)")
        logger.info("="*70)
        
        print(f"\n{'='*70}")
        print("ğŸ“Š å®éªŒæ‘˜è¦ (Average Metrics)")
        print("="*70)
        
        # âœ… ä¿®æ­£ï¼šå…ˆéæ¿¾æ‰å¤±æ•—çš„æ¸¬è©¦ (latency_ms = 0.0) å†è¨ˆç®—å¹³å‡å»¶é²
        success_df = df_results[df_results['latency_ms'] > 0]
        failed_count = len(df_results) - len(success_df)
        
        if failed_count > 0:
            logger.warning(f"âš ï¸  æœ‰ {failed_count} å€‹æ¸¬è©¦å¤±æ•—ï¼ˆå»¶é²çµ±è¨ˆå·²æ’é™¤ï¼‰")
            print(f"âš ï¸  æ³¨æ„: æœ‰ {failed_count} å€‹æ¸¬è©¦å¤±æ•—ï¼ˆå»¶é²çµ±è¨ˆå·²æ’é™¤ï¼‰\n")
        
        # æŒ‰ hop å’Œ top_k åˆ†ç»„ç»Ÿè®¡ï¼ˆåŒ…å«è©•ä¼°æŒ‡æ¨™ï¼‰
        summary = df_results.groupby(['hop', 'top_k']).agg({
            'f1_score': 'mean',
            'cosine_similarity': 'mean',
            'is_effective': 'mean',  # æœ‰æ•ˆå›ç­”ç‡
            'question_id': 'count'
        }).round(3)
        
        # å–®ç¨è¨ˆç®—å¹³å‡å»¶é²ï¼ˆåªç®—æˆåŠŸçš„ï¼‰
        if len(success_df) > 0:
            latency_summary = success_df.groupby(['hop', 'top_k'])['latency_ms'].mean().round(1)
            summary['Avg_Latency_ms'] = latency_summary
        else:
            summary['Avg_Latency_ms'] = 0.0
        
        # âœ… ä¿®æ­£ï¼šç›´æ¥ä½¿ç”¨ç•¶å‰æ¬„ä½é †åºï¼Œä¸å†é‡æ–°å‘½å
        summary.columns = ['Avg_F1', 'Avg_Cosine', 'Effective_Rate', 'Num_Questions', 'Avg_Latency_ms']
        
        # è¼¸å‡ºåˆ°æ—¥èªŒå’Œæ§åˆ¶å°
        summary_str = summary.to_string()
        for line in summary_str.split('\n'):
            logger.info(line)
        
        print(summary)
        print("="*70)
        print("\nğŸ’¡ æç¤ºï¼šEffective_Rate ä»£è¡¨æ¨¡å‹çµ¦å‡ºæœ‰æ•ˆå›ç­”ï¼ˆéæ‹’ç­”ï¼‰çš„æ¯”ä¾‹")
        logger.info("="*70)
        logger.info("ğŸ’¡ æç¤ºï¼šEffective_Rate ä»£è¡¨æ¨¡å‹çµ¦å‡ºæœ‰æ•ˆå›ç­”ï¼ˆéæ‹’ç­”ï¼‰çš„æ¯”ä¾‹")


class IndexingAblationRunner(BaseExperimentRunner):
    """Phase 1: ç´¢å¼•æ¶ˆèå¯¦é©— (Chunk Size / Overlap)"""
    
    def run_experiment(
        self, 
        text_path: Path, 
        chunk_configs: List[Dict[str, int]], 
        questions_path: Path,
        max_questions: int = 150
    ):
        print(f"\nğŸš€ é–‹å§‹ Phase 1: ç´¢å¼•æ¶ˆèå¯¦é©—")
        print(f"   é…ç½®æ•¸: {len(chunk_configs)}")
        print(f"   æ¯çµ„å•é¡Œæ•¸: {max_questions}")
        
        logger.info("="*70)
        logger.info("ğŸš€ é–‹å§‹ Phase 1: ç´¢å¼•æ¶ˆèå¯¦é©—")
        logger.info(f"   é…ç½®æ•¸: {len(chunk_configs)}")
        logger.info(f"   æ¯çµ„å•é¡Œæ•¸: {max_questions}")
        logger.info("="*70)
        
        df_questions = pd.read_csv(questions_path)
        if len(df_questions) > max_questions:
            df_questions = df_questions.head(max_questions)
            
        all_results = []
        builder = GraphBuilder(self.driver, self.ollama_client)
        
        for config in chunk_configs:
            chunk_size = config['chunk_size']
            overlap = config['overlap']
            exp_id = f"Chunk-{chunk_size}_Overlap-{overlap}"
            
            print(f"\n{'-'*60}")
            print(f"ğŸ—ï¸  æ§‹å»ºé…ç½®: {exp_id}")
            print(f"{'-'*60}")
            
            logger.info(f"\n{'-'*60}")
            logger.info(f"ğŸ—ï¸  æ§‹å»ºé…ç½®: {exp_id}")
            logger.info(f"{'-'*60}")
            
            # 1. æ¸…ç©ºè³‡æ–™åº«
            print("ğŸ—‘ï¸  æ¸…ç©ºè³‡æ–™åº«...")
            clean_database(self.driver, "", clean_all=True)
            
            # 2. é‡å»ºåœ–è­œ
            try:
                print(f"ğŸ”¨ é‡å»ºåœ–è­œ (Chunk={chunk_size}, Overlap={overlap})...")
                builder.build_graph(text_path, chunk_size=chunk_size, overlap=overlap)
            except Exception as e:
                print(f"âŒ å»ºåœ–å¤±æ•—: {e}")
                logger.error(f"âŒ å»ºåœ–å¤±æ•—: {e}")
                continue
                
            # 3. åŸ·è¡Œ QA è©•æ¸¬ (å›ºå®šä½¿ç”¨ Hop=2, TopK=10 ä½œç‚ºåŸºæº–)
            print(f"ğŸ“ åŸ·è¡Œ QA è©•æ¸¬...")
            for idx, row in df_questions.iterrows():
                question = row.get('question', '')
                reference = row.get('answer', row.get('reference_answer', ''))
                
                try:
                    # ä½¿ç”¨ RetrievalEngine é€²è¡Œå›ç­”
                    qa_result = self.engine.run_qa(
                        question=question, 
                        hop=0,  # å›ºå®šåƒæ•¸ä»¥æ¯”è¼ƒ Index æ•ˆæœ
                        top_k=10, 
                        reference_answer=reference,
                        verbose=False
                    )
                    
                    # è¨ˆç®—æŒ‡æ¨™
                    f1 = calculate_f1_score(qa_result.predicted_answer, reference)
                    cos = calculate_cosine_similarity_score(qa_result.predicted_answer, reference, self.embedder)
                    
                    all_results.append({
                        "timestamp": datetime.now().isoformat(),
                        "experiment_id": exp_id,
                        "chunk_size": chunk_size,
                        "overlap": overlap,
                        "question_id": idx,
                        "question": question,
                        "reference_answer": reference,
                        "predicted_answer": qa_result.predicted_answer,
                        "f1_score": f1,
                        "cosine_similarity": cos,
                        "num_chunks": qa_result.num_chunks,
                        "latency_ms": qa_result.inference_latency_ms
                    })
                    print(f"   Q{idx} Cos={cos:.2f}", end='\r')
                    logger.info(f"âœ… Q{idx} | F1={f1:.3f} | Cos={cos:.3f}")
                    
                except Exception as e:
                    print(f"   âš ï¸ QA Error: {e}")
                    logger.error(f"âŒ Q{idx} Error: {e}")
            
            print()
            
            # âš ï¸  æ¯å€‹é…ç½®å®Œæˆå¾Œæ¸…ç©ºçŸ¥è­˜åº«
            print(f"ğŸ—‘ï¸  æ¸…ç©ºçŸ¥è­˜åº«ï¼ˆæº–å‚™ä¸‹ä¸€å€‹é…ç½®ï¼‰...")
            logger.info(f"ğŸ—‘ï¸  æ¸…ç©ºçŸ¥è­˜åº«ï¼ˆæº–å‚™ä¸‹ä¸€å€‹é…ç½®ï¼‰...")
            clean_database(self.driver, "", clean_all=True)

        # 4. å„²å­˜çµæœ
        df_results = self._save_results(all_results, "indexing_ablation")
        self._print_summary(df_results)
        
        return df_results
        
    def _print_summary(self, df: pd.DataFrame):
        print(f"\n{'='*70}")
        print("ğŸ“Š Phase 1 å¯¦é©—æ‘˜è¦ (Indexing Strategy)")
        print("="*70)
        
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ“Š Phase 1 å¯¦é©—æ‘˜è¦ (Indexing Strategy)")
        logger.info("="*70)
        
        # Check if DataFrame is empty or missing required columns
        required_cols = ['chunk_size', 'overlap', 'f1_score', 'cosine_similarity', 'num_chunks']
        if df.empty or not all(col in df.columns for col in required_cols):
            msg = "âš ï¸ ç„¡æœ‰æ•ˆçµæœå¯é¡¯ç¤ºï¼ˆæ‰€æœ‰é…ç½®å‡å¤±æ•—ï¼‰"
            print(msg)
            logger.warning(msg)
            print("="*70)
            return
        
        summary = df.groupby(['chunk_size', 'overlap']).agg({
            'f1_score': 'mean',
            'cosine_similarity': 'mean',
            'num_chunks': 'mean'
        }).round(3)
        
        summary.columns = ['Avg_F1', 'Avg_Cos', 'Avg_Retrieved']
        
        summary_str = summary.to_string()
        for line in summary_str.split('\n'):
            logger.info(line)
        
        print(summary)
        print("="*70)
