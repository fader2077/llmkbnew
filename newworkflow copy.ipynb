{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443f89e6",
   "metadata": {},
   "source": [
    "# ðŸ Knowledge Graph-Enhanced LLM for Goat Disease QA System\n",
    "\n",
    "## ðŸ“‘ è«–æ–‡å¯¦é©—æµç¨‹å¯¦ä½œ\n",
    "**è«–æ–‡æ¨™é¡Œ:** A Knowledge Graph-Enhanced Large Language Model Framework for Goat Disease Question Answering System\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ å¯¦é©—è¨­è¨ˆæ¦‚è¦½\n",
    "\n",
    "æœ¬ç­†è¨˜æœ¬å¯¦ä½œè«–æ–‡ä¸­çš„ **4 éšŽæ®µå¯¦é©—è¨­è¨ˆ (4-Phase Experimental Design)**:\n",
    "\n",
    "### **Phase 1: ç´¢å¼•å„ªåŒ– (Indexing Optimization)**\n",
    "- æ¶ˆèžå¯¦é©—æ¸¬è©¦ä¸åŒçš„ Chunk Size èˆ‡ Overlap\n",
    "- è¼¸å‡º: `ablation_chunking.csv`\n",
    "\n",
    "### **Phase 2: åŸºæº–ç·šèˆ‡è¨ºæ–· (Baseline & Diagnosis)**\n",
    "- è¨ˆç®—ã€Œå„ªåŒ–å‰ã€çš„åœ–è­œå“è³ªæŒ‡æ¨™\n",
    "- æ¯”è¼ƒ Vector-Only RAG vs. Initial Graph RAG\n",
    "- è¼¸å‡º: `baseline_comparison.csv`\n",
    "\n",
    "### **Phase 3: çµæ§‹å„ªåŒ– (Structural Optimization)**\n",
    "- åŸ·è¡Œ Algorithm 1 (æ˜Ÿåž‹æ‹“æ¨¸åµæ¸¬)\n",
    "- åŸ·è¡Œ Algorithm 2 (åœ–è­œå¢žå¼·)\n",
    "- ç›®æ¨™: é—œä¿‚å¯†åº¦ (Relation Density) > 2.0\n",
    "\n",
    "### **Phase 4: æª¢ç´¢æ¶ˆèž (Retrieval Ablation)**\n",
    "- æ¸¬è©¦ä¸åŒçš„ Hop Counts èˆ‡ Top-K å€¼\n",
    "- è¼¸å‡º: `retrieval_ablation.csv` + `final_experiment.jsonl`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ è¼¸å‡ºæ ¼å¼\n",
    "\n",
    "- **JSONL**: è©³ç´°æ—¥èªŒï¼ˆæ¯å€‹å•é¡Œçš„å®Œæ•´è»Œè·¡ï¼‰\n",
    "- **CSV**: æŒ‡æ¨™çµ±è¨ˆè¡¨ï¼ˆç”¨æ–¼è£½ä½œæ¶ˆèžå¯¦é©—è¡¨æ ¼ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ å¿«é€Ÿé–‹å§‹\n",
    "\n",
    "1. ç¢ºä¿ Neo4j èˆ‡ Ollama æœå‹™å·²å•Ÿå‹•\n",
    "2. ä¾åºåŸ·è¡Œä¸‹æ–¹æ‰€æœ‰ Cell\n",
    "3. é ä¼°ç¸½åŸ·è¡Œæ™‚é–“ï¼š2-4 å°æ™‚ï¼ˆå–æ±ºæ–¼å•é¡Œæ•¸é‡ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a4928fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é…ç½®è¼‰å…¥å®Œæˆ\n",
      "ðŸ“ çŸ¥è­˜åº«è·¯å¾‘: c:\\Users\\kbllm\\Desktop\\llm-KB\\new\\goat_data_text collection-1.2-eng.txt\n",
      "ðŸ“ å•é¡Œé›†è·¯å¾‘: c:\\Users\\kbllm\\Desktop\\llm-KB\\new\\topic.csv\n",
      "ðŸ”§ Neo4j URI: bolt://localhost:7687\n",
      "ðŸ¤– æŽ¨è«–æ¨¡åž‹: deepseek-r1:8b-llama-distill-q4_K_M\n",
      "ðŸ“Š ç´¢å¼•æ¶ˆèžå¯¦é©—çµ„æ•¸: 1\n",
      "ðŸ“Š æª¢ç´¢æ¶ˆèžå¯¦é©—çµ„æ•¸: 4\n",
      "ðŸ”¥ è‡ªå®šç¾©ä¸‰å…ƒçµ„æŠ½å– Prompt å·²è¼‰å…¥ï¼ˆ3215 å­—å…ƒï¼‰\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“‹ ç¬¬ 1 æ­¥ï¼šé…ç½®è¨­å®š (Configuration Setup)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# åŸºç¤Žè·¯å¾‘\n",
    "BASE_DIR = Path.cwd()\n",
    "KNOWLEDGE_BASE_PATH = BASE_DIR / \"goat_data_text collection-1.2-eng.txt\"\n",
    "QUESTION_DATASET_PATH = BASE_DIR / \"topic.csv\"\n",
    "\n",
    "# ðŸ”¥ è‡ªå®šç¾©ä¸‰å…ƒçµ„æŠ½å– Promptï¼ˆé«˜å¯†åº¦çŸ¥è­˜åœ–è­œï¼‰\n",
    "TRIPLE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert knowledge graph engineer. Your task is to extract **explicit and implicit semantic triples** from the text to build a high-density knowledge graph in {language}.\n",
    "\n",
    "ðŸŽ¯ **Core Objectives (Target Density > 1.8)**:\n",
    "1. **Zero Isolated Nodes**: Ensure every entity has 2+ connections. Transform weak entities into connected hubs.\n",
    "2. **Deep Implicit Mining**: Extract causal, functional, and attribute relationships hidden within and across sentences.\n",
    "3. **Strict Relation Types**: Use specific predicates (e.g., 'causes', 'contains') instead of vague ones (e.g., 'related').\n",
    "4. **Attribute as Relations**: Treat numbers, states, time, and types as relation tails (e.g., (goat, weight_is, 45kg)).\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## ðŸ› ï¸ Extraction Strategy Checklist (Must Execute)\n",
    "\n",
    "### 1. ðŸ” Explicit & Implicit Relationship Mining\n",
    "* **Layer 1 (Explicit)**: Extract directly stated relations (A causes B).\n",
    "* **Layer 2 (Intra-sentence Implicit)**: Infer hidden links (Subject â†’ Action â†’ Outcome).\n",
    "    * *Example*: \"Vitamin A deficiency causes night blindness.\" â†’ Extract (Vitamin_A_deficiency, causes, night_blindness) AND (night_blindness, symptom_of, Vitamin_A_deficiency).\n",
    "* **Layer 3 (Cross-sentence Implicit)**: Connect entities across sentences via shared context.\n",
    "    * *Example*: \"Goats lack Vitamin A. It causes blindness.\" â†’ Connect (goat, deficient_in, Vitamin_A) AND (Vitamin_A, prevents, blindness).\n",
    "\n",
    "### 2. ðŸ”¢ Attribute & Data Extraction (Crucial for Density)\n",
    "* **Numerical**: (feed, protein_content_is, 18%), (goat, weight_is, 45kg)\n",
    "* **State/Characteristic**: (sick_goat, state_is, lethargic), (lesion, color_is, red)\n",
    "* **Time/Frequency**: (treatment, duration_is, 7_days), (medication, frequency_is, twice_daily)\n",
    "* **Classification**: (goat, breed_is, Boer), (pneumonia, type_is, respiratory_disease)\n",
    "\n",
    "### 3. ðŸ”— Coreference Resolution (Mandatory)\n",
    "* **Resolve Pronouns**: Replace 'it', 'this', 'that', 'the animal' with the specific entity name.\n",
    "    * *Bad*: (it, causes, death)\n",
    "    * *Good*: (viral_infection, causes, death)\n",
    "* **Restore Omitted Subjects**: If a sentence starts with a verb, link it to the subject from the previous sentence.\n",
    "\n",
    "### 4. ðŸ“ Standardized Relation Types (Use These Verbs)\n",
    "* **Causality**: causes, leads_to, triggers, induces, results_in, prevents, inhibits\n",
    "* **Composition**: contains, comprised_of, part_of, ingredient_is\n",
    "* **Attribute**: weight_is, length_is, color_is, state_is, located_at, occurs_at\n",
    "* **Hierarchy**: is_a, belongs_to, type_of, classified_as\n",
    "* **Function**: used_for, treats, improves, requires, depends_on\n",
    "* **ðŸš« BANNED**: related_to, associated_with, has, is (unless 'is_a'), involving.\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "## âœ… Output Format\n",
    "Output **ONLY** a JSON array of triples. No markdown, no explanations.\n",
    "\n",
    "**Example**:\n",
    "[\n",
    "  {{\"head\": \"goat\", \"relation\": \"deficient_in\", \"tail\": \"vitamin_A\"}},\n",
    "  {{\"head\": \"vitamin_A_deficiency\", \"relation\": \"causes\", \"tail\": \"night_blindness\"}},\n",
    "  {{\"head\": \"night_blindness\", \"relation\": \"symptom_of\", \"tail\": \"nutritional_deficiency\"}},\n",
    "  {{\"head\": \"goat\", \"relation\": \"weight_is\", \"tail\": \"45kg\"}}\n",
    "]\n",
    "\n",
    "**Text to Extract**:\n",
    "{chunk}\n",
    "\"\"\"\n",
    "\n",
    "CONFIG = {\n",
    "    # ==========================================\n",
    "    # A. ç’°å¢ƒèˆ‡åŸºç¤Žè¨­æ–½ (ä½¿ç”¨è€…æä¾›)\n",
    "    # ==========================================\n",
    "    \"infrastructure\": {\n",
    "        \"neo4j_uri\": os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\"),\n",
    "        \"neo4j_auth\": (\n",
    "            os.environ.get(\"NEO4J_USER\", \"neo4j\"),\n",
    "            os.environ.get(\"NEO4J_PASSWORD\", \"neo4jgoat\")\n",
    "        ),\n",
    "        \"ollama_host\": os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\"),\n",
    "        \"dataset_id\": KNOWLEDGE_BASE_PATH.stem.replace(\" \", \"_\"),\n",
    "        \"vector_index_name\": \"chunk_embeddings\",\n",
    "        \"fulltext_index_name\": \"chunk_text_fts\",\n",
    "    },\n",
    "\n",
    "    # ==========================================\n",
    "    # B. æ¨¡åž‹è¨­å®š\n",
    "    # ==========================================\n",
    "    \"models\": {\n",
    "        \"llm_model\": \"deepseek-r1:8b-llama-distill-q4_K_M\",\n",
    "        \"graph_create_model\": \"deepseek-r1:8b-llama-distill-q4_K_M\",\n",
    "        \"embed_model\": \"nomic-embed-text\",\n",
    "        \"answer_language\": \"english\"\n",
    "    },\n",
    "\n",
    "    # ==========================================\n",
    "    # C. ç”Ÿæˆåƒæ•¸ï¼ˆå„ªåŒ–ä»¥é¿å… CUDA OOMï¼‰\n",
    "    # ==========================================\n",
    "    \"generation\": {\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_questions\": 20,      # ðŸ”¥ æ¸›å°‘æ¸¬è©¦æ•¸é‡\n",
    "        \"context_window\": 4096,\n",
    "        \"batch_size\": 1,          # ðŸ”¥ å–®æ¬¡è™•ç†ä¸€å€‹ chunk\n",
    "        \"max_workers\": 1          # ðŸ”¥ é¿å…ä¸¦è¡Œé€ æˆè¨˜æ†¶é«”æº¢å‡º\n",
    "    },\n",
    "\n",
    "    # ==========================================\n",
    "    # D. ç¬¬ä¸€éšŽæ®µï¼šç´¢å¼•æ¶ˆèžç¶²æ ¼ï¼ˆç°¡åŒ–æ¸¬è©¦ï¼‰\n",
    "    # ==========================================\n",
    "    \"indexing_grid\": [\n",
    "        {\"chunk_size\": 2048, \"overlap\": 512},  # åƒ…æ¸¬è©¦æœ€ä½³é…ç½®\n",
    "    ],\n",
    "    \"optimal_indexing\": {\"chunk_size\": 2048, \"overlap\": 512},\n",
    "\n",
    "    # ==========================================\n",
    "    # E. ç¬¬ä¸‰éšŽæ®µï¼šåœ–è­œå„ªåŒ–åƒæ•¸\n",
    "    # ==========================================\n",
    "    \"optimization\": {\n",
    "        \"hub_threshold_percentile\": 95,\n",
    "        \"max_iterations\": 2,      # ðŸ”¥ æ¸›å°‘è¿­ä»£æ¬¡æ•¸\n",
    "        \"quality_threshold\": 2.0\n",
    "    },\n",
    "\n",
    "    # ==========================================\n",
    "    # F. ç¬¬å››éšŽæ®µï¼šæª¢ç´¢æ¶ˆèžç¶²æ ¼ï¼ˆç°¡åŒ–æ¸¬è©¦ï¼‰\n",
    "    # ==========================================\n",
    "    \"retrieval_grid\": {\n",
    "        \"hop_counts\": [1, 2],     # ðŸ”¥ æ¸›å°‘æ¸¬è©¦çµ„åˆ\n",
    "        \"top_k_values\": [10, 15],\n",
    "        \"rerank_thresholds\": [0.6, 0.75]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… é…ç½®è¼‰å…¥å®Œæˆ\")\n",
    "print(f\"ðŸ“ çŸ¥è­˜åº«è·¯å¾‘: {KNOWLEDGE_BASE_PATH}\")\n",
    "print(f\"ðŸ“ å•é¡Œé›†è·¯å¾‘: {QUESTION_DATASET_PATH}\")\n",
    "print(f\"ðŸ”§ Neo4j URI: {CONFIG['infrastructure']['neo4j_uri']}\")\n",
    "print(f\"ðŸ¤– æŽ¨è«–æ¨¡åž‹: {CONFIG['models']['llm_model']}\")\n",
    "print(f\"ðŸ“Š ç´¢å¼•æ¶ˆèžå¯¦é©—çµ„æ•¸: {len(CONFIG['indexing_grid'])}\")\n",
    "print(f\"ðŸ“Š æª¢ç´¢æ¶ˆèžå¯¦é©—çµ„æ•¸: {len(CONFIG['retrieval_grid']['hop_counts']) * len(CONFIG['retrieval_grid']['top_k_values'])}\")\n",
    "print(f\"ðŸ”¥ è‡ªå®šç¾©ä¸‰å…ƒçµ„æŠ½å– Prompt å·²è¼‰å…¥ï¼ˆ{len(TRIPLE_PROMPT_TEMPLATE)} å­—å…ƒï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51dc973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU å¯ç”¨: NVIDIA GeForce RTX 4090\n",
      "   è¨˜æ†¶é«”: 23.99 GB\n",
      "âœ… æ‰€æœ‰ä¾è³´å¥—ä»¶å°Žå…¥å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“¦ ç¬¬ 2 æ­¥ï¼šä¾è³´å¥—ä»¶å°Žå…¥èˆ‡è¨˜æ†¶é«”ç®¡ç†\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# Neo4j & GraphRAG\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# ðŸ”¥ ç›´æŽ¥ä½¿ç”¨ ollama å¥—ä»¶ï¼ˆé¿å… neo4j_graphrag çš„æ•´åˆå•é¡Œï¼‰\n",
    "import ollama\n",
    "\n",
    "# è©•ä¼°æŒ‡æ¨™\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ðŸ”¥ GPU èˆ‡è¨˜æ†¶é«”ç®¡ç†\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU å¯ç”¨: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    # è¨­ç½®è¨˜æ†¶é«”å„ªåŒ–\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"âš ï¸ GPU ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ CPU\")\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"æ¸…ç† GPU å’Œç³»çµ±è¨˜æ†¶é«”\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# ðŸ”¥ å°è£ Ollama å®¢æˆ¶ç«¯\n",
    "class OllamaEmbedder:\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "        self.client = ollama.Client()\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"ç”Ÿæˆæ–‡å­—åµŒå…¥\"\"\"\n",
    "        response = self.client.embeddings(model=self.model, prompt=text)\n",
    "        return response['embedding']\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"æ‰¹æ¬¡ç”ŸæˆåµŒå…¥\"\"\"\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "class OllamaLLMWrapper:\n",
    "    def __init__(self, model_name: str, temperature: float = 0.0):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.client = ollama.Client()\n",
    "    \n",
    "    def invoke(self, prompt: str) -> Any:\n",
    "        \"\"\"å‘¼å« LLM ç”Ÿæˆå›žæ‡‰\"\"\"\n",
    "        response = self.client.generate(\n",
    "            model=self.model_name,\n",
    "            prompt=prompt,\n",
    "            options={\"temperature\": self.temperature}\n",
    "        )\n",
    "        # åŒ…è£æˆé¡žä¼¼ langchain çš„æ ¼å¼\n",
    "        class Response:\n",
    "            def __init__(self, content):\n",
    "                self.content = content\n",
    "        return Response(response['response'])\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰ä¾è³´å¥—ä»¶å°Žå…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c086afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ç’°å¢ƒé©—è­‰æ¸¬è©¦\n",
      "============================================================\n",
      "\n",
      "[1/4] æ¸¬è©¦ Neo4j é€£æŽ¥...\n",
      "âœ… Neo4j é€£æŽ¥æˆåŠŸ\n",
      "\n",
      "[2/4] æ¸¬è©¦ Ollama æœå‹™...\n",
      "âœ… Ollama Embeddings æ­£å¸¸ (ç¶­åº¦: 768)\n",
      "âœ… Ollama LLM æ­£å¸¸ (å›žæ‡‰: Hello! How can I assist you today? ðŸ˜Š...)\n",
      "\n",
      "[3/4] æ¸¬è©¦è³‡æ–™æª”æ¡ˆ...\n",
      "âœ… çŸ¥è­˜åº«æª”æ¡ˆå­˜åœ¨ (679.61 KB)\n",
      "âœ… å•é¡Œé›†æª”æ¡ˆå­˜åœ¨ (15 å€‹å•é¡Œ)\n",
      "\n",
      "[4/4] æ¸¬è©¦è¨ˆç®—è³‡æº...\n",
      "âœ… CPU æ ¸å¿ƒæ•¸: 28\n",
      "âœ… å¯ç”¨è¨˜æ†¶é«”: 37.26 GB\n",
      "\n",
      "============================================================\n",
      "âœ… ç’°å¢ƒé©—è­‰å®Œæˆï¼å¯ä»¥é–‹å§‹åŸ·è¡Œå¯¦é©—\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ§ª ç’°å¢ƒé©—è­‰æ¸¬è©¦ (Environment Verification)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ç’°å¢ƒé©—è­‰æ¸¬è©¦\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# æ¸¬è©¦ 1: Neo4j é€£æŽ¥\n",
    "print(\"\\n[1/4] æ¸¬è©¦ Neo4j é€£æŽ¥...\")\n",
    "try:\n",
    "    test_driver = GraphDatabase.driver(\n",
    "        CONFIG['infrastructure']['neo4j_uri'],\n",
    "        auth=CONFIG['infrastructure']['neo4j_auth']\n",
    "    )\n",
    "    with test_driver.session() as session:\n",
    "        result = session.run(\"RETURN 1 as test\")\n",
    "        assert result.single()[\"test\"] == 1\n",
    "    test_driver.close()\n",
    "    print(\"âœ… Neo4j é€£æŽ¥æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Neo4j é€£æŽ¥å¤±æ•—: {e}\")\n",
    "\n",
    "# æ¸¬è©¦ 2: Ollama æœå‹™\n",
    "print(\"\\n[2/4] æ¸¬è©¦ Ollama æœå‹™...\")\n",
    "try:\n",
    "    test_embedder = OllamaEmbedder(model=CONFIG['models']['embed_model'])\n",
    "    test_vector = test_embedder.embed_query(\"test\")\n",
    "    assert len(test_vector) > 0\n",
    "    print(f\"âœ… Ollama Embeddings æ­£å¸¸ (ç¶­åº¦: {len(test_vector)})\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama Embeddings å¤±æ•—: {e}\")\n",
    "\n",
    "try:\n",
    "    test_llm = OllamaLLMWrapper(\n",
    "        model_name=CONFIG['models']['llm_model'],\n",
    "        temperature=CONFIG['generation']['temperature']\n",
    "    )\n",
    "    response = test_llm.invoke(\"Say 'Hello'\")\n",
    "    print(f\"âœ… Ollama LLM æ­£å¸¸ (å›žæ‡‰: {response.content[:50]}...)\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama LLM å¤±æ•—: {e}\")\n",
    "\n",
    "# æ¸¬è©¦ 3: æª”æ¡ˆå­˜åœ¨æ€§\n",
    "print(\"\\n[3/4] æ¸¬è©¦è³‡æ–™æª”æ¡ˆ...\")\n",
    "if KNOWLEDGE_BASE_PATH.exists():\n",
    "    print(f\"âœ… çŸ¥è­˜åº«æª”æ¡ˆå­˜åœ¨ ({KNOWLEDGE_BASE_PATH.stat().st_size / 1024:.2f} KB)\")\n",
    "else:\n",
    "    print(f\"âŒ çŸ¥è­˜åº«æª”æ¡ˆä¸å­˜åœ¨: {KNOWLEDGE_BASE_PATH}\")\n",
    "\n",
    "if QUESTION_DATASET_PATH.exists():\n",
    "    df = pd.read_csv(QUESTION_DATASET_PATH)\n",
    "    print(f\"âœ… å•é¡Œé›†æª”æ¡ˆå­˜åœ¨ ({len(df)} å€‹å•é¡Œ)\")\n",
    "else:\n",
    "    print(f\"âŒ å•é¡Œé›†æª”æ¡ˆä¸å­˜åœ¨: {QUESTION_DATASET_PATH}\")\n",
    "\n",
    "# æ¸¬è©¦ 4: è¨ˆç®—è³‡æº\n",
    "print(\"\\n[4/4] æ¸¬è©¦è¨ˆç®—è³‡æº...\")\n",
    "import psutil\n",
    "print(f\"âœ… CPU æ ¸å¿ƒæ•¸: {psutil.cpu_count()}\")\n",
    "print(f\"âœ… å¯ç”¨è¨˜æ†¶é«”: {psutil.virtual_memory().available / (1024**3):.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… ç’°å¢ƒé©—è­‰å®Œæˆï¼å¯ä»¥é–‹å§‹åŸ·è¡Œå¯¦é©—\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5e226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a40e6df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒå·¥å…·å‡½æ•¸å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ”§ ç¬¬ 3 æ­¥ï¼šæ ¸å¿ƒå·¥å…·å‡½æ•¸\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.1 ç³»çµ±æç¤ºè©ž (Section 3.4 Knowledge Grounding)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "GRAPH_RAG_SYSTEM_PROMPT = \"\"\"\n",
    "You are a domain expert in Goat Disease Diagnosis.\n",
    "Answer the question based STRICTLY on the provided Knowledge Graph Context.\n",
    "\n",
    "### â›” Constraints:\n",
    "1. **No Hallucination**: Do NOT use internal knowledge. If the answer is not in the context, say \"Insufficient information.\"\n",
    "2. **Traceability**: Explicitly cite the relations used (e.g., \"According to [Disease -> has_symptom -> Symptom]...\").\n",
    "3. **Format**: Use concise SVO (Subject-Verb-Object) sentences.\n",
    "\n",
    "### ðŸ” Context:\n",
    "{context_str}\n",
    "\n",
    "### ðŸ‘¤ Question:\n",
    "{query_str}\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.2 æ•¸æ“šè¨˜éŒ„å‡½æ•¸\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def save_jsonl_log(record: Dict[str, Any], filename: str):\n",
    "    \"\"\"\n",
    "    å„²å­˜å–®ä¸€å•é¡Œçš„è©³ç´°æ—¥èªŒè‡³ JSONL æª”æ¡ˆ\n",
    "    \n",
    "    Args:\n",
    "        record: åŒ…å«å•é¡Œã€æª¢ç´¢çµæžœã€ç­”æ¡ˆã€æŒ‡æ¨™ç­‰çš„å­—å…¸\n",
    "        filename: è¼¸å‡ºæª”æ¡ˆåç¨±\n",
    "    \"\"\"\n",
    "    with open(filename, 'a', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def save_csv_metric(row_dict: Dict[str, Any], filename: str):\n",
    "    \"\"\"\n",
    "    å„²å­˜å¯¦é©—æ‰¹æ¬¡çš„çµ±è¨ˆæŒ‡æ¨™è‡³ CSV æª”æ¡ˆ\n",
    "    \n",
    "    Args:\n",
    "        row_dict: åŒ…å«éšŽæ®µã€åƒæ•¸ã€å¹³å‡æŒ‡æ¨™ç­‰çš„å­—å…¸\n",
    "        filename: è¼¸å‡ºæª”æ¡ˆåç¨±\n",
    "    \"\"\"\n",
    "    file_exists = Path(filename).exists()\n",
    "    \n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row_dict.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.3 è©•ä¼°æŒ‡æ¨™è¨ˆç®—\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def calculate_f1_score(predicted: str, reference: str) -> float:\n",
    "    \"\"\"è¨ˆç®—åŸºæ–¼è©žå½™çš„ F1 åˆ†æ•¸\"\"\"\n",
    "    pred_tokens = set(predicted.lower().split())\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    common = pred_tokens & ref_tokens\n",
    "    precision = len(common) / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "    recall = len(common) / len(ref_tokens) if len(ref_tokens) > 0 else 0\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def calculate_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"è¨ˆç®—å…©æ®µæ–‡å­—çš„é¤˜å¼¦ç›¸ä¼¼åº¦\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf = vectorizer.fit_transform([text1, text2])\n",
    "        return cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.4 åœ–è­œå“è³ªæŒ‡æ¨™\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def calculate_graph_metrics(driver) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    è¨ˆç®—åœ–è­œå“è³ªæŒ‡æ¨™\n",
    "    \n",
    "    Returns:\n",
    "        - relation_density: å¹³å‡æ¯å€‹å¯¦é«”çš„é—œä¿‚æ•¸é‡\n",
    "        - weak_entity_ratio: å­¤ç«‹æˆ–ä½Žé€£æŽ¥å¯¦é«”çš„æ¯”ä¾‹\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        # è¨ˆç®—é—œä¿‚å¯†åº¦\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e:__Entity__)\n",
    "            OPTIONAL MATCH (e)-[r]->()\n",
    "            WITH e, count(r) as degree\n",
    "            RETURN avg(degree) as avg_degree, \n",
    "                   sum(CASE WHEN degree <= 1 THEN 1 ELSE 0 END) * 1.0 / count(e) as weak_ratio\n",
    "        \"\"\")\n",
    "        record = result.single()\n",
    "        \n",
    "        return {\n",
    "            \"relation_density\": record[\"avg_degree\"] if record else 0.0,\n",
    "            \"weak_entity_ratio\": record[\"weak_ratio\"] if record else 0.0\n",
    "        }\n",
    "\n",
    "print(\"âœ… æ ¸å¿ƒå·¥å…·å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e405063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… çŸ¥è­˜åœ–è­œå»ºæ§‹å‡½æ•¸å®šç¾©å®Œæˆï¼ˆä½¿ç”¨è‡ªå®šç¾©é«˜å¯†åº¦ Promptï¼‰\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ—ï¸ ç¬¬ 4 æ­¥ï¼šçŸ¥è­˜åœ–è­œå»ºæ§‹å‡½æ•¸ï¼ˆä½¿ç”¨è‡ªå®šç¾© Promptï¼‰\n",
    "# ============================================================\n",
    "\n",
    "def simple_text_splitter(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:\n",
    "    \"\"\"ç°¡å–®çš„æ–‡å­—åˆ‡å¡Šå‡½æ•¸\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - chunk_overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_json_from_response(content: str) -> List[Dict]:\n",
    "    \"\"\"å¾ž LLM å›žæ‡‰ä¸­æå– JSON é™£åˆ—\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # ç¢ºä¿ content æ˜¯å­—ä¸²\n",
    "    if not isinstance(content, str):\n",
    "        content = str(content)\n",
    "    \n",
    "    # æ–¹æ³• 1: æå– markdown åŒ…è£¹çš„ JSON\n",
    "    if \"```json\" in content:\n",
    "        content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in content:\n",
    "        # æ‰¾æ‰€æœ‰ ``` å€å¡Š\n",
    "        matches = re.findall(r'```(?:json)?\\n?(.*?)\\n?```', content, re.DOTALL)\n",
    "        if matches:\n",
    "            content = matches[0].strip()\n",
    "    \n",
    "    # æ–¹æ³• 2: å°‹æ‰¾ç¬¬ä¸€å€‹ JSON é™£åˆ—\n",
    "    json_match = re.search(r'\\[\\s*\\{.*?\\}\\s*\\]', content, re.DOTALL)\n",
    "    if json_match:\n",
    "        content = json_match.group(0)\n",
    "    \n",
    "    # æ¸…ç†è¨»è§£\n",
    "    content = '\\n'.join([line for line in content.split('\\n') \n",
    "                         if not line.strip().startswith('//')])\n",
    "    \n",
    "    # å˜—è©¦è§£æž\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def build_knowledge_graph_with_custom_prompt(\n",
    "    text_content: str,\n",
    "    chunk_size: int,\n",
    "    chunk_overlap: int,\n",
    "    driver,\n",
    "    embedder,\n",
    "    llm\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    å»ºæ§‹çŸ¥è­˜åœ–è­œï¼ˆä½¿ç”¨è‡ªå®šç¾©é«˜å¯†åº¦ Promptï¼‰\n",
    "    \n",
    "    Args:\n",
    "        text_content: çŸ¥è­˜åº«æ–‡å­—å…§å®¹\n",
    "        chunk_size: æ–‡å­—åˆ‡å¡Šå¤§å°\n",
    "        chunk_overlap: åˆ‡å¡Šé‡ç–Šå¤§å°\n",
    "        driver: Neo4j é©…å‹•å™¨\n",
    "        embedder: åµŒå…¥æ¨¡åž‹\n",
    "        llm: å¤§åž‹èªžè¨€æ¨¡åž‹\n",
    "        \n",
    "    Returns:\n",
    "        åŒ…å«å»ºæ§‹çµ±è¨ˆè³‡è¨Šçš„å­—å…¸\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # æ¸…ç©ºç¾æœ‰åœ–è­œ\n",
    "    print(\"ðŸ—‘ï¸ æ¸…ç©ºç¾æœ‰åœ–è­œ...\")\n",
    "    with driver.session() as session:\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        session.run(\"DROP INDEX chunk_embeddings IF EXISTS\")\n",
    "        session.run(\"DROP INDEX chunk_text_fts IF EXISTS\")\n",
    "    \n",
    "    # æ¸…ç†è¨˜æ†¶é«”\n",
    "    clear_memory()\n",
    "    \n",
    "    # 1. æ–‡å­—åˆ‡å¡Š\n",
    "    print(f\"âœ‚ï¸ åˆ‡å¡Šä¸­ (size={chunk_size}, overlap={chunk_overlap})...\")\n",
    "    chunks = simple_text_splitter(text_content, chunk_size, chunk_overlap)\n",
    "    print(f\"   ç”Ÿæˆ {len(chunks)} å€‹ chunks\")\n",
    "    \n",
    "    # 2. ä½¿ç”¨è‡ªå®šç¾© Prompt æŠ½å–ä¸‰å…ƒçµ„\n",
    "    print(f\"ðŸ” ä½¿ç”¨é«˜å¯†åº¦ Prompt æŠ½å–ä¸‰å…ƒçµ„...\")\n",
    "    \n",
    "    all_entities = set()\n",
    "    all_relations = []\n",
    "    successful_chunks = 0\n",
    "    failed_chunks = 0\n",
    "    \n",
    "    for idx, chunk_text in enumerate(chunks):\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"   è™•ç†é€²åº¦: {idx+1}/{len(chunks)} (æˆåŠŸ: {successful_chunks}, å¤±æ•—: {failed_chunks})\")\n",
    "            clear_memory()\n",
    "        \n",
    "        try:\n",
    "            # æº–å‚™ Prompt\n",
    "            prompt = TRIPLE_PROMPT_TEMPLATE.format(\n",
    "                language=CONFIG['models']['answer_language'],\n",
    "                chunk=chunk_text\n",
    "            )\n",
    "            \n",
    "            # å‘¼å« LLM æŠ½å–\n",
    "            response = llm.invoke(prompt)\n",
    "            content = response.content\n",
    "            \n",
    "            # æå– JSON\n",
    "            triples = extract_json_from_response(content)\n",
    "            \n",
    "            if not triples:\n",
    "                failed_chunks += 1\n",
    "                continue\n",
    "            \n",
    "            # 3. å„²å­˜åˆ° Neo4j\n",
    "            with driver.session() as session:\n",
    "                # å»ºç«‹ Chunk ç¯€é»ž\n",
    "                chunk_id = f\"chunk_{idx}\"\n",
    "                session.run(\"\"\"\n",
    "                    CREATE (c:Chunk {\n",
    "                        id: $chunk_id,\n",
    "                        text: $text,\n",
    "                        index: $index\n",
    "                    })\n",
    "                \"\"\", {\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"text\": chunk_text,\n",
    "                    \"index\": idx\n",
    "                })\n",
    "                \n",
    "                # å»ºç«‹å¯¦é«”å’Œé—œä¿‚\n",
    "                for triple in triples:\n",
    "                    head = triple.get('head', '').strip()\n",
    "                    relation = triple.get('relation', '').strip()\n",
    "                    tail = triple.get('tail', '').strip()\n",
    "                    \n",
    "                    if not (head and relation and tail):\n",
    "                        continue\n",
    "                    \n",
    "                    # å»ºç«‹å¯¦é«”ç¯€é»ž\n",
    "                    session.run(\"\"\"\n",
    "                        MERGE (e1:__Entity__ {id: $head})\n",
    "                        MERGE (e2:__Entity__ {id: $tail})\n",
    "                    \"\"\", {\"head\": head, \"tail\": tail})\n",
    "                    \n",
    "                    # å»ºç«‹é—œä¿‚\n",
    "                    session.run(f\"\"\"\n",
    "                        MATCH (e1:__Entity__ {{id: $head}})\n",
    "                        MATCH (e2:__Entity__ {{id: $tail}})\n",
    "                        MERGE (e1)-[r:RELATION {{type: $relation}}]->(e2)\n",
    "                    \"\"\", {\"head\": head, \"tail\": tail, \"relation\": relation})\n",
    "                    \n",
    "                    # é€£æŽ¥ Chunk åˆ°å¯¦é«”\n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (c:Chunk {id: $chunk_id})\n",
    "                        MATCH (e:__Entity__ {id: $entity})\n",
    "                        MERGE (c)-[:MENTIONS]->(e)\n",
    "                    \"\"\", {\"chunk_id\": chunk_id, \"entity\": head})\n",
    "                    \n",
    "                    session.run(\"\"\"\n",
    "                        MATCH (c:Chunk {id: $chunk_id})\n",
    "                        MATCH (e:__Entity__ {id: $entity})\n",
    "                        MERGE (c)-[:MENTIONS]->(e)\n",
    "                    \"\"\", {\"chunk_id\": chunk_id, \"entity\": tail})\n",
    "                    \n",
    "                    all_relations.append((head, relation, tail))\n",
    "                    all_entities.add(head)\n",
    "                    all_entities.add(tail)\n",
    "            \n",
    "            successful_chunks += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            failed_chunks += 1\n",
    "            if failed_chunks <= 5:  # åªé¡¯ç¤ºå‰ 5 å€‹éŒ¯èª¤\n",
    "                print(f\"   âš ï¸ Chunk {idx} è™•ç†å¤±æ•—: {str(e)[:100]}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"   æœ€çµ‚çµ±è¨ˆ: æˆåŠŸ {successful_chunks}/{len(chunks)}, å¤±æ•— {failed_chunks}\")\n",
    "    \n",
    "    # 4. ç”ŸæˆåµŒå…¥å‘é‡\n",
    "    print(f\"ðŸ§® ç”ŸæˆåµŒå…¥å‘é‡...\")\n",
    "    with driver.session() as session:\n",
    "        chunks_to_embed = session.run(\"MATCH (c:Chunk) RETURN c.id as id, c.text as text\").data()\n",
    "        \n",
    "        for idx, chunk in enumerate(chunks_to_embed):\n",
    "            if (idx + 1) % 50 == 0:\n",
    "                print(f\"   åµŒå…¥é€²åº¦: {idx+1}/{len(chunks_to_embed)}\")\n",
    "                clear_memory()\n",
    "            \n",
    "            try:\n",
    "                embedding = embedder.embed_query(chunk['text'])\n",
    "                session.run(\"\"\"\n",
    "                    MATCH (c:Chunk {id: $id})\n",
    "                    SET c.embedding = $embedding\n",
    "                \"\"\", {\"id\": chunk['id'], \"embedding\": embedding})\n",
    "            except Exception as e:\n",
    "                print(f\"   âš ï¸ åµŒå…¥å¤±æ•—: {e}\")\n",
    "    \n",
    "    # 5. å»ºç«‹ç´¢å¼•\n",
    "    print(f\"ðŸ“‡ å»ºç«‹ç´¢å¼•...\")\n",
    "    with driver.session() as session:\n",
    "        session.run(f\"\"\"\n",
    "            CREATE VECTOR INDEX {CONFIG['infrastructure']['vector_index_name']} IF NOT EXISTS\n",
    "            FOR (c:Chunk) ON c.embedding\n",
    "            OPTIONS {{indexConfig: {{\n",
    "                `vector.dimensions`: 768,\n",
    "                `vector.similarity_function`: 'cosine'\n",
    "            }}}}\n",
    "        \"\"\")\n",
    "        \n",
    "        session.run(f\"\"\"\n",
    "            CREATE FULLTEXT INDEX {CONFIG['infrastructure']['fulltext_index_name']} IF NOT EXISTS\n",
    "            FOR (c:Chunk) ON EACH [c.text]\n",
    "        \"\"\")\n",
    "    \n",
    "    build_time = time.time() - start_time\n",
    "    \n",
    "    # çµ±è¨ˆè³‡è¨Š\n",
    "    with driver.session() as session:\n",
    "        stats = session.run(\"\"\"\n",
    "            MATCH (c:Chunk) \n",
    "            WITH count(c) as chunk_count\n",
    "            MATCH (e:__Entity__)\n",
    "            WITH chunk_count, count(e) as entity_count\n",
    "            MATCH ()-[r]->()\n",
    "            RETURN chunk_count, entity_count, count(r) as relation_count\n",
    "        \"\"\").single()\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    print(f\"âœ… å»ºæ§‹å®Œæˆï¼š{stats['chunk_count']} chunks, {stats['entity_count']} entities, {stats['relation_count']} relations\")\n",
    "    \n",
    "    return {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"build_time\": build_time,\n",
    "        \"chunk_count\": stats[\"chunk_count\"] if stats else 0,\n",
    "        \"entity_count\": stats[\"entity_count\"] if stats else 0,\n",
    "        \"relation_count\": stats[\"relation_count\"] if stats else 0\n",
    "    }\n",
    "\n",
    "print(\"âœ… çŸ¥è­˜åœ–è­œå»ºæ§‹å‡½æ•¸å®šç¾©å®Œæˆï¼ˆä½¿ç”¨è‡ªå®šç¾©é«˜å¯†åº¦ Promptï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9edac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å‘é‡æª¢ç´¢å™¨å’Œå¤šè·³æª¢ç´¢å™¨å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ” ç¬¬ 5 æ­¥ï¼šæª¢ç´¢å™¨å¯¦ä½œ (Vector & Multi-Hop Retriever)\n",
    "# ============================================================\n",
    "\n",
    "from neo4j_graphrag.retrievers.base import Retriever\n",
    "from neo4j_graphrag.types import RetrieverResultItem, RawSearchResult\n",
    "\n",
    "class VectorRetriever(Retriever):\n",
    "    \"\"\"\n",
    "    ç´”å‘é‡æª¢ç´¢å™¨ï¼ˆä¸ä½¿ç”¨åœ–çµæ§‹ï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        driver,\n",
    "        index_name: str,\n",
    "        embedder,\n",
    "        neo4j_database: str = None,\n",
    "    ):\n",
    "        self.driver = driver\n",
    "        self.index_name = index_name\n",
    "        self.embedder = embedder\n",
    "        self.neo4j_database = neo4j_database\n",
    "        \n",
    "    def search(\n",
    "        self,\n",
    "        query_text: str = None,\n",
    "        query_vector: List[float] = None,\n",
    "        top_k: int = 5,\n",
    "    ) -> RawSearchResult:\n",
    "        \"\"\"åŸ·è¡Œç´”å‘é‡æª¢ç´¢\"\"\"\n",
    "        \n",
    "        # ç²å–æŸ¥è©¢å‘é‡\n",
    "        if query_vector is None and query_text:\n",
    "            query_vector = self.embedder.embed_query(query_text)\n",
    "        \n",
    "        # å‘é‡æª¢ç´¢\n",
    "        with self.driver.session(database=self.neo4j_database) as session:\n",
    "            result = session.run(f\"\"\"\n",
    "                CALL db.index.vector.queryNodes(\n",
    "                    $index_name, \n",
    "                    $top_k, \n",
    "                    $query_vector\n",
    "                )\n",
    "                YIELD node, score\n",
    "                RETURN node.text as text, \n",
    "                       elementId(node) as node_id,\n",
    "                       score\n",
    "                ORDER BY score DESC\n",
    "            \"\"\", {\n",
    "                \"index_name\": self.index_name,\n",
    "                \"top_k\": top_k,\n",
    "                \"query_vector\": query_vector\n",
    "            })\n",
    "            \n",
    "            records_list = list(result)  # Neo4j Record objects\n",
    "        \n",
    "        # æ§‹å»ºè¿”å›žçµæžœ\n",
    "        items = [\n",
    "            RetrieverResultItem(\n",
    "                content=record[\"text\"],\n",
    "                metadata={\"score\": record[\"score\"], \"node_id\": record[\"node_id\"]}\n",
    "            )\n",
    "            for record in records_list\n",
    "        ]\n",
    "        \n",
    "        return RawSearchResult(items=items, records=records_list)\n",
    "\n",
    "\n",
    "class MultiHopRetriever(Retriever):\n",
    "    \"\"\"\n",
    "    æ”¯æ´å¤šè·³æŽ¨ç†çš„è‡ªå®šç¾©æª¢ç´¢å™¨\n",
    "    \n",
    "    æª¢ç´¢ç­–ç•¥:\n",
    "    1. å‘é‡æª¢ç´¢åˆå§‹ Chunks (0-hop)\n",
    "    2. æ“´å±•åˆ° MENTIONS çš„ Entities (1-hop)\n",
    "    3. æ²¿è‘— RELATION é‚Šéæ­·ç›¸é„° Entities (2-hop, 3-hop, ...)\n",
    "    4. æ”¶é›†è·¯å¾‘ä¸Šçš„æ‰€æœ‰ Chunks ä½œç‚ºä¸Šä¸‹æ–‡\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        driver,\n",
    "        embedder,\n",
    "        hop_count: int = 1,\n",
    "        top_k: int = 15,\n",
    "        rerank_threshold: float = 0.6,\n",
    "        vector_index_name: str = \"chunk_embeddings\",\n",
    "        neo4j_database: str = None,\n",
    "    ):\n",
    "        self.driver = driver\n",
    "        self.embedder = embedder\n",
    "        self.hop_count = hop_count\n",
    "        self.top_k = top_k\n",
    "        self.rerank_threshold = rerank_threshold\n",
    "        self.vector_index_name = vector_index_name\n",
    "        self.neo4j_database = neo4j_database\n",
    "        \n",
    "    def search(\n",
    "        self,\n",
    "        query_text: str = None,\n",
    "        query_vector: List[float] = None,\n",
    "        top_k: int = None,\n",
    "    ) -> RawSearchResult:\n",
    "        \"\"\"åŸ·è¡Œå¤šè·³æª¢ç´¢\"\"\"\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_k = self.top_k\n",
    "        \n",
    "        # 1. ç²å–æŸ¥è©¢å‘é‡\n",
    "        if query_vector is None and query_text:\n",
    "            query_vector = self.embedder.embed_query(query_text)\n",
    "        \n",
    "        # 2. å‘é‡æª¢ç´¢åˆå§‹ Chunks\n",
    "        with self.driver.session(database=self.neo4j_database) as session:\n",
    "            initial_result = session.run(f\"\"\"\n",
    "                CALL db.index.vector.queryNodes(\n",
    "                    $index_name, \n",
    "                    $top_k, \n",
    "                    $query_vector\n",
    "                )\n",
    "                YIELD node, score\n",
    "                RETURN node.text as text, \n",
    "                       elementId(node) as node_id,\n",
    "                       score\n",
    "                ORDER BY score DESC\n",
    "            \"\"\", {\n",
    "                \"index_name\": self.vector_index_name,\n",
    "                \"top_k\": top_k,\n",
    "                \"query_vector\": query_vector\n",
    "            })\n",
    "            \n",
    "            initial_records = list(initial_result)  # Neo4j Record objects\n",
    "        \n",
    "        # 3. å¤šè·³æ“´å±•\n",
    "        all_records = list(initial_records)\n",
    "        collected_entities = set()\n",
    "        \n",
    "        for record in initial_records:\n",
    "            # ç²å–æ­¤ Chunk æåŠçš„ Entities\n",
    "            with self.driver.session(database=self.neo4j_database) as session:\n",
    "                entities = session.run(\"\"\"\n",
    "                    MATCH (c:Chunk)\n",
    "                    WHERE elementId(c) = $chunk_id\n",
    "                    MATCH (c)-[:MENTIONS]->(e:__Entity__)\n",
    "                    RETURN DISTINCT elementId(e) as entity_id, e.id as entity_name\n",
    "                    LIMIT 20\n",
    "                \"\"\", {\n",
    "                    \"chunk_id\": record[\"node_id\"]\n",
    "                }).data()\n",
    "                \n",
    "                for entity in entities:\n",
    "                    collected_entities.add(entity[\"entity_id\"])\n",
    "        \n",
    "        # 4. æ²¿è‘—é—œä¿‚é‚Šéæ­·ï¼ˆå¤šè·³ï¼‰\n",
    "        for hop in range(self.hop_count):\n",
    "            if not collected_entities:\n",
    "                break\n",
    "                \n",
    "            with self.driver.session(database=self.neo4j_database) as session:\n",
    "                # ç²å–ç›¸é„°å¯¦é«”\n",
    "                neighbor_result = session.run(\"\"\"\n",
    "                    MATCH (e:__Entity__)\n",
    "                    WHERE elementId(e) IN $entity_ids\n",
    "                    MATCH (e)-[r]->(neighbor:__Entity__)\n",
    "                    WHERE NOT elementId(neighbor) IN $entity_ids\n",
    "                    WITH DISTINCT neighbor\n",
    "                    LIMIT 30\n",
    "                    MATCH (c:Chunk)-[:MENTIONS]->(neighbor)\n",
    "                    RETURN DISTINCT c.text as text,\n",
    "                           elementId(c) as node_id,\n",
    "                           0.5 as score\n",
    "                \"\"\", {\n",
    "                    \"entity_ids\": list(collected_entities)\n",
    "                })\n",
    "                \n",
    "                new_records = list(neighbor_result)\n",
    "                all_records.extend(new_records)\n",
    "        \n",
    "        # 5. é™åˆ¶ç¸½æ•¸é‡ä¸¦æ§‹å»ºè¿”å›žçµæžœ\n",
    "        limited_records = all_records[:top_k * 2]\n",
    "        \n",
    "        items = [\n",
    "            RetrieverResultItem(\n",
    "                content=record[\"text\"],\n",
    "                metadata={\"score\": record[\"score\"], \"node_id\": record[\"node_id\"]}\n",
    "            )\n",
    "            for record in limited_records\n",
    "        ]\n",
    "        \n",
    "        return RawSearchResult(items=items, records=limited_records)\n",
    "\n",
    "print(\"âœ… å‘é‡æª¢ç´¢å™¨å’Œå¤šè·³æª¢ç´¢å™¨å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36ebed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… QA ç”Ÿæˆèˆ‡è©•ä¼°å‡½æ•¸å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ§ª ç¬¬ 6 æ­¥ï¼šQA ç”Ÿæˆèˆ‡è©•ä¼°å‡½æ•¸\n",
    "# ============================================================\n",
    "\n",
    "def generate_answer(\n",
    "    question: str,\n",
    "    retriever,\n",
    "    llm,\n",
    "    reference_answer: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Graph RAG ç”Ÿæˆç­”æ¡ˆä¸¦è©•ä¼°\n",
    "    \n",
    "    Args:\n",
    "        question: å•é¡Œæ–‡å­—\n",
    "        retriever: æª¢ç´¢å™¨å¯¦ä¾‹\n",
    "        llm: å¤§åž‹èªžè¨€æ¨¡åž‹\n",
    "        reference_answer: åƒè€ƒç­”æ¡ˆï¼ˆç”¨æ–¼è©•ä¼°ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "        åŒ…å«ç­”æ¡ˆã€æª¢ç´¢ä¸Šä¸‹æ–‡ã€è©•ä¼°æŒ‡æ¨™çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡\n",
    "    search_result = retriever.search(query_text=question, top_k=15)\n",
    "    context_str = \"\\n\\n\".join([item.content for item in search_result.items])\n",
    "    \n",
    "    # 2. ç”Ÿæˆç­”æ¡ˆ\n",
    "    prompt = GRAPH_RAG_SYSTEM_PROMPT.format(\n",
    "        context_str=context_str,\n",
    "        query_str=question\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    # 3. è©•ä¼°æŒ‡æ¨™\n",
    "    metrics = {}\n",
    "    if reference_answer:\n",
    "        metrics[\"f1_score\"] = calculate_f1_score(response.content, reference_answer)\n",
    "        metrics[\"cosine_similarity\"] = calculate_cosine_similarity(response.content, reference_answer)\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": response.content,\n",
    "        \"context\": context_str,\n",
    "        \"retrieved_chunks\": len(search_result.items),\n",
    "        \"latency\": latency,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "def run_qa_batch(\n",
    "    questions: List[Dict],\n",
    "    retriever,\n",
    "    llm,\n",
    "    max_questions: int = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    æ‰¹æ¬¡åŸ·è¡Œ QA\n",
    "    \n",
    "    Args:\n",
    "        questions: å•é¡Œåˆ—è¡¨ï¼ˆæ¯å€‹å•é¡Œæ˜¯ä¸€å€‹å­—å…¸ï¼ŒåŒ…å« 'question' å’Œå¯é¸çš„ 'answer'ï¼‰\n",
    "        retriever: æª¢ç´¢å™¨å¯¦ä¾‹\n",
    "        llm: å¤§åž‹èªžè¨€æ¨¡åž‹\n",
    "        max_questions: æœ€å¤§å•é¡Œæ•¸é‡é™åˆ¶\n",
    "        \n",
    "    Returns:\n",
    "        çµæžœåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    question_subset = questions[:max_questions] if max_questions else questions\n",
    "    \n",
    "    for idx, q in enumerate(question_subset):\n",
    "        print(f\"[{idx+1}/{len(question_subset)}] è™•ç†å•é¡Œ: {q['question'][:50]}...\")\n",
    "        \n",
    "        result = generate_answer(\n",
    "            question=q['question'],\n",
    "            retriever=retriever,\n",
    "            llm=llm,\n",
    "            reference_answer=q.get('answer')\n",
    "        )\n",
    "        \n",
    "        result['question_id'] = idx\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… QA ç”Ÿæˆèˆ‡è©•ä¼°å‡½æ•¸å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76513211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Algorithm 1 å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸŒŸ ç¬¬ 7 æ­¥ï¼šAlgorithm 1 - æ˜Ÿåž‹æ‹“æ¨¸åµæ¸¬ (Star Topology Detection)\n",
    "# ============================================================\n",
    "\n",
    "def detect_hub_entities(driver, percentile: float = 95) -> List[str]:\n",
    "    \"\"\"\n",
    "    Algorithm 1: åµæ¸¬æ˜Ÿåž‹æ‹“æ¨¸çš„ä¸­å¿ƒç¯€é»ž (Hub Entities)\n",
    "    \n",
    "    ä¾æ“šè«–æ–‡ Section 3.5ï¼š\n",
    "    - è¨ˆç®—æ¯å€‹å¯¦é«”çš„åº¦æ•¸ (Degree)\n",
    "    - è­˜åˆ¥åº¦æ•¸è¶…éŽç¬¬ 95 ç™¾åˆ†ä½çš„å¯¦é«”ä½œç‚º Hub\n",
    "    \n",
    "    Args:\n",
    "        driver: Neo4j é©…å‹•å™¨\n",
    "        percentile: ç™¾åˆ†ä½é–¾å€¼\n",
    "        \n",
    "    Returns:\n",
    "        Hub å¯¦é«”çš„ ID åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        # è¨ˆç®—æ‰€æœ‰å¯¦é«”çš„åº¦æ•¸\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e:__Entity__)\n",
    "            OPTIONAL MATCH (e)-[r]-()\n",
    "            WITH e, count(DISTINCT r) as degree\n",
    "            RETURN e.id as entity_id, degree\n",
    "            ORDER BY degree DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        degrees = [(r[\"entity_id\"], r[\"degree\"]) for r in result]\n",
    "        \n",
    "        if not degrees:\n",
    "            return []\n",
    "        \n",
    "        # è¨ˆç®—ç™¾åˆ†ä½é–¾å€¼\n",
    "        degree_values = [d[1] for d in degrees]\n",
    "        threshold = np.percentile(degree_values, percentile)\n",
    "        \n",
    "        # ç¯©é¸ Hub å¯¦é«”\n",
    "        hub_entities = [entity_id for entity_id, degree in degrees if degree >= threshold]\n",
    "        \n",
    "        print(f\"âœ… åµæ¸¬åˆ° {len(hub_entities)} å€‹ Hub å¯¦é«” (åº¦æ•¸é–¾å€¼: {threshold:.2f})\")\n",
    "        \n",
    "        return hub_entities\n",
    "\n",
    "print(\"âœ… Algorithm 1 å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa067aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Algorithm 2 å®šç¾©å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ”— ç¬¬ 8 æ­¥ï¼šAlgorithm 2 - åœ–è­œå¢žå¼· (Graph Enhancement)\n",
    "# ============================================================\n",
    "\n",
    "def enhance_knowledge_graph(\n",
    "    driver,\n",
    "    llm,\n",
    "    hub_entities: List[str],\n",
    "    questions: List[Dict],\n",
    "    max_iterations: int = 3,\n",
    "    quality_threshold: float = 2.0\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Algorithm 2: çŸ¥è­˜åœ–è­œå¢žå¼·\n",
    "    \n",
    "    ä¾æ“šè«–æ–‡ Section 3.5ï¼š\n",
    "    1. å…¨åŸŸæŽ¨è«– (Global Inference): é€£æŽ¥å­¤ç«‹çš„ Hub å¯¦é«”\n",
    "    2. å•é¡Œå°Žå‘å¯†é›†åŒ– (Question-Oriented Densification): æ ¹æ“šå•é¡Œé›†å¢žå¼·é—œä¿‚\n",
    "    \n",
    "    Args:\n",
    "        driver: Neo4j é©…å‹•å™¨\n",
    "        llm: å¤§åž‹èªžè¨€æ¨¡åž‹ï¼ˆç”¨æ–¼æŽ¨è«–æ–°é—œä¿‚ï¼‰\n",
    "        hub_entities: Hub å¯¦é«”åˆ—è¡¨\n",
    "        questions: å•é¡Œé›†ï¼ˆç”¨æ–¼å°Žå‘å¯†é›†åŒ–ï¼‰\n",
    "        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•¸\n",
    "        quality_threshold: ç›®æ¨™é—œä¿‚å¯†åº¦\n",
    "        \n",
    "    Returns:\n",
    "        å¢žå¼·çµ±è¨ˆè³‡è¨Š\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”— é–‹å§‹åœ–è­œå¢žå¼·...\")\n",
    "    \n",
    "    iteration = 0\n",
    "    initial_metrics = calculate_graph_metrics(driver)\n",
    "    print(f\"åˆå§‹é—œä¿‚å¯†åº¦: {initial_metrics['relation_density']:.2f}\")\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"\\n--- è¿­ä»£ {iteration}/{max_iterations} ---\")\n",
    "        \n",
    "        # Step 1: å…¨åŸŸæŽ¨è«– - é€£æŽ¥å­¤ç«‹çš„ Hub å¯¦é«”\n",
    "        with driver.session() as session:\n",
    "            # æ‰¾å‡ºå­¤ç«‹çš„ Hub å¯¦é«”å°\n",
    "            isolated_pairs = session.run(\"\"\"\n",
    "                MATCH (h1:__Entity__)\n",
    "                WHERE h1.id IN $hub_ids\n",
    "                MATCH (h2:__Entity__)\n",
    "                WHERE h2.id IN $hub_ids AND h1.id < h2.id\n",
    "                WHERE NOT (h1)-[]-(h2)\n",
    "                RETURN h1.id as entity1, h2.id as entity2\n",
    "                LIMIT 10\n",
    "            \"\"\", {\"hub_ids\": hub_entities}).data()\n",
    "            \n",
    "            print(f\"ç™¼ç¾ {len(isolated_pairs)} å°å­¤ç«‹çš„ Hub å¯¦é«”\")\n",
    "            \n",
    "            # ä½¿ç”¨ LLM æŽ¨è«–å¯èƒ½çš„é—œä¿‚\n",
    "            for pair in isolated_pairs:\n",
    "                prompt = f\"\"\"\n",
    "                Based on domain knowledge in Goat Disease:\n",
    "                Entity 1: {pair['entity1']}\n",
    "                Entity 2: {pair['entity2']}\n",
    "                \n",
    "                If there is a logical relation between these entities, \n",
    "                output the relation type (e.g., \"causes\", \"treats\", \"prevents\").\n",
    "                If no relation exists, output \"NONE\".\n",
    "                \n",
    "                Relation Type:\n",
    "                \"\"\"\n",
    "                \n",
    "                try:\n",
    "                    response = llm.invoke(prompt)\n",
    "                    relation_type = response.content.strip()\n",
    "                    \n",
    "                    if relation_type and relation_type != \"NONE\":\n",
    "                        # å»ºç«‹æ–°é—œä¿‚\n",
    "                        session.run(\"\"\"\n",
    "                            MATCH (e1:__Entity__ {id: $entity1})\n",
    "                            MATCH (e2:__Entity__ {id: $entity2})\n",
    "                            MERGE (e1)-[r:INFERRED_RELATION]->(e2)\n",
    "                            SET r.type = $relation_type, r.source = 'global_inference'\n",
    "                        \"\"\", {\n",
    "                            \"entity1\": pair['entity1'],\n",
    "                            \"entity2\": pair['entity2'],\n",
    "                            \"relation_type\": relation_type\n",
    "                        })\n",
    "                        print(f\"  âœ… å»ºç«‹é—œä¿‚: {pair['entity1']} -{relation_type}-> {pair['entity2']}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ æŽ¨è«–å¤±æ•—: {e}\")\n",
    "        \n",
    "        # Step 2: å•é¡Œå°Žå‘å¯†é›†åŒ–\n",
    "        # æ ¹æ“šå•é¡Œé›†ä¸­çš„é—œéµè©žï¼Œå¢žå¼·ç›¸é—œå¯¦é«”ä¹‹é–“çš„é€£æŽ¥\n",
    "        with driver.session() as session:\n",
    "            for q in questions[:20]:  # ä½¿ç”¨å‰ 20 å€‹å•é¡Œ\n",
    "                # æå–å•é¡Œä¸­çš„å¯¦é«”\n",
    "                entities_in_question = session.run(\"\"\"\n",
    "                    MATCH (e:__Entity__)\n",
    "                    WHERE toLower($question) CONTAINS toLower(e.id)\n",
    "                    RETURN e.id as entity_id\n",
    "                    LIMIT 5\n",
    "                \"\"\", {\"question\": q['question']}).data()\n",
    "                \n",
    "                entity_ids = [e['entity_id'] for e in entities_in_question]\n",
    "                \n",
    "                if len(entity_ids) >= 2:\n",
    "                    # å¢žå¼·é€™äº›å¯¦é«”ä¹‹é–“çš„é€£æŽ¥\n",
    "                    for i in range(len(entity_ids)):\n",
    "                        for j in range(i+1, len(entity_ids)):\n",
    "                            session.run(\"\"\"\n",
    "                                MATCH (e1:__Entity__ {id: $entity1})\n",
    "                                MATCH (e2:__Entity__ {id: $entity2})\n",
    "                                WHERE NOT (e1)-[]-(e2)\n",
    "                                MERGE (e1)-[r:QUESTION_LINKED]->(e2)\n",
    "                                SET r.source = 'question_oriented'\n",
    "                            \"\"\", {\n",
    "                                \"entity1\": entity_ids[i],\n",
    "                                \"entity2\": entity_ids[j]\n",
    "                            })\n",
    "        \n",
    "        # æª¢æŸ¥æ˜¯å¦é”åˆ°å“è³ªé–¾å€¼\n",
    "        current_metrics = calculate_graph_metrics(driver)\n",
    "        print(f\"ç•¶å‰é—œä¿‚å¯†åº¦: {current_metrics['relation_density']:.2f}\")\n",
    "        \n",
    "        if current_metrics['relation_density'] >= quality_threshold:\n",
    "            print(f\"âœ… é”åˆ°å“è³ªé–¾å€¼ ({quality_threshold})\")\n",
    "            break\n",
    "    \n",
    "    final_metrics = calculate_graph_metrics(driver)\n",
    "    \n",
    "    return {\n",
    "        \"iterations\": iteration,\n",
    "        \"initial_density\": initial_metrics['relation_density'],\n",
    "        \"final_density\": final_metrics['relation_density'],\n",
    "        \"initial_weak_ratio\": initial_metrics['weak_entity_ratio'],\n",
    "        \"final_weak_ratio\": final_metrics['weak_entity_ratio'],\n",
    "        \"improvement\": final_metrics['relation_density'] - initial_metrics['relation_density']\n",
    "    }\n",
    "\n",
    "print(\"âœ… Algorithm 2 å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c1674",
   "metadata": {},
   "source": [
    "# ðŸš€ ä¸»åŸ·è¡Œæµç¨‹ (Main Execution Workflow)\n",
    "\n",
    "---\n",
    "\n",
    "## æº–å‚™å·¥ä½œ\n",
    "\n",
    "åœ¨åŸ·è¡Œå¯¦é©—å‰ï¼Œè«‹ç¢ºä¿ï¼š\n",
    "1. âœ… Neo4j è³‡æ–™åº«å·²å•Ÿå‹•\n",
    "2. âœ… Ollama æœå‹™å·²å•Ÿå‹•\n",
    "3. âœ… çŸ¥è­˜åº«æª”æ¡ˆ `goat_data_text collection-1.2-eng.txt` å­˜åœ¨\n",
    "4. âœ… å•é¡Œé›†æª”æ¡ˆ `topic.csv` å­˜åœ¨\n",
    "\n",
    "---\n",
    "\n",
    "## åŸ·è¡Œé †åº\n",
    "\n",
    "è«‹ä¾åºåŸ·è¡Œä»¥ä¸‹ 4 å€‹éšŽæ®µçš„ Cellï¼š\n",
    "1. **Phase 1**: ç´¢å¼•å„ªåŒ– (Indexing Optimization)\n",
    "2. **Phase 2**: åŸºæº–ç·šèˆ‡è¨ºæ–· (Baseline & Diagnosis)\n",
    "3. **Phase 3**: çµæ§‹å„ªåŒ– (Structural Optimization)\n",
    "4. **Phase 4**: æª¢ç´¢æ¶ˆèž (Retrieval Ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d397bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Phase 1: ç´¢å¼•å„ªåŒ–æ¶ˆèžå¯¦é©—\n",
      "============================================================\n",
      "\n",
      "ðŸ“‚ è¼‰å…¥è³‡æ–™...\n",
      "âœ… è¼‰å…¥ 694140 å€‹å­—å…ƒçš„çŸ¥è­˜åº«\n",
      "âœ… è¼‰å…¥ 15 å€‹å•é¡Œ\n",
      "\n",
      "ðŸ”’ Phase 1 å›ºå®šæª¢ç´¢åƒæ•¸: Hop=2, Top-K=15, Threshold=0.6\n",
      "\n",
      "============================================================\n",
      "[1/1] æ¸¬è©¦é…ç½®: Chunk=2048, Overlap=512\n",
      "============================================================\n",
      "ðŸ—‘ï¸ æ¸…ç©ºç¾æœ‰åœ–è­œ...\n",
      "âœ‚ï¸ åˆ‡å¡Šä¸­ (size=2048, overlap=512)...\n",
      "   ç”Ÿæˆ 452 å€‹ chunks\n",
      "ðŸ” ä½¿ç”¨é«˜å¯†åº¦ Prompt æŠ½å–ä¸‰å…ƒçµ„...\n",
      "   è™•ç†é€²åº¦: 10/452 (æˆåŠŸ: 9, å¤±æ•—: 0)\n",
      "   è™•ç†é€²åº¦: 20/452 (æˆåŠŸ: 19, å¤±æ•—: 0)\n",
      "   è™•ç†é€²åº¦: 30/452 (æˆåŠŸ: 29, å¤±æ•—: 0)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š Phase 1: ç´¢å¼•å„ªåŒ– (Indexing Optimization)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 1: ç´¢å¼•å„ªåŒ–æ¶ˆèžå¯¦é©—\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åˆå§‹åŒ–é€£æŽ¥\n",
    "driver = GraphDatabase.driver(\n",
    "    CONFIG['infrastructure']['neo4j_uri'],\n",
    "    auth=CONFIG['infrastructure']['neo4j_auth']\n",
    ")\n",
    "\n",
    "# ðŸ”¥ ä½¿ç”¨è‡ªå®šç¾©çš„ Ollama Wrapper\n",
    "embedder = OllamaEmbedder(model=CONFIG['models']['embed_model'])\n",
    "\n",
    "llm = OllamaLLMWrapper(\n",
    "    model_name=CONFIG['models']['llm_model'],\n",
    "    temperature=CONFIG['generation']['temperature']\n",
    ")\n",
    "\n",
    "# è¼‰å…¥çŸ¥è­˜åº«\n",
    "print(\"\\nðŸ“‚ è¼‰å…¥è³‡æ–™...\")\n",
    "with open(KNOWLEDGE_BASE_PATH, 'r', encoding='utf-8') as f:\n",
    "    knowledge_text = f.read()\n",
    "\n",
    "# è¼‰å…¥å•é¡Œé›†\n",
    "questions_df = pd.read_csv(QUESTION_DATASET_PATH)\n",
    "questions = questions_df.to_dict('records')\n",
    "\n",
    "print(f\"âœ… è¼‰å…¥ {len(knowledge_text)} å€‹å­—å…ƒçš„çŸ¥è­˜åº«\")\n",
    "print(f\"âœ… è¼‰å…¥ {len(questions)} å€‹å•é¡Œ\")\n",
    "\n",
    "# åŸ·è¡Œæ¶ˆèžå¯¦é©—\n",
    "phase1_results = []\n",
    "\n",
    "# ðŸŽ¯ Phase 1 & 2 çš„å›ºå®šæª¢ç´¢åƒæ•¸ï¼ˆæŽ§åˆ¶è®Šå› ï¼‰\n",
    "FIXED_RETRIEVAL_PARAMS = {\n",
    "    \"hop_count\": 2,           # å›ºå®š 2 è·³ - Graph RAG çš„æ¨™æº–æ·±åº¦\n",
    "    \"top_k\": 15,              # å›ºå®š Top-K=15 - ç”¨æˆ¶é è¨­å€¼\n",
    "    \"rerank_threshold\": 0.60  # å›ºå®šé–€æª» 0.6 - ä¸­ç­‰åš´æ ¼åº¦\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ”’ Phase 1 å›ºå®šæª¢ç´¢åƒæ•¸: Hop={FIXED_RETRIEVAL_PARAMS['hop_count']}, Top-K={FIXED_RETRIEVAL_PARAMS['top_k']}, Threshold={FIXED_RETRIEVAL_PARAMS['rerank_threshold']}\")\n",
    "\n",
    "for idx, config in enumerate(CONFIG['indexing_grid']):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{idx+1}/{len(CONFIG['indexing_grid'])}] æ¸¬è©¦é…ç½®: Chunk={config['chunk_size']}, Overlap={config['overlap']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # ðŸ”¥ ä½¿ç”¨è‡ªå®šç¾© Prompt å»ºæ§‹åœ–è­œ\n",
    "        build_stats = build_knowledge_graph_with_custom_prompt(\n",
    "            text_content=knowledge_text,\n",
    "            chunk_size=config['chunk_size'],\n",
    "            chunk_overlap=config['overlap'],\n",
    "            driver=driver,\n",
    "            embedder=embedder,\n",
    "            llm=llm\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n  âœ… å»ºæ§‹å®Œæˆ: {build_stats['chunk_count']} chunks, {build_stats['entity_count']} entities, {build_stats['relation_count']} relations\")\n",
    "        \n",
    "        # è¨ˆç®—åœ–è­œå“è³ª\n",
    "        metrics = calculate_graph_metrics(driver)\n",
    "        print(f\"  ðŸ“Š åœ–è­œå“è³ª: Density={metrics['relation_density']:.3f}, Weak Entity Ratio={metrics['weak_entity_ratio']:.2%}\")\n",
    "        \n",
    "        # ðŸ§ª åŸ·è¡Œ QA è©•ä¼°ï¼ˆä½¿ç”¨å›ºå®šæª¢ç´¢åƒæ•¸ï¼‰\n",
    "        print(f\"\\n  ðŸ” åŸ·è¡Œ QA è©•ä¼° (é™åˆ¶ {CONFIG['generation']['max_questions']} å€‹å•é¡Œ)...\")\n",
    "        retriever = MultiHopRetriever(\n",
    "            driver=driver,\n",
    "            embedder=embedder,\n",
    "            hop_count=FIXED_RETRIEVAL_PARAMS['hop_count'],\n",
    "            top_k=FIXED_RETRIEVAL_PARAMS['top_k'],\n",
    "            rerank_threshold=FIXED_RETRIEVAL_PARAMS['rerank_threshold']\n",
    "        )\n",
    "        \n",
    "        qa_results = run_qa_batch(\n",
    "            questions=questions,\n",
    "            retriever=retriever,\n",
    "            llm=llm,\n",
    "            max_questions=CONFIG['generation']['max_questions']\n",
    "        )\n",
    "        \n",
    "        # è¨ˆç®—å¹³å‡æŒ‡æ¨™\n",
    "        valid_results = [r for r in qa_results if r['metrics']]\n",
    "        if valid_results:\n",
    "            avg_f1 = np.mean([r['metrics']['f1_score'] for r in valid_results])\n",
    "            avg_cosine = np.mean([r['metrics']['cosine_similarity'] for r in valid_results])\n",
    "            avg_latency = np.mean([r['latency'] for r in valid_results])\n",
    "        else:\n",
    "            avg_f1 = avg_cosine = avg_latency = 0.0\n",
    "        \n",
    "        print(f\"  âœ… QA å¹³å‡æŒ‡æ¨™: F1={avg_f1:.3f}, Cosine={avg_cosine:.3f}, Latency={avg_latency:.2f}s\")\n",
    "        \n",
    "        # è¨˜éŒ„çµæžœ\n",
    "        result_row = {\n",
    "            \"phase\": \"indexing_optimization\",\n",
    "            \"chunk_size\": config['chunk_size'],\n",
    "            \"overlap\": config['overlap'],\n",
    "            \"hop_count\": FIXED_RETRIEVAL_PARAMS['hop_count'],\n",
    "            \"top_k\": FIXED_RETRIEVAL_PARAMS['top_k'],\n",
    "            \"rerank_threshold\": FIXED_RETRIEVAL_PARAMS['rerank_threshold'],\n",
    "            \"chunk_count\": build_stats['chunk_count'],\n",
    "            \"entity_count\": build_stats['entity_count'],\n",
    "            \"relation_count\": build_stats['relation_count'],\n",
    "            \"relation_density\": metrics['relation_density'],\n",
    "            \"weak_entity_ratio\": metrics['weak_entity_ratio'],\n",
    "            \"avg_f1\": avg_f1,\n",
    "            \"avg_cosine_sim\": avg_cosine,\n",
    "            \"avg_latency\": avg_latency,\n",
    "            \"build_time\": build_stats['build_time']\n",
    "        }\n",
    "        \n",
    "        phase1_results.append(result_row)\n",
    "        save_csv_metric(result_row, \"ablation_chunking.csv\")\n",
    "        \n",
    "        # å„²å­˜è©³ç´°çš„ QA æ—¥èªŒ\n",
    "        for qa_result in qa_results:\n",
    "            log_entry = {\n",
    "                \"phase\": \"indexing_optimization\",\n",
    "                \"chunk_size\": config['chunk_size'],\n",
    "                \"overlap\": config['overlap'],\n",
    "                **qa_result\n",
    "            }\n",
    "            save_jsonl_log(log_entry, \"phase1_qa_details.jsonl\")\n",
    "        \n",
    "        print(f\"  ðŸ’¾ çµæžœå·²å„²å­˜\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ é…ç½®å¤±æ•—: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        clear_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Phase 1 å®Œæˆï¼çµæžœå·²å„²å­˜è‡³ ablation_chunking.csv å’Œ phase1_qa_details.jsonl\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fccf9185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Phase 2: åŸºæº–ç·šèˆ‡è¨ºæ–· - Before State\n",
      "============================================================\n",
      "ðŸ”’ Phase 2 å›ºå®šæª¢ç´¢åƒæ•¸: Hop=2, Top-K=15, Threshold=0.6\n",
      "\n",
      "ä½¿ç”¨æœ€ä½³é…ç½®: Chunk=2048, Overlap=512\n",
      "ðŸ—‘ï¸ æ¸…ç©ºç¾æœ‰åœ–è­œ...\n",
      "âœ‚ï¸ åˆ‡å¡Šä¸­ (size=2048, overlap=512)...\n",
      "   ç”Ÿæˆ 452 å€‹ chunks\n",
      "ðŸ” ä½¿ç”¨é«˜å¯†åº¦ Prompt æŠ½å–ä¸‰å…ƒçµ„...\n",
      "   è™•ç†é€²åº¦: 10/452 (æˆåŠŸ: 9, å¤±æ•—: 0)\n",
      "   è™•ç†é€²åº¦: 20/452 (æˆåŠŸ: 19, å¤±æ•—: 0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m optimal_config = CONFIG[\u001b[33m'\u001b[39m\u001b[33moptimal_indexing\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mä½¿ç”¨æœ€ä½³é…ç½®: Chunk=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_config[\u001b[33m'\u001b[39m\u001b[33mchunk_size\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Overlap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_config[\u001b[33m'\u001b[39m\u001b[33moverlap\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m build_stats = \u001b[43mbuild_knowledge_graph_with_custom_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mknowledge_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimal_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mchunk_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimal_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverlap\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedder\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# è¨ˆç®—åˆå§‹åœ–è­œå“è³ª\u001b[39;00m\n\u001b[32m     33\u001b[39m initial_metrics = calculate_graph_metrics(driver)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mbuild_knowledge_graph_with_custom_prompt\u001b[39m\u001b[34m(text_content, chunk_size, chunk_overlap, driver, embedder, llm)\u001b[39m\n\u001b[32m    108\u001b[39m prompt = TRIPLE_PROMPT_TEMPLATE.format(\n\u001b[32m    109\u001b[39m     language=CONFIG[\u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33manswer_language\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    110\u001b[39m     chunk=chunk_text\n\u001b[32m    111\u001b[39m )\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# å‘¼å« LLM æŠ½å–\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m content = response.content\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# æå– JSON\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mOllamaLLMWrapper.invoke\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m     64\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"å‘¼å« LLM ç”Ÿæˆå›žæ‡‰\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# åŒ…è£æˆé¡žä¼¼ langchain çš„æ ¼å¼\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mResponse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_client.py:127\u001b[39m, in \u001b[36mClient.generate\u001b[39m\u001b[34m(self, model, prompt, system, template, context, stream, raw, format, images, options, keep_alive)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[32m    125\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[33m'\u001b[39m\u001b[33mmust provide a model\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemplate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_encode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moptions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkeep_alive\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_client.py:98\u001b[39m, in \u001b[36mClient._request_stream\u001b[39m\u001b[34m(self, stream, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_stream\u001b[39m(\n\u001b[32m     93\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     94\u001b[39m   *args,\n\u001b[32m     95\u001b[39m   stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     96\u001b[39m   **kwargs,\n\u001b[32m     97\u001b[39m ) -> Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream(*args, **kwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_client.py:69\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, method, url, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, **kwargs) -> httpx.Response:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     72\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kbllm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“ˆ Phase 2: åŸºæº–ç·šèˆ‡è¨ºæ–· (Baseline & Diagnosis)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 2: åŸºæº–ç·šèˆ‡è¨ºæ–· - Before State\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ðŸŽ¯ Phase 2 åŒæ¨£ä½¿ç”¨å›ºå®šæª¢ç´¢åƒæ•¸ï¼ˆæŽ§åˆ¶è®Šå› ï¼‰\n",
    "FIXED_RETRIEVAL_PARAMS = {\n",
    "    \"hop_count\": 2,           # å›ºå®š 2 è·³\n",
    "    \"top_k\": 15,              # å›ºå®š Top-K=15\n",
    "    \"rerank_threshold\": 0.60  # å›ºå®šé–€æª» 0.6\n",
    "}\n",
    "\n",
    "print(f\"ðŸ”’ Phase 2 å›ºå®šæª¢ç´¢åƒæ•¸: Hop={FIXED_RETRIEVAL_PARAMS['hop_count']}, Top-K={FIXED_RETRIEVAL_PARAMS['top_k']}, Threshold={FIXED_RETRIEVAL_PARAMS['rerank_threshold']}\")\n",
    "\n",
    "try:\n",
    "    # ä½¿ç”¨æœ€ä½³ç´¢å¼•é…ç½®å»ºæ§‹åœ–è­œ\n",
    "    optimal_config = CONFIG['optimal_indexing']\n",
    "    print(f\"\\nä½¿ç”¨æœ€ä½³é…ç½®: Chunk={optimal_config['chunk_size']}, Overlap={optimal_config['overlap']}\")\n",
    "\n",
    "    build_stats = build_knowledge_graph_with_custom_prompt(\n",
    "        text_content=knowledge_text,\n",
    "        chunk_size=optimal_config['chunk_size'],\n",
    "        chunk_overlap=optimal_config['overlap'],\n",
    "        driver=driver,\n",
    "        embedder=embedder,\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    # è¨ˆç®—åˆå§‹åœ–è­œå“è³ª\n",
    "    initial_metrics = calculate_graph_metrics(driver)\n",
    "    print(f\"\\nðŸ“Š åˆå§‹åœ–è­œå“è³ªæŒ‡æ¨™:\")\n",
    "    print(f\"  - é—œä¿‚å¯†åº¦ (Relation Density): {initial_metrics['relation_density']:.3f}\")\n",
    "    print(f\"  - å¼±å¯¦é«”æ¯”ä¾‹ (Weak Entity Ratio): {initial_metrics['weak_entity_ratio']:.2%}\")\n",
    "\n",
    "    # æ¸¬è©¦ 1: Vector-Only RAG (åŸºæº–ç·š)\n",
    "    print(\"\\nðŸ” æ¸¬è©¦ 1: Vector-Only RAG (ç´”å‘é‡æª¢ç´¢)...\")\n",
    "    vector_retriever = VectorRetriever(\n",
    "        driver=driver,\n",
    "        index_name=CONFIG['infrastructure']['vector_index_name'],\n",
    "        embedder=embedder\n",
    "    )\n",
    "\n",
    "    vector_results = run_qa_batch(\n",
    "        questions=questions,\n",
    "        retriever=vector_retriever,\n",
    "        llm=llm,\n",
    "        max_questions=10\n",
    "    )\n",
    "\n",
    "    valid_vector = [r for r in vector_results if r['metrics']]\n",
    "    vector_f1 = np.mean([r['metrics']['f1_score'] for r in valid_vector]) if valid_vector else 0.0\n",
    "    vector_cosine = np.mean([r['metrics']['cosine_similarity'] for r in valid_vector]) if valid_vector else 0.0\n",
    "    vector_latency = np.mean([r['latency'] for r in vector_results]) if vector_results else 0.0\n",
    "\n",
    "    print(f\"  âœ… Vector-Only: F1={vector_f1:.3f}, Cosine={vector_cosine:.3f}, Latency={vector_latency:.2f}s\")\n",
    "    \n",
    "    clear_memory()\n",
    "\n",
    "    # æ¸¬è©¦ 2: Initial Graph RAGï¼ˆä½¿ç”¨å›ºå®šæª¢ç´¢åƒæ•¸ï¼‰\n",
    "    print(f\"\\nðŸ” æ¸¬è©¦ 2: Initial Graph RAG (Hop={FIXED_RETRIEVAL_PARAMS['hop_count']}, Top-K={FIXED_RETRIEVAL_PARAMS['top_k']})...\")\n",
    "    graph_retriever = MultiHopRetriever(\n",
    "        driver=driver,\n",
    "        embedder=embedder,\n",
    "        hop_count=FIXED_RETRIEVAL_PARAMS['hop_count'],\n",
    "        top_k=FIXED_RETRIEVAL_PARAMS['top_k'],\n",
    "        rerank_threshold=FIXED_RETRIEVAL_PARAMS['rerank_threshold']\n",
    "    )\n",
    "\n",
    "    graph_results = run_qa_batch(\n",
    "        questions=questions,\n",
    "        retriever=graph_retriever,\n",
    "        llm=llm,\n",
    "        max_questions=10\n",
    "    )\n",
    "\n",
    "    valid_graph = [r for r in graph_results if r['metrics']]\n",
    "    graph_f1 = np.mean([r['metrics']['f1_score'] for r in valid_graph]) if valid_graph else 0.0\n",
    "    graph_cosine = np.mean([r['metrics']['cosine_similarity'] for r in valid_graph]) if valid_graph else 0.0\n",
    "    graph_latency = np.mean([r['latency'] for r in graph_results]) if graph_results else 0.0\n",
    "\n",
    "    print(f\"  âœ… Graph RAG: F1={graph_f1:.3f}, Cosine={graph_cosine:.3f}, Latency={graph_latency:.2f}s\")\n",
    "\n",
    "    # è¨˜éŒ„åŸºæº–ç·šçµæžœ\n",
    "    baseline_results = [\n",
    "        {\n",
    "            \"phase\": \"baseline\",\n",
    "            \"method\": \"Vector-Only RAG\",\n",
    "            \"hop_count\": 0,\n",
    "            \"top_k\": FIXED_RETRIEVAL_PARAMS['top_k'],\n",
    "            \"rerank_threshold\": 0.0,\n",
    "            \"avg_f1\": vector_f1,\n",
    "            \"avg_cosine_sim\": vector_cosine,\n",
    "            \"avg_latency\": vector_latency,\n",
    "            \"relation_density\": initial_metrics['relation_density'],\n",
    "            \"weak_entity_ratio\": initial_metrics['weak_entity_ratio']\n",
    "        },\n",
    "        {\n",
    "            \"phase\": \"baseline\",\n",
    "            \"method\": \"Initial Graph RAG\",\n",
    "            \"hop_count\": FIXED_RETRIEVAL_PARAMS['hop_count'],\n",
    "            \"top_k\": FIXED_RETRIEVAL_PARAMS['top_k'],\n",
    "            \"rerank_threshold\": FIXED_RETRIEVAL_PARAMS['rerank_threshold'],\n",
    "            \"avg_f1\": graph_f1,\n",
    "            \"avg_cosine_sim\": graph_cosine,\n",
    "            \"avg_latency\": graph_latency,\n",
    "            \"relation_density\": initial_metrics['relation_density'],\n",
    "            \"weak_entity_ratio\": initial_metrics['weak_entity_ratio']\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for result in baseline_results:\n",
    "        save_csv_metric(result, \"baseline_comparison.csv\")\n",
    "\n",
    "    # å„²å­˜è©³ç´° QA æ—¥èªŒ\n",
    "    for result in vector_results:\n",
    "        log_entry = {\"phase\": \"baseline\", \"method\": \"Vector-Only\", **result}\n",
    "        save_jsonl_log(log_entry, \"phase2_qa_details.jsonl\")\n",
    "    \n",
    "    for result in graph_results:\n",
    "        log_entry = {\"phase\": \"baseline\", \"method\": \"Graph RAG\", **result}\n",
    "        save_jsonl_log(log_entry, \"phase2_qa_details.jsonl\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Phase 2 å®Œæˆï¼çµæžœå·²å„²å­˜è‡³ baseline_comparison.csv å’Œ phase2_qa_details.jsonl\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Phase 2 åŸ·è¡Œå¤±æ•—: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ”§ Phase 3: çµæ§‹å„ªåŒ– (Structural Optimization)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 3: çµæ§‹å„ªåŒ– - Graph Enhancement\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ðŸŽ¯ Phase 3 åŒæ¨£ä½¿ç”¨å›ºå®šæª¢ç´¢åƒæ•¸ï¼ˆæŽ§åˆ¶è®Šå› ï¼‰\n",
    "FIXED_RETRIEVAL_PARAMS = {\n",
    "    \"hop_count\": 2,\n",
    "    \"top_k\": 15,\n",
    "    \"rerank_threshold\": 0.60\n",
    "}\n",
    "\n",
    "print(f\"ðŸ”’ Phase 3 å›ºå®šæª¢ç´¢åƒæ•¸: Hop={FIXED_RETRIEVAL_PARAMS['hop_count']}, Top-K={FIXED_RETRIEVAL_PARAMS['top_k']}, Threshold={FIXED_RETRIEVAL_PARAMS['rerank_threshold']}\")\n",
    "\n",
    "try:\n",
    "    # Step 1: è¨˜éŒ„å„ªåŒ–å‰çš„ç‹€æ…‹\n",
    "    print(\"\\nðŸ“¸ è¨˜éŒ„å„ªåŒ–å‰çš„åŸºæº–è¡¨ç¾...\")\n",
    "    retriever_before = MultiHopRetriever(\n",
    "        driver=driver,\n",
    "        embedder=embedder,\n",
    "        hop_count=FIXED_RETRIEVAL_PARAMS['hop_count'],\n",
    "        top_k=FIXED_RETRIEVAL_PARAMS['top_k'],\n",
    "        rerank_threshold=FIXED_RETRIEVAL_PARAMS['rerank_threshold']\n",
    "    )\n",
    "    \n",
    "    results_before = run_qa_batch(\n",
    "        questions=questions,\n",
    "        retriever=retriever_before,\n",
    "        llm=llm,\n",
    "        max_questions=10\n",
    "    )\n",
    "    \n",
    "    valid_before = [r for r in results_before if r['metrics']]\n",
    "    f1_before = np.mean([r['metrics']['f1_score'] for r in valid_before]) if valid_before else 0.0\n",
    "    cosine_before = np.mean([r['metrics']['cosine_similarity'] for r in valid_before]) if valid_before else 0.0\n",
    "    \n",
    "    print(f\"  âœ… å„ªåŒ–å‰: F1={f1_before:.3f}, Cosine={cosine_before:.3f}\")\n",
    "    \n",
    "    # Step 2: åµæ¸¬ Hub å¯¦é«”\n",
    "    print(\"\\nðŸŒŸ åŸ·è¡Œ Algorithm 1: æ˜Ÿåž‹æ‹“æ¨¸åµæ¸¬...\")\n",
    "    hub_entities = detect_hub_entities(\n",
    "        driver=driver,\n",
    "        percentile=CONFIG['optimization']['hub_threshold_percentile']\n",
    "    )\n",
    "\n",
    "    print(f\"  ç™¼ç¾ {len(hub_entities)} å€‹ Hub å¯¦é«”\")\n",
    "    if hub_entities:\n",
    "        print(f\"  Hub å¯¦é«”ç¯„ä¾‹:\")\n",
    "        for hub in hub_entities[:10]:\n",
    "            print(f\"    - {hub}\")\n",
    "\n",
    "    # Step 3: åœ–è­œå¢žå¼·\n",
    "    print(\"\\nðŸ”— åŸ·è¡Œ Algorithm 2: åœ–è­œå¢žå¼·...\")\n",
    "    enhancement_stats = enhance_knowledge_graph(\n",
    "        driver=driver,\n",
    "        llm=llm,\n",
    "        hub_entities=hub_entities,\n",
    "        questions=questions,\n",
    "        max_iterations=CONFIG['optimization']['max_iterations'],\n",
    "        quality_threshold=CONFIG['optimization']['quality_threshold']\n",
    "    )\n",
    "\n",
    "    print(f\"\\nðŸ“Š å¢žå¼·çµæžœ:\")\n",
    "    print(f\"  - è¿­ä»£æ¬¡æ•¸: {enhancement_stats['iterations']}\")\n",
    "    print(f\"  - åˆå§‹å¯†åº¦: {enhancement_stats['initial_density']:.3f}\")\n",
    "    print(f\"  - æœ€çµ‚å¯†åº¦: {enhancement_stats['final_density']:.3f}\")\n",
    "    print(f\"  - æå‡å¹…åº¦: {enhancement_stats['improvement']:.3f}\")\n",
    "    print(f\"  - åˆå§‹å¼±å¯¦é«”æ¯”ä¾‹: {enhancement_stats['initial_weak_ratio']:.2%}\")\n",
    "    print(f\"  - æœ€çµ‚å¼±å¯¦é«”æ¯”ä¾‹: {enhancement_stats['final_weak_ratio']:.2%}\")\n",
    "\n",
    "    # Step 4: è©•ä¼°å„ªåŒ–å¾Œçš„æ•ˆæžœ\n",
    "    print(\"\\nðŸ§ª è©•ä¼°å„ªåŒ–å¾Œçš„ QA è¡¨ç¾...\")\n",
    "    retriever_after = MultiHopRetriever(\n",
    "        driver=driver,\n",
    "        embedder=embedder,\n",
    "        hop_count=FIXED_RETRIEVAL_PARAMS['hop_count'],\n",
    "        top_k=FIXED_RETRIEVAL_PARAMS['top_k'],\n",
    "        rerank_threshold=FIXED_RETRIEVAL_PARAMS['rerank_threshold']\n",
    "    )\n",
    "    \n",
    "    results_after = run_qa_batch(\n",
    "        questions=questions,\n",
    "        retriever=retriever_after,\n",
    "        llm=llm,\n",
    "        max_questions=10\n",
    "    )\n",
    "    \n",
    "    valid_after = [r for r in results_after if r['metrics']]\n",
    "    f1_after = np.mean([r['metrics']['f1_score'] for r in valid_after]) if valid_after else 0.0\n",
    "    cosine_after = np.mean([r['metrics']['cosine_similarity'] for r in valid_after]) if valid_after else 0.0\n",
    "    \n",
    "    print(f\"  âœ… å„ªåŒ–å¾Œ: F1={f1_after:.3f}, Cosine={cosine_after:.3f}\")\n",
    "    print(f\"  ðŸ“ˆ æ”¹å–„: Î”F1={f1_after-f1_before:+.3f}, Î”Cosine={cosine_after-cosine_before:+.3f}\")\n",
    "\n",
    "    # è¨˜éŒ„å„ªåŒ–çµæžœ\n",
    "    optimization_result = {\n",
    "        \"phase\": \"optimization\",\n",
    "        \"hop_count\": FIXED_RETRIEVAL_PARAMS['hop_count'],\n",
    "        \"top_k\": FIXED_RETRIEVAL_PARAMS['top_k'],\n",
    "        \"rerank_threshold\": FIXED_RETRIEVAL_PARAMS['rerank_threshold'],\n",
    "        \"iterations\": enhancement_stats['iterations'],\n",
    "        \"initial_density\": enhancement_stats['initial_density'],\n",
    "        \"final_density\": enhancement_stats['final_density'],\n",
    "        \"initial_weak_ratio\": enhancement_stats['initial_weak_ratio'],\n",
    "        \"final_weak_ratio\": enhancement_stats['final_weak_ratio'],\n",
    "        \"improvement\": enhancement_stats['improvement'],\n",
    "        \"hub_count\": len(hub_entities),\n",
    "        \"f1_before\": f1_before,\n",
    "        \"f1_after\": f1_after,\n",
    "        \"f1_improvement\": f1_after - f1_before,\n",
    "        \"cosine_before\": cosine_before,\n",
    "        \"cosine_after\": cosine_after,\n",
    "        \"cosine_improvement\": cosine_after - cosine_before\n",
    "    }\n",
    "\n",
    "    save_csv_metric(optimization_result, \"optimization_results.csv\")\n",
    "    \n",
    "    # å„²å­˜è©³ç´° QA æ—¥èªŒ\n",
    "    for result in results_after:\n",
    "        log_entry = {\"phase\": \"optimization_after\", **result}\n",
    "        save_jsonl_log(log_entry, \"phase3_qa_details.jsonl\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Phase 3 å®Œæˆï¼çµæžœå·²å„²å­˜è‡³ optimization_results.csv å’Œ phase3_qa_details.jsonl\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Phase 3 åŸ·è¡Œå¤±æ•—: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0699f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸŽ¯ Phase 4: æª¢ç´¢æ¶ˆèž (Retrieval Ablation)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 4: æª¢ç´¢æ¶ˆèžå¯¦é©— - After State\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ðŸ”¥ Phase 4 æ˜¯å”¯ä¸€æœƒæ¸¬è©¦ä¸åŒæª¢ç´¢åƒæ•¸çš„éšŽæ®µ\n",
    "print(\"\\nâš ï¸ Phase 4 é–‹å§‹æ¸¬è©¦ä¸åŒçš„ Hop Count å’Œ Top-K çµ„åˆ\")\n",
    "\n",
    "try:\n",
    "    # ä½¿ç”¨å„ªåŒ–å¾Œçš„åœ–è­œé€²è¡Œæª¢ç´¢æ¶ˆèžå¯¦é©—\n",
    "    retrieval_grid = CONFIG['retrieval_grid']\n",
    "\n",
    "    phase4_results = []\n",
    "    jsonl_filename = \"final_experiment.jsonl\"\n",
    "\n",
    "    # æ¸…ç©º JSONL æª”æ¡ˆ\n",
    "    if Path(jsonl_filename).exists():\n",
    "        Path(jsonl_filename).unlink()\n",
    "\n",
    "    total_combinations = len(retrieval_grid['hop_counts']) * len(retrieval_grid['top_k_values'])\n",
    "    print(f\"\\né–‹å§‹æ¸¬è©¦ {len(retrieval_grid['hop_counts'])} Ã— {len(retrieval_grid['top_k_values'])} = {total_combinations} ç¨®çµ„åˆ...\")\n",
    "    print(f\"æ¯ç¨®çµ„åˆæ¸¬è©¦ {min(CONFIG['max_questions'], len(questions))} å€‹å•é¡Œ\\n\")\n",
    "\n",
    "    combination_idx = 0\n",
    "    for hop_count in retrieval_grid['hop_counts']:\n",
    "        for top_k in retrieval_grid['top_k_values']:\n",
    "            combination_idx += 1\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"[{combination_idx}/{total_combinations}] æ¸¬è©¦é…ç½®: Hop={hop_count}, TopK={top_k}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            try:\n",
    "                # å»ºç«‹æª¢ç´¢å™¨\n",
    "                retriever = MultiHopRetriever(\n",
    "                    driver=driver,\n",
    "                    embedder=embedder,\n",
    "                    hop_count=hop_count,\n",
    "                    top_k=top_k,\n",
    "                    rerank_threshold=CONFIG['generation']['rerank_threshold']\n",
    "                )\n",
    "                \n",
    "                # åŸ·è¡Œ QA æ‰¹æ¬¡\n",
    "                batch_results = []\n",
    "                max_questions = min(CONFIG['max_questions'], len(questions))\n",
    "                \n",
    "                for idx, q in enumerate(questions[:max_questions]):\n",
    "                    if (idx + 1) % 5 == 0:\n",
    "                        print(f\"  è™•ç†é€²åº¦: {idx+1}/{max_questions}\")\n",
    "                        clear_memory()\n",
    "                    \n",
    "                    result = generate_answer(\n",
    "                        question=q['question'],\n",
    "                        retriever=retriever,\n",
    "                        llm=llm,\n",
    "                        reference_answer=q.get('answer')\n",
    "                    )\n",
    "                    \n",
    "                    # è¨˜éŒ„è©³ç´°æ—¥èªŒè‡³ JSONL\n",
    "                    log_record = {\n",
    "                        \"question_id\": idx,\n",
    "                        \"question_text\": q['question'],\n",
    "                        \"reference_answer\": q.get('answer', ''),\n",
    "                        \"model_response\": result['answer'],\n",
    "                        \"retrieved_chunks\": result['retrieved_chunks'],\n",
    "                        \"context_preview\": result['context'][:500] + \"...\" if len(result['context']) > 500 else result['context'],\n",
    "                        \"metrics\": result['metrics'],\n",
    "                        \"latency\": result['latency'],\n",
    "                        \"config_snapshot\": {\n",
    "                            \"hop_count\": hop_count,\n",
    "                            \"top_k\": top_k,\n",
    "                            \"rerank_threshold\": CONFIG['generation']['rerank_threshold'],\n",
    "                            \"chunk_size\": CONFIG['optimal_indexing']['chunk_size'],\n",
    "                            \"overlap\": CONFIG['optimal_indexing']['overlap']\n",
    "                        },\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    save_jsonl_log(log_record, jsonl_filename)\n",
    "                    batch_results.append(result)\n",
    "                \n",
    "                # è¨ˆç®—æ‰¹æ¬¡çµ±è¨ˆæŒ‡æ¨™\n",
    "                valid_results = [r for r in batch_results if r['metrics']]\n",
    "                avg_f1 = np.mean([r['metrics']['f1_score'] for r in valid_results]) if valid_results else 0.0\n",
    "                avg_cosine = np.mean([r['metrics']['cosine_similarity'] for r in valid_results]) if valid_results else 0.0\n",
    "                avg_latency = np.mean([r['latency'] for r in batch_results]) if batch_results else 0.0\n",
    "                \n",
    "                # ç²å–ç•¶å‰åœ–è­œå“è³ª\n",
    "                current_metrics = calculate_graph_metrics(driver)\n",
    "                \n",
    "                # è¨˜éŒ„è‡³ CSV\n",
    "                result_row = {\n",
    "                    \"phase\": \"retrieval_ablation\",\n",
    "                    \"hop_count\": hop_count,\n",
    "                    \"top_k\": top_k,\n",
    "                    \"rerank_threshold\": CONFIG['generation']['rerank_threshold'],\n",
    "                    \"avg_f1\": avg_f1,\n",
    "                    \"avg_cosine_sim\": avg_cosine,\n",
    "                    \"avg_latency\": avg_latency,\n",
    "                    \"relation_density\": current_metrics['relation_density'],\n",
    "                    \"weak_entity_ratio\": current_metrics['weak_entity_ratio'],\n",
    "                    \"questions_tested\": len(batch_results)\n",
    "                }\n",
    "                \n",
    "                phase4_results.append(result_row)\n",
    "                save_csv_metric(result_row, \"retrieval_ablation.csv\")\n",
    "                \n",
    "                print(f\"\\n  âœ… çµæžœ: F1={avg_f1:.3f}, Cosine={avg_cosine:.3f}, Latency={avg_latency:.2f}s\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ é…ç½® (Hop={hop_count}, TopK={top_k}) å¤±æ•—: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            finally:\n",
    "                clear_memory()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Phase 4 å®Œæˆï¼\")\n",
    "    print(f\"  - è©³ç´°æ—¥èªŒ: {jsonl_filename}\")\n",
    "    print(f\"  - çµ±è¨ˆè¡¨æ ¼: retrieval_ablation.csv\")\n",
    "    print(f\"  - æ¸¬è©¦çµ„åˆæ•¸: {len(phase4_results)}/{total_combinations}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Phase 4 åŸ·è¡Œå¤±æ•—: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # é—œé–‰é€£æŽ¥\n",
    "    driver.close()\n",
    "    clear_memory()\n",
    "    print(\"\\nâœ… æ‰€æœ‰å¯¦é©—å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e0de21",
   "metadata": {},
   "source": [
    "# ðŸ“Š çµæžœåˆ†æžèˆ‡è¦–è¦ºåŒ–\n",
    "\n",
    "ä»¥ä¸‹ Cell æä¾›å¯¦é©—çµæžœçš„åˆ†æžèˆ‡è¦–è¦ºåŒ–åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“ˆ çµæžœè¦–è¦ºåŒ–ï¼šPhase 1 ç´¢å¼•æ¶ˆèžå¯¦é©—\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# è¨­å®šç¹é«”ä¸­æ–‡å­—åž‹\n",
    "plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è¼‰å…¥ Phase 1 çµæžœ\n",
    "df_phase1 = pd.read_csv(\"ablation_chunking.csv\")\n",
    "\n",
    "# å»ºç«‹è¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. F1 Score vs Chunk Size\n",
    "axes[0, 0].plot(df_phase1['chunk_size'], df_phase1['avg_f1'], marker='o', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Chunk Size', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Average F1 Score', fontsize=12)\n",
    "axes[0, 0].set_title('Phase 1: F1 Score vs Chunk Size', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Cosine Similarity vs Chunk Size\n",
    "axes[0, 1].plot(df_phase1['chunk_size'], df_phase1['avg_cosine_sim'], marker='s', color='orange', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Chunk Size', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Average Cosine Similarity', fontsize=12)\n",
    "axes[0, 1].set_title('Phase 1: Cosine Similarity vs Chunk Size', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Relation Density vs Chunk Size\n",
    "axes[1, 0].plot(df_phase1['chunk_size'], df_phase1['relation_density'], marker='^', color='green', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Chunk Size', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Relation Density', fontsize=12)\n",
    "axes[1, 0].set_title('Phase 1: Relation Density vs Chunk Size', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Latency vs Chunk Size\n",
    "axes[1, 1].plot(df_phase1['chunk_size'], df_phase1['avg_latency'], marker='d', color='red', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Chunk Size', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Average Latency (s)', fontsize=12)\n",
    "axes[1, 1].set_title('Phase 1: Latency vs Chunk Size', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('phase1_ablation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Phase 1 è¦–è¦ºåŒ–å®Œæˆï¼åœ–ç‰‡å·²å„²å­˜ç‚º phase1_ablation_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddea3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š çµæžœè¦–è¦ºåŒ–ï¼šPhase 4 æª¢ç´¢æ¶ˆèžå¯¦é©—ï¼ˆç†±åŠ›åœ–ï¼‰\n",
    "# ============================================================\n",
    "\n",
    "# è¼‰å…¥ Phase 4 çµæžœ\n",
    "df_phase4 = pd.read_csv(\"retrieval_ablation.csv\")\n",
    "\n",
    "# å»ºç«‹ F1 Score çš„æ¨žç´è¡¨\n",
    "pivot_f1 = df_phase4.pivot(index='hop_count', columns='top_k', values='avg_f1')\n",
    "\n",
    "# å»ºç«‹ Cosine Similarity çš„æ¨žç´è¡¨\n",
    "pivot_cosine = df_phase4.pivot(index='hop_count', columns='top_k', values='avg_cosine_sim')\n",
    "\n",
    "# å»ºç«‹è¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. F1 Score ç†±åŠ›åœ–\n",
    "sns.heatmap(pivot_f1, annot=True, fmt='.3f', cmap='YlGnBu', ax=axes[0], cbar_kws={'label': 'F1 Score'})\n",
    "axes[0].set_xlabel('Top-K', fontsize=12)\n",
    "axes[0].set_ylabel('Hop Count', fontsize=12)\n",
    "axes[0].set_title('Phase 4: F1 Score Heatmap (Hop Count Ã— Top-K)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Cosine Similarity ç†±åŠ›åœ–\n",
    "sns.heatmap(pivot_cosine, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[1], cbar_kws={'label': 'Cosine Similarity'})\n",
    "axes[1].set_xlabel('Top-K', fontsize=12)\n",
    "axes[1].set_ylabel('Hop Count', fontsize=12)\n",
    "axes[1].set_title('Phase 4: Cosine Similarity Heatmap (Hop Count Ã— Top-K)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('phase4_retrieval_ablation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Phase 4 è¦–è¦ºåŒ–å®Œæˆï¼åœ–ç‰‡å·²å„²å­˜ç‚º phase4_retrieval_ablation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“‹ çµæžœæ‘˜è¦è¡¨æ ¼ç”Ÿæˆ\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"å¯¦é©—çµæžœæ‘˜è¦ (Experimental Results Summary)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Phase 1: ç´¢å¼•å„ªåŒ–æœ€ä½³é…ç½®\n",
    "df_phase1 = pd.read_csv(\"ablation_chunking.csv\")\n",
    "best_phase1 = df_phase1.loc[df_phase1['avg_f1'].idxmax()]\n",
    "\n",
    "print(\"\\nðŸ“Š Phase 1: ç´¢å¼•å„ªåŒ–æœ€ä½³é…ç½®\")\n",
    "print(f\"  - Chunk Size: {int(best_phase1['chunk_size'])}\")\n",
    "print(f\"  - Overlap: {int(best_phase1['overlap'])}\")\n",
    "print(f\"  - F1 Score: {best_phase1['avg_f1']:.3f}\")\n",
    "print(f\"  - Cosine Similarity: {best_phase1['avg_cosine_sim']:.3f}\")\n",
    "print(f\"  - Relation Density: {best_phase1['relation_density']:.2f}\")\n",
    "\n",
    "# Phase 2: åŸºæº–ç·šæ¯”è¼ƒ\n",
    "try:\n",
    "    df_phase2 = pd.read_csv(\"baseline_comparison.csv\")\n",
    "    print(\"\\nðŸ“ˆ Phase 2: åŸºæº–ç·šæ¯”è¼ƒ\")\n",
    "    for _, row in df_phase2.iterrows():\n",
    "        print(f\"  - {row['method']}:\")\n",
    "        print(f\"      F1 Score: {row['avg_f1']:.3f}\")\n",
    "        print(f\"      Cosine Similarity: {row['avg_cosine_sim']:.3f}\")\n",
    "except:\n",
    "    print(\"\\nâš ï¸ Phase 2 çµæžœæª”æ¡ˆä¸å­˜åœ¨\")\n",
    "\n",
    "# Phase 3: åœ–è­œå„ªåŒ–\n",
    "try:\n",
    "    df_phase3 = pd.read_csv(\"optimization_results.csv\")\n",
    "    opt_result = df_phase3.iloc[-1]\n",
    "    print(\"\\nðŸ”§ Phase 3: åœ–è­œå„ªåŒ–çµæžœ\")\n",
    "    print(f\"  - åˆå§‹é—œä¿‚å¯†åº¦: {opt_result['initial_density']:.2f}\")\n",
    "    print(f\"  - æœ€çµ‚é—œä¿‚å¯†åº¦: {opt_result['final_density']:.2f}\")\n",
    "    print(f\"  - æå‡å¹…åº¦: {opt_result['improvement']:.2f}\")\n",
    "    print(f\"  - Hub å¯¦é«”æ•¸é‡: {int(opt_result['hub_count'])}\")\n",
    "except:\n",
    "    print(\"\\nâš ï¸ Phase 3 çµæžœæª”æ¡ˆä¸å­˜åœ¨\")\n",
    "\n",
    "# Phase 4: æª¢ç´¢æ¶ˆèžæœ€ä½³é…ç½®\n",
    "df_phase4 = pd.read_csv(\"retrieval_ablation.csv\")\n",
    "best_phase4 = df_phase4.loc[df_phase4['avg_f1'].idxmax()]\n",
    "\n",
    "print(\"\\nðŸŽ¯ Phase 4: æª¢ç´¢æ¶ˆèžæœ€ä½³é…ç½®\")\n",
    "print(f\"  - Hop Count: {int(best_phase4['hop_count'])}\")\n",
    "print(f\"  - Top-K: {int(best_phase4['top_k'])}\")\n",
    "print(f\"  - F1 Score: {best_phase4['avg_f1']:.3f}\")\n",
    "print(f\"  - Cosine Similarity: {best_phase4['avg_cosine_sim']:.3f}\")\n",
    "print(f\"  - Average Latency: {best_phase4['avg_latency']:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… æ‰€æœ‰å¯¦é©—çµæžœæ‘˜è¦å®Œæˆï¼\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ç”Ÿæˆ LaTeX è¡¨æ ¼ï¼ˆç”¨æ–¼è«–æ–‡ï¼‰\n",
    "print(\"\\nðŸ“ LaTeX è¡¨æ ¼ï¼ˆç”¨æ–¼è«–æ–‡ Section 4.5ï¼‰ï¼š\")\n",
    "print(\"\\n% Table 1: Chunking Strategy Ablation Study\")\n",
    "print(\"\\\\begin{table}[h]\")\n",
    "print(\"\\\\centering\")\n",
    "print(\"\\\\caption{Impact of Chunking Strategies on System Performance}\")\n",
    "print(\"\\\\begin{tabular}{|c|c|c|c|c|}\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"Chunk Size & Overlap & F1 Score & Cosine Sim & Relation Density \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "for _, row in df_phase1.iterrows():\n",
    "    print(f\"{int(row['chunk_size'])} & {int(row['overlap'])} & {row['avg_f1']:.3f} & {row['avg_cosine_sim']:.3f} & {row['relation_density']:.2f} \\\\\\\\\")\n",
    "print(\"\\\\hline\")\n",
    "print(\"\\\\end{tabular}\")\n",
    "print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ” æ¡ˆä¾‹ç ”ç©¶ (Case Study) - å¾ž JSONL ä¸­æŠ½å–ç¯„ä¾‹\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"æ¡ˆä¾‹ç ”ç©¶ (Case Study) - Section 4.6\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# è®€å– JSONL æ—¥èªŒ\n",
    "jsonl_path = \"final_experiment.jsonl\"\n",
    "\n",
    "if Path(jsonl_path).exists():\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        logs = [json.loads(line) for line in f]\n",
    "    \n",
    "    # æ‰¾å‡º F1 Score æœ€é«˜çš„ 3 å€‹æ¡ˆä¾‹\n",
    "    logs_sorted = sorted(logs, key=lambda x: x['metrics'].get('f1_score', 0), reverse=True)\n",
    "    \n",
    "    print(\"\\nâœ… F1 Score æœ€é«˜çš„ 3 å€‹æ¡ˆä¾‹ï¼š\\n\")\n",
    "    \n",
    "    for idx, log in enumerate(logs_sorted[:3]):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"æ¡ˆä¾‹ {idx+1}:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nðŸ“ å•é¡Œ: {log['question_text']}\")\n",
    "        print(f\"\\nðŸ“Œ åƒè€ƒç­”æ¡ˆ: {log['reference_answer']}\")\n",
    "        print(f\"\\nðŸ¤– æ¨¡åž‹å›žæ‡‰: {log['model_response']}\")\n",
    "        print(f\"\\nðŸ“Š æŒ‡æ¨™:\")\n",
    "        print(f\"  - F1 Score: {log['metrics'].get('f1_score', 0):.3f}\")\n",
    "        print(f\"  - Cosine Similarity: {log['metrics'].get('cosine_similarity', 0):.3f}\")\n",
    "        print(f\"  - Latency: {log['latency']:.2f}s\")\n",
    "        print(f\"\\nðŸ” æª¢ç´¢é…ç½®:\")\n",
    "        print(f\"  - Hop Count: {log['config_snapshot']['hop_count']}\")\n",
    "        print(f\"  - Top-K: {log['config_snapshot']['top_k']}\")\n",
    "        print(f\"\\nðŸ“„ æª¢ç´¢ä¸Šä¸‹æ–‡é è¦½:\")\n",
    "        print(f\"  {log['context_preview']}\")\n",
    "        print()\n",
    "    \n",
    "    # æ‰¾å‡º F1 Score æœ€ä½Žçš„ 3 å€‹æ¡ˆä¾‹ï¼ˆç”¨æ–¼åˆ†æžå¤±æ•—åŽŸå› ï¼‰\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"âš ï¸ F1 Score æœ€ä½Žçš„ 3 å€‹æ¡ˆä¾‹ï¼ˆå¤±æ•—åˆ†æžï¼‰ï¼š\\n\")\n",
    "    \n",
    "    logs_worst = sorted(logs, key=lambda x: x['metrics'].get('f1_score', 0))\n",
    "    \n",
    "    for idx, log in enumerate(logs_worst[:3]):\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"å¤±æ•—æ¡ˆä¾‹ {idx+1}:\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"\\nðŸ“ å•é¡Œ: {log['question_text']}\")\n",
    "        print(f\"\\nðŸ“Œ åƒè€ƒç­”æ¡ˆ: {log['reference_answer']}\")\n",
    "        print(f\"\\nðŸ¤– æ¨¡åž‹å›žæ‡‰: {log['model_response']}\")\n",
    "        print(f\"\\nðŸ“Š æŒ‡æ¨™:\")\n",
    "        print(f\"  - F1 Score: {log['metrics'].get('f1_score', 0):.3f}\")\n",
    "        print(f\"  - Cosine Similarity: {log['metrics'].get('cosine_similarity', 0):.3f}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âš ï¸ JSONL æ—¥èªŒæª”æ¡ˆä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œ Phase 4\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… æ¡ˆä¾‹ç ”ç©¶å®Œæˆï¼\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893dce43",
   "metadata": {},
   "source": [
    "# ðŸ“š ä½¿ç”¨èªªæ˜Žèˆ‡æ³¨æ„äº‹é …\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ åŸ·è¡Œé †åº\n",
    "\n",
    "1. **ä¾åºåŸ·è¡Œä¸Šæ–¹æ‰€æœ‰ Cell**ï¼Œå¾žé…ç½®è¨­å®šåˆ° Phase 4\n",
    "2. **ä¸è¦è·³éŽä»»ä½• Cell**ï¼Œå› ç‚ºå¾ŒçºŒæ­¥é©Ÿä¾è³´å‰é¢çš„å®šç¾©\n",
    "3. **åŸ·è¡Œæ™‚é–“é ä¼°**ï¼š\n",
    "   - Phase 1: ç´„ 30-60 åˆ†é˜ï¼ˆ7 ç¨®é…ç½®ï¼‰\n",
    "   - Phase 2: ç´„ 10-15 åˆ†é˜\n",
    "   - Phase 3: ç´„ 5-10 åˆ†é˜\n",
    "   - Phase 4: ç´„ 1-3 å°æ™‚ï¼ˆå–æ±ºæ–¼å•é¡Œæ•¸é‡ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ è¼¸å‡ºæª”æ¡ˆ\n",
    "\n",
    "åŸ·è¡Œå®Œæˆå¾Œæœƒç”¢ç”Ÿä»¥ä¸‹æª”æ¡ˆï¼š\n",
    "\n",
    "### CSV æª”æ¡ˆï¼ˆçµ±è¨ˆè¡¨æ ¼ï¼‰\n",
    "- `ablation_chunking.csv` - Phase 1 ç´¢å¼•æ¶ˆèžå¯¦é©—çµæžœ\n",
    "- `baseline_comparison.csv` - Phase 2 åŸºæº–ç·šæ¯”è¼ƒ\n",
    "- `optimization_results.csv` - Phase 3 åœ–è­œå„ªåŒ–çµæžœ\n",
    "- `retrieval_ablation.csv` - Phase 4 æª¢ç´¢æ¶ˆèžå¯¦é©—çµæžœ\n",
    "\n",
    "### JSONL æª”æ¡ˆï¼ˆè©³ç´°æ—¥èªŒï¼‰\n",
    "- `final_experiment.jsonl` - Phase 4 æ¯å€‹å•é¡Œçš„å®Œæ•´è»Œè·¡\n",
    "\n",
    "### è¦–è¦ºåŒ–åœ–ç‰‡\n",
    "- `phase1_ablation_results.png` - Phase 1 çµæžœåœ–è¡¨\n",
    "- `phase4_retrieval_ablation_heatmap.png` - Phase 4 ç†±åŠ›åœ–\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ é…ç½®èª¿æ•´\n",
    "\n",
    "å¦‚éœ€èª¿æ•´å¯¦é©—åƒæ•¸ï¼Œè«‹ä¿®æ”¹ç¬¬ä¸€å€‹ Cell ä¸­çš„ `CONFIG` å­—å…¸ï¼š\n",
    "\n",
    "- **æ¸¬è©¦æ›´å¤šå•é¡Œ**: å¢žåŠ  `CONFIG['generation']['max_questions']`\n",
    "- **èª¿æ•´æª¢ç´¢æ·±åº¦**: ä¿®æ”¹ `CONFIG['retrieval_grid']['hop_counts']`\n",
    "- **æ›´æ”¹ Top-K å€¼**: ä¿®æ”¹ `CONFIG['retrieval_grid']['top_k_values']`\n",
    "- **èª¿æ•´åœ–è­œå„ªåŒ–ç›®æ¨™**: ä¿®æ”¹ `CONFIG['optimization']['quality_threshold']`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› æ•…éšœæŽ’é™¤\n",
    "\n",
    "### å•é¡Œï¼šNeo4j é€£ç·šå¤±æ•—\n",
    "**è§£æ±ºæ–¹æ³•**: æª¢æŸ¥ Neo4j æ˜¯å¦å•Ÿå‹•ï¼Œä¸¦ç¢ºèª URI å’Œèªè­‰è³‡è¨Šæ­£ç¢º\n",
    "\n",
    "### å•é¡Œï¼šOllama æ¨¡åž‹è¼‰å…¥å¤±æ•—\n",
    "**è§£æ±ºæ–¹æ³•**: åŸ·è¡Œ `ollama pull deepseek-r1:8b-llama-distill-q4_K_M` å’Œ `ollama pull nomic-embed-text:nomic`\n",
    "\n",
    "### å•é¡Œï¼šè¨˜æ†¶é«”ä¸è¶³\n",
    "**è§£æ±ºæ–¹æ³•**: æ¸›å°‘ `max_questions` æˆ–ä½¿ç”¨æ›´å°çš„ Chunk Size\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– å°æ‡‰è«–æ–‡ç« ç¯€\n",
    "\n",
    "- **Phase 1** â†’ Section 4.5, Table 1\n",
    "- **Phase 2** â†’ Section 4.3, Baseline Comparison\n",
    "- **Phase 3** â†’ Section 3.5, Algorithm 1 & 2\n",
    "- **Phase 4** â†’ Section 4.5, Figure 3; Section 4.6, Case Study\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ“ å¼•ç”¨\n",
    "\n",
    "å¦‚ä½¿ç”¨æ­¤å¯¦ä½œï¼Œè«‹å¼•ç”¨åŽŸè«–æ–‡ï¼š\n",
    "```\n",
    "A Knowledge Graph-Enhanced Large Language Model Framework \n",
    "for Goat Disease Question Answering System\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53de7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# âœ… åŸ·è¡Œå‰æª¢æŸ¥æ¸…å–® (Pre-execution Checklist)\n",
    "\n",
    "åœ¨é–‹å§‹å¯¦é©—å‰ï¼Œè«‹ç¢ºèªä»¥ä¸‹é …ç›®ï¼š\n",
    "\n",
    "## 1. ç’°å¢ƒæª¢æŸ¥\n",
    "```bash\n",
    "# æª¢æŸ¥ Neo4j æ˜¯å¦å•Ÿå‹•\n",
    "# ç€è¦½å™¨è¨ªå•: http://localhost:7474\n",
    "\n",
    "# æª¢æŸ¥ Ollama æ˜¯å¦å•Ÿå‹•\n",
    "ollama list\n",
    "\n",
    "# ç¢ºèªæ¨¡åž‹å·²ä¸‹è¼‰\n",
    "ollama pull deepseek-r1:8b-llama-distill-q4_K_M\n",
    "ollama pull nomic-embed-text:nomic\n",
    "```\n",
    "\n",
    "## 2. æª”æ¡ˆæª¢æŸ¥\n",
    "- âœ… `goat_data_text collection-1.2-eng.txt` å­˜åœ¨\n",
    "- âœ… `topic.csv` å­˜åœ¨\n",
    "\n",
    "## 3. ä¾è³´å¥—ä»¶æª¢æŸ¥\n",
    "```python\n",
    "# åŸ·è¡Œæ­¤ Cell æª¢æŸ¥å¥—ä»¶æ˜¯å¦å®‰è£\n",
    "import sys\n",
    "required_packages = [\n",
    "    'neo4j', 'pandas', 'numpy', 'scikit-learn', \n",
    "    'matplotlib', 'seaborn', 'neo4j_graphrag'\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "        print(f\"âœ… {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {pkg} - è«‹åŸ·è¡Œ: pip install {pkg}\")\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nâš ï¸ è«‹å…ˆå®‰è£ç¼ºå°‘çš„å¥—ä»¶: pip install {' '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nâœ… æ‰€æœ‰ä¾è³´å¥—ä»¶å·²å®‰è£ï¼\")\n",
    "```\n",
    "\n",
    "## 4. åŸ·è¡Œé †åºæé†’\n",
    "1. å¾žä¸Šåˆ°ä¸‹ä¾åºåŸ·è¡Œæ‰€æœ‰ Cell\n",
    "2. Phase 1 æœƒèŠ±è²»è¼ƒé•·æ™‚é–“ï¼ˆç´„ 30-60 åˆ†é˜ï¼‰\n",
    "3. Phase 4 æœƒç”¢ç”Ÿå¤§é‡æ—¥èªŒæª”æ¡ˆ\n",
    "4. å»ºè­°åœ¨åŸ·è¡Œå‰å…ˆå‚™ä»½ç¾æœ‰çš„ Neo4j è³‡æ–™åº«\n",
    "\n",
    "---\n",
    "\n",
    "**æº–å‚™å¥½äº†å—Žï¼Ÿå¾žç¬¬äºŒå€‹ Cell é–‹å§‹åŸ·è¡Œå§ï¼** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
