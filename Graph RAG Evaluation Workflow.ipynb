{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a918cf",
   "metadata": {},
   "source": [
    "# ⚠️ 重要：多跳推理功能啟用說明\n",
    "\n",
    "## 🚨 執行前必讀\n",
    "\n",
    "如果您看到錯誤訊息：\n",
    "```\n",
    "NameError: name 'MultiHopRetriever' is not defined\n",
    "```\n",
    "\n",
    "**解決方法：執行下方的 Cell 2（MultiHopRetriever 類別定義）**\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 正確執行順序\n",
    "\n",
    "### 必須執行的 Cells（按順序）：\n",
    "\n",
    "1. **Cell 2** - 🔍 `MultiHopRetriever` 類別定義\n",
    "   - 定義自定義多跳檢索器\n",
    "   - **⚠️ 必須先執行此 Cell！**\n",
    "\n",
    "2. **Cell 55** - 🎛️ 多跳參數配置\n",
    "   - 設定 `RETRIEVAL_DEPTH`, `MULTIHOP_CONFIG`\n",
    "   \n",
    "3. **Cell 56** - 🧪 `test_multihop_retrieval()` 函數\n",
    "   - 單問題測試函數\n",
    "\n",
    "4. **Cell 57** - 🔬 `run_multihop_ablation()` 函數\n",
    "   - 批量消融實驗函數\n",
    "\n",
    "5. **Cell 58** - 🚀 執行測試\n",
    "   - 取消註釋並執行測試代碼\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 快速啟動\n",
    "\n",
    "```python\n",
    "# 1. 執行 Cell 2（定義 MultiHopRetriever）\n",
    "# 2. 執行 Cell 55（設定參數）\n",
    "# 3. 在 Cell 58 取消註釋以下代碼：\n",
    "\n",
    "results = test_multihop_retrieval(\n",
    "    question=test_q['question'],\n",
    "    reference_answer=test_q.get('answer', ''),\n",
    "    hop_values=[1, 2, 3]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**👇 請繼續執行下方 Cell 2 來定義 MultiHopRetriever 類別**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403e329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MultiHopRetriever 類別已定義\n",
      "\n",
      "📝 支持的跳數:\n",
      "  - 1-hop: Chunk → Entity\n",
      "  - 2-hop: Chunk → Entity → Related Entity → Related Chunks\n",
      "  - 3-hop: Chunk → Entity → [深度遍歷] → Distant Entity → Chunks\n",
      "\n",
      "💡 使用範例:\n",
      "  retriever = MultiHopRetriever(\n",
      "      driver=GRAPH_DRIVER,\n",
      "      vector_index_name=VECTOR_INDEX_NAME,\n",
      "      embedder=GRAPH_EMBEDDER,\n",
      "      retrieval_depth=2  # 設定跳數\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔍 自定義多跳檢索器實現\n",
    "# ============================================================\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from neo4j_graphrag.retrievers.base import Retriever\n",
    "from neo4j_graphrag.types import RetrieverResultItem, RawSearchResult\n",
    "\n",
    "class MultiHopRetriever(Retriever):\n",
    "    \"\"\"\n",
    "    支持多跳推理的自定義檢索器\n",
    "    \n",
    "    檢索策略:\n",
    "    1. 向量檢索初始 Chunks (0-hop)\n",
    "    2. 擴展到 MENTIONS 的 Entities (1-hop)\n",
    "    3. 沿著 RELATION 邊遍歷相鄰 Entities (2-hop, 3-hop, ...)\n",
    "    4. 收集路徑上的所有 Chunks 作為上下文\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        driver,\n",
    "        vector_index_name: str,\n",
    "        embedder,\n",
    "        retrieval_depth: int = 1,\n",
    "        max_entities_per_hop: int = 10,\n",
    "        neo4j_database: str = None,\n",
    "    ):\n",
    "        self.driver = driver\n",
    "        self.vector_index_name = vector_index_name\n",
    "        self.embedder = embedder\n",
    "        self.retrieval_depth = retrieval_depth\n",
    "        self.max_entities_per_hop = max_entities_per_hop\n",
    "        self.neo4j_database = neo4j_database\n",
    "        \n",
    "    def search(\n",
    "        self,\n",
    "        query_text: str = None,\n",
    "        query_vector: List[float] = None,\n",
    "        top_k: int = 5,\n",
    "    ) -> RawSearchResult:\n",
    "        \"\"\"執行多跳檢索\"\"\"\n",
    "        \n",
    "        # 1. 獲取查詢向量\n",
    "        if query_vector is None and query_text is not None:\n",
    "            query_vector = self.embedder.embed_query(query_text)\n",
    "        \n",
    "        # 2. 構建多跳 Cypher 查詢\n",
    "        cypher_query = self._build_multihop_cypher(top_k)\n",
    "        \n",
    "        # 3. 執行查詢並返回原始 Neo4j records\n",
    "        with self.driver.session(database=self.neo4j_database) as session:\n",
    "            result = session.run(\n",
    "                cypher_query,\n",
    "                vector_index_name=self.vector_index_name,\n",
    "                query_vector=query_vector,\n",
    "                top_k=top_k,\n",
    "                max_entities=self.max_entities_per_hop\n",
    "            )\n",
    "            \n",
    "            # RawSearchResult 需要 Neo4j Record 對象，不是 dict\n",
    "            records = list(result)\n",
    "        \n",
    "        # 4. 返回原始搜尋結果\n",
    "        return RawSearchResult(records=records)\n",
    "    \n",
    "    def _build_multihop_cypher(self, top_k: int) -> str:\n",
    "        \"\"\"根據 retrieval_depth 構建不同的 Cypher 查詢\"\"\"\n",
    "        \n",
    "        if self.retrieval_depth == 1:\n",
    "            # 1-hop: Chunk -> Entity (返回標準格式: node, score)\n",
    "            return f\"\"\"\n",
    "            CALL db.index.vector.queryNodes($vector_index_name, $top_k, $query_vector)\n",
    "            YIELD node, score\n",
    "            RETURN node, score\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $top_k\n",
    "            \"\"\"\n",
    "        \n",
    "        elif self.retrieval_depth == 2:\n",
    "            # 2-hop: Chunk -> Entity -> Related Entity -> Related Chunks\n",
    "            return f\"\"\"\n",
    "            CALL db.index.vector.queryNodes($vector_index_name, $top_k, $query_vector)\n",
    "            YIELD node AS initial_chunk, score\n",
    "            \n",
    "            // 1-hop: 獲取初始實體\n",
    "            MATCH (initial_chunk)-[:MENTIONS]->(e1:Entity)\n",
    "            WITH initial_chunk, score, e1\n",
    "            LIMIT $max_entities\n",
    "            \n",
    "            // 2-hop: 擴展到相關實體\n",
    "            OPTIONAL MATCH (e1)-[r:RELATION]->(e2:Entity)\n",
    "            WITH initial_chunk, score, e1, e2, r\n",
    "            LIMIT $max_entities * 2\n",
    "            \n",
    "            // 收集相關 Chunks\n",
    "            OPTIONAL MATCH (related_chunk:Chunk)-[:MENTIONS]->(e2)\n",
    "            WHERE related_chunk <> initial_chunk\n",
    "            \n",
    "            WITH initial_chunk, score, \n",
    "                 collect(DISTINCT related_chunk) AS related_chunks\n",
    "            \n",
    "            // 返回初始 Chunk + 相關 Chunks（標準格式）\n",
    "            UNWIND [initial_chunk] + related_chunks AS node\n",
    "            WITH node, \n",
    "                 CASE WHEN node = initial_chunk THEN score ELSE score * 0.7 END AS adjusted_score\n",
    "            RETURN DISTINCT node, adjusted_score AS score\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $top_k * 2\n",
    "            \"\"\"\n",
    "        \n",
    "        elif self.retrieval_depth == 3:\n",
    "            # 3-hop: 更深層遍歷\n",
    "            return f\"\"\"\n",
    "            CALL db.index.vector.queryNodes($vector_index_name, $top_k, $query_vector)\n",
    "            YIELD node AS initial_chunk, score\n",
    "            \n",
    "            // 多跳路徑遍歷\n",
    "            MATCH path = (initial_chunk)-[:MENTIONS]->(e1:Entity)\n",
    "                        -[:RELATION*0..2]->(e_final:Entity)\n",
    "            WITH initial_chunk, score, e_final, \n",
    "                 length(path) AS path_length\n",
    "            LIMIT $max_entities * 3\n",
    "            \n",
    "            // 收集終點實體相關的 Chunks\n",
    "            OPTIONAL MATCH (related_chunk:Chunk)-[:MENTIONS]->(e_final)\n",
    "            WHERE related_chunk <> initial_chunk\n",
    "            \n",
    "            WITH initial_chunk, score, path_length,\n",
    "                 collect(DISTINCT related_chunk) AS related_chunks\n",
    "            \n",
    "            // 返回標準格式：node, score\n",
    "            UNWIND [initial_chunk] + related_chunks AS node\n",
    "            WITH node,\n",
    "                 CASE \n",
    "                     WHEN node = initial_chunk THEN score \n",
    "                     ELSE score * (0.8 ^ path_length)\n",
    "                 END AS adjusted_score\n",
    "            RETURN DISTINCT node, adjusted_score AS score\n",
    "            ORDER BY score DESC\n",
    "            LIMIT $top_k * 3\n",
    "            \"\"\"\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"不支持的 retrieval_depth: {self.retrieval_depth}\")\n",
    "\n",
    "\n",
    "print(\"✅ MultiHopRetriever 類別已定義\")\n",
    "print(\"\\n📝 支持的跳數:\")\n",
    "print(\"  - 1-hop: Chunk → Entity\")\n",
    "print(\"  - 2-hop: Chunk → Entity → Related Entity → Related Chunks\")\n",
    "print(\"  - 3-hop: Chunk → Entity → [深度遍歷] → Distant Entity → Chunks\")\n",
    "print(\"\\n💡 使用範例:\")\n",
    "print(\"  retriever = MultiHopRetriever(\")\n",
    "print(\"      driver=GRAPH_DRIVER,\")\n",
    "print(\"      vector_index_name=VECTOR_INDEX_NAME,\")\n",
    "print(\"      embedder=GRAPH_EMBEDDER,\")\n",
    "print(\"      retrieval_depth=2  # 設定跳數\")\n",
    "print(\"  )\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141194a6",
   "metadata": {},
   "source": [
    "# 📚 Graph RAG Evaluation Workflow - 執行指南\n",
    "\n",
    "## 🎯 Notebook 概述\n",
    "\n",
    "本 Notebook 提供完整的 **Graph RAG（知識圖譜檢索增強生成）評估工作流程**，包含：\n",
    "- 知識圖譜構建與品質診斷\n",
    "- 圖譜優化（密集化、推理、增強）\n",
    "- RAG 檢索與生成評估\n",
    "- Reranking 消融實驗\n",
    "- 完整的視覺化分析\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 實際執行順序（根據 Execution Count）\n",
    "\n",
    "### ⚡ 正確的執行流程\n",
    "\n",
    "根據實際的執行記錄，本 Notebook 的**正確執行順序**應該是：\n",
    "\n",
    "**🔵 第一階段：RAG 系統初始化（單元格 24-30）**\n",
    "- 單元格 24: 初始化 Embedder 與基本配置 (Exec: 12)\n",
    "- 單元格 25: 載入與處理數據 (Exec: 4)\n",
    "- 單元格 26: 建立 GraphRAG 系統 (Exec: 5)\n",
    "- 單元格 27: 問題集處理 (Exec: 6)\n",
    "- 單元格 28: 測試運行 (Exec: 7)\n",
    "- 單元格 29: 批次評估功能 (Exec: 8)\n",
    "\n",
    "**🟢 第二階段：圖譜診斷與驗證（單元格 5-23）**\n",
    "- 單元格 5-7: 基礎診斷 (Exec: 76-78)\n",
    "- 單元格 11: 配置確認 (Exec: 79)\n",
    "- 單元格 12-14: 知識圖譜構建 (Exec: 80-82)\n",
    "- 單元格 17-18: 圖譜分析 (Exec: 84-85)\n",
    "- 單元格 20-22: 結果驗證 (Exec: 86-88)\n",
    "\n",
    "**🟡 第三階段：消融實驗執行（單元格 30-51）**\n",
    "- 單元格 30: 消融實驗主程式 (Exec: 14)\n",
    "- 單元格 31-46: 結果分析 (Exec: 17-33, 35)\n",
    "- 單元格 48-51: 進階分析 (Exec: 54-56, 62)\n",
    "\n",
    "**🔴 第四階段：Reranking 評估（單元格 52-54）**\n",
    "- 單元格 52-54: 尚未執行\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 推薦執行順序（標準流程）\n",
    "\n",
    "### 🔹 階段 1：RAG 系統初始化（★ 最優先）\n",
    "**單元格範圍：24-30**\n",
    "\n",
    "| 單元格 | 重要性 | 功能說明 | 建議執行順序 |\n",
    "|--------|--------|----------|------------|\n",
    "| 24 | ⭐⭐⭐ | 初始化 Embedder 與 LLM 配置 | **第 1 步** |\n",
    "| 25 | ⭐⭐⭐ | 載入測試問題集 | **第 2 步** |\n",
    "| 26 | ⭐⭐⭐ | 建立 GraphRAG 與 Retriever | **第 3 步** |\n",
    "| 27 | ⭐⭐ | 問題集預處理 | **第 4 步** |\n",
    "| 28 | ⭐⭐ | 單一問答測試 | **第 5 步** |\n",
    "| 29 | ⭐⭐ | 批次評估功能定義 | **第 6 步** |\n",
    "| 30 | ⭐ | 消融實驗準備 | **第 7 步** |\n",
    "\n",
    "**關鍵變數**：`ABLATION_EMBEDDER`, `ABLATION_LLM`, `GRAPH_RAG`, `GRAPH_RETRIEVER`, `QUESTIONS_DF_ABLATION`\n",
    "\n",
    "**⚠️ 重要**：此階段是整個 Notebook 的核心，必須最先執行！\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 階段 2：環境診斷（可選但建議）\n",
    "**單元格範圍：5-11**\n",
    "\n",
    "| 單元格 | 重要性 | 功能說明 |\n",
    "|--------|--------|----------|\n",
    "| 5 | ⭐⭐⭐ | 標準化計數驗證（節點、關係統計） |\n",
    "| 6 | ⭐⭐⭐ | 關係完整性分析 |\n",
    "| 7 | ⭐⭐ | MERGE 邏輯驗證 |\n",
    "| 8-10 | ⭐ | 完整性診斷報告（可選） |\n",
    "| 11 | ⭐⭐ | 配置驗證 |\n",
    "\n",
    "**關鍵變數**：`GRAPH_DRIVER`, `total_entities`, `relation_type_count`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 階段 3：知識圖譜構建與分析\n",
    "**單元格範圍：12-23**\n",
    "\n",
    "| 單元格 | 重要性 | 功能說明 |\n",
    "|--------|--------|----------|\n",
    "| 12 | ⭐⭐⭐ | 載入知識庫文本 |\n",
    "| 13 | ⭐⭐⭐ | 文本分塊 |\n",
    "| 14 | ⭐⭐⭐ | 三元組抽取與 KG 構建 |\n",
    "| 15-16 | ⭐ | 關係處理（可選） |\n",
    "| 17 | ⭐⭐⭐ | 圖譜品質診斷 |\n",
    "| 18 | ⭐⭐⭐ | 圖譜指標視覺化 |\n",
    "| 19 | ⭐⭐ | 圖譜密集化說明（Markdown） |\n",
    "| 20 | ⭐⭐⭐ | 執行密集化優化 |\n",
    "| 21 | ⭐⭐ | 密集化結果檢查 |\n",
    "| 22-23 | ⭐⭐⭐ | 關係推理與驗證 |\n",
    "\n",
    "**關鍵變數**：`KNOWLEDGE_TEXT`, `KNOWLEDGE_CHUNKS`, `GRAPH_METRICS`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 階段 4：檢索參數消融實驗\n",
    "**單元格範圍：30-51**\n",
    "\n",
    "| 單元格 | 重要性 | 功能說明 |\n",
    "|--------|--------|----------|\n",
    "| 30 | ⭐⭐⭐ | 消融實驗主程式 |\n",
    "| 31-46 | ⭐⭐ | 結果分析與視覺化 |\n",
    "| 47-51 | ⭐⭐ | 進階分析工具 |\n",
    "\n",
    "**測試參數**：`alpha`, `top_k`, `ranker` (RRF/NORMALIZED_SUM)\n",
    "\n",
    "**關鍵變數**：`ABLATION_RESULTS_FIXED`, `best_f1_row`, `best_sim_row`\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 階段 5：Reranking 消融實驗\n",
    "**單元格範圍：52-54**\n",
    "\n",
    "| 單元格 | 重要性 | 功能說明 |\n",
    "|--------|--------|----------|\n",
    "| 52 | ⭐⭐⭐ | Reranking 函數定義 |\n",
    "| 53 | ⭐⭐⭐ | 執行 Reranking 消融實驗 |\n",
    "| 54 | ⭐⭐⭐ | Reranking 結果分析 |\n",
    "\n",
    "**測試參數**：\n",
    "- `INITIAL_TOP_K_VALUES`: [10, 15, 20, 25]\n",
    "- `RERANK_THRESHOLD_VALUES`: [0.5, 0.6, 0.7]\n",
    "- `RERANK_TOP_N_VALUES`: [3, 5, 7]\n",
    "- `RETRIEVER_ALPHA_VALUES`: [0.3, 0.5, 0.7]\n",
    "- `RETRIEVER_RANKER_VALUES`: [RRF, NORMALIZED_SUM]\n",
    "\n",
    "**關鍵變數**：`reranking_ablation_df`, `top5_configs`\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 三種使用場景\n",
    "\n",
    "### 📝 場景 A：完整評估流程（推薦新用戶）\n",
    "**執行順序**：24→25→26→27→28→29→5→6→7→11→12→13→14→17→18→20→21→22→30→31-51→52→53→54\n",
    "\n",
    "**預計時間**：3-5 小時\n",
    "\n",
    "**適用對象**：首次使用，需要完整評估\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 場景 B：快速評估（已有圖譜）\n",
    "**執行順序**：24→25→26→27→28→5→6→30→31-46→52→53→54\n",
    "\n",
    "**預計時間**：1-2 小時\n",
    "\n",
    "**適用對象**：已完成知識圖譜構建，只需評估檢索效能\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 場景 C：僅測試 Reranking\n",
    "**執行順序**：24→25→26→27→52→53→54\n",
    "\n",
    "**預計時間**：1-2 小時\n",
    "\n",
    "**適用對象**：只需測試 Reranking 策略效果\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 關鍵變數清單\n",
    "\n",
    "### 🌐 連線配置\n",
    "- `NEO4J_URI`: Neo4j 資料庫連線地址\n",
    "- `NEO4J_USER` / `NEO4J_PASSWORD`: 認證資訊\n",
    "- `OLLAMA_HOST`: Ollama LLM 服務地址\n",
    "- `GRAPH_DRIVER`: Neo4j 驅動實例\n",
    "\n",
    "### 📊 知識圖譜核心\n",
    "- `KNOWLEDGE_TEXT`: 原始知識庫文本\n",
    "- `KNOWLEDGE_CHUNKS`: 分塊後的文本列表\n",
    "- `GRAPH_METRICS`: 圖譜品質指標（密度、平均度等）\n",
    "- `GRAPH_RAG`: GraphRAG 主實例\n",
    "\n",
    "### 🔍 檢索配置\n",
    "- `RETRIEVER_ALPHA`: Vector vs Fulltext 混合比例（0-1）\n",
    "- `RETRIEVER_RANKER`: 排序演算法（RRF/NORMALIZED_SUM）\n",
    "- `TOP_K`: 檢索返回的結果數量\n",
    "\n",
    "### 🧪 評估相關\n",
    "- `QUESTIONS_DF_ABLATION`: 測試問題集\n",
    "- `ABLATION_RESULTS_FIXED`: 消融實驗完整結果\n",
    "- `reranking_ablation_df`: Reranking 實驗結果\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 常見問題\n",
    "\n",
    "### ❌ 執行失敗常見原因\n",
    "\n",
    "1. **Neo4j 連線失敗**\n",
    "   - 檢查 `NEO4J_URI` 是否正確\n",
    "   - 確認 Neo4j 服務已啟動\n",
    "   - 驗證帳號密碼\n",
    "\n",
    "2. **Ollama 模型不可用**\n",
    "   - 確認 Ollama 服務運行中\n",
    "   - 檢查 `LLM_MODEL` 和 `EMBED_MODEL` 是否已下載\n",
    "\n",
    "3. **記憶體不足**\n",
    "   - 減少 `MAX_QUESTIONS` 數量\n",
    "   - 減少 `CHUNK_SIZE`\n",
    "   - 簡化消融實驗參數範圍\n",
    "\n",
    "4. **索引建立失敗**\n",
    "   - 檢查是否已有同名索引\n",
    "   - 確認 Neo4j 版本支援向量索引\n",
    "\n",
    "### 🔧 除錯技巧\n",
    "\n",
    "```python\n",
    "# 檢查關鍵變數是否存在\n",
    "missing_vars = []\n",
    "for var in ['GRAPH_RAG', 'ABLATION_EMBEDDER', 'QUESTIONS_DF_ABLATION']:\n",
    "    if var not in globals():\n",
    "        missing_vars.append(var)\n",
    "        \n",
    "if missing_vars:\n",
    "    print(f\"⚠️ 缺少變數: {missing_vars}\")\n",
    "    print(\"請先執行對應的初始化單元格\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 效能建議\n",
    "\n",
    "### ⚡ 加速執行\n",
    "- 使用 GPU 加速的 Embedding 模型\n",
    "- 限制消融實驗的參數組合數量\n",
    "- 使用較小的測試問題集（20-50 題）\n",
    "\n",
    "### 💾 資源優化\n",
    "- 定期清理 Neo4j 中的舊實驗數據\n",
    "- 使用 JSONL 格式保存中間結果（避免記憶體溢出）\n",
    "- 批次處理大量問題時設定合理的 `batch_size`\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 輸出文件說明\n",
    "\n",
    "### 消融實驗結果\n",
    "- `reranking_ablation_summary_{timestamp}.csv`: 所有配置的摘要\n",
    "- `rerank_ablation_config{XX}_{timestamp}.jsonl`: 單一配置詳細結果\n",
    "\n",
    "### 圖譜診斷\n",
    "- 圖譜指標會直接輸出到 stdout\n",
    "- 視覺化圖表會即時顯示\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 開始使用\n",
    "\n",
    "**建議執行流程**：\n",
    "1. 從上到下依序閱讀 Markdown 說明單元格（單元格 1-5）\n",
    "2. 執行階段 1 的診斷單元格（單元格 6-8）確認環境\n",
    "3. 根據需求選擇對應的執行路徑\n",
    "4. 查看本指南的「快速啟動」章節\n",
    "\n",
    "**祝實驗順利！** 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d2f0a8",
   "metadata": {},
   "source": [
    "# 🎴 快速參考卡\n",
    "\n",
    "## ⚡ 正確的執行順序（重要！）\n",
    "\n",
    "**本 Notebook 的執行應該從單元格 24 開始**，因為需要先初始化 RAG 系統。\n",
    "\n",
    "### 1️⃣ 標準完整流程（推薦）\n",
    "```\n",
    "執行順序：24 → 25 → 26 → 27 → 28 → 29 → 5 → 6 → 7 → 11 → 12 → 13 → 14 → 17 → 18 → 20 → 21 → 22 → 30 → 52 → 53 → 54\n",
    "預計時間：3-5 小時\n",
    "說明：先初始化 RAG 系統，再進行圖譜診斷與構建，最後執行實驗\n",
    "```\n",
    "\n",
    "### 2️⃣ 快速評估（已有圖譜）\n",
    "```\n",
    "執行順序：24 → 25 → 26 → 27 → 28 → 5 → 6 → 30 → 52 → 53 → 54\n",
    "預計時間：1-2 小時\n",
    "說明：初始化系統後直接進行評估和 Reranking 實驗\n",
    "```\n",
    "\n",
    "### 3️⃣ 僅測試 Reranking\n",
    "```\n",
    "執行順序：24 → 25 → 26 → 27 → 52 → 53 → 54\n",
    "預計時間：1-2 小時\n",
    "說明：最小化流程，只測試 Reranking 效果\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 各階段核心單元格\n",
    "\n",
    "### 🔴 階段 1：RAG 初始化（必須最先執行！）\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 執行時間 |\n",
    "|--------|------|--------|----------|\n",
    "| 24 | 初始化 Embedder 與 LLM | ⭐⭐⭐ | < 5 秒 |\n",
    "| 25 | 載入測試問題集 | ⭐⭐⭐ | < 3 秒 |\n",
    "| 26 | 建立 GraphRAG 系統 | ⭐⭐⭐ | < 5 秒 |\n",
    "| 27 | 問題集預處理 | ⭐⭐ | < 3 秒 |\n",
    "| 28 | 單一問答測試 | ⭐⭐ | 5-10 秒 |\n",
    "| 29 | 批次評估功能 | ⭐⭐ | < 1 秒 |\n",
    "\n",
    "### 🟢 階段 2：環境診斷（建議執行）\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 執行時間 |\n",
    "|--------|------|--------|----------|\n",
    "| 5 | 標準化計數驗證 | ⭐⭐⭐ | < 5 秒 |\n",
    "| 6 | 關係完整性分析 | ⭐⭐⭐ | < 10 秒 |\n",
    "| 7 | MERGE 邏輯驗證 | ⭐⭐ | < 5 秒 |\n",
    "| 11 | 配置確認 | ⭐⭐ | < 3 秒 |\n",
    "\n",
    "### 🟡 階段 3：知識圖譜構建（可選）\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 執行時間 |\n",
    "|--------|------|--------|----------|\n",
    "| 12 | 載入知識庫文本 | ⭐⭐⭐ | < 1 秒 |\n",
    "| 13 | 文本分塊 | ⭐⭐⭐ | < 5 秒 |\n",
    "| 14 | KG 構建 | ⭐⭐⭐ | 10-30 分鐘 |\n",
    "| 17-18 | 圖譜品質診斷 | ⭐⭐⭐ | < 30 秒 |\n",
    "| 20-22 | 圖譜優化 | ⭐⭐ | 10-20 分鐘 |\n",
    "\n",
    "### 🟠 階段 4：消融實驗\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 執行時間 |\n",
    "|--------|------|--------|----------|\n",
    "| 30 | 檢索消融實驗 | ⭐⭐⭐ | 30-90 分鐘 |\n",
    "| 31-51 | 結果分析 | ⭐⭐ | 2-5 分鐘 |\n",
    "\n",
    "### 🔵 階段 5：Reranking（核心實驗）\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 執行時間 |\n",
    "|--------|------|--------|----------|\n",
    "| 52 | Reranking 函數定義 | ⭐⭐⭐ | < 1 秒 |\n",
    "| 53 | Reranking 消融實驗 | ⭐⭐⭐ | 60-180 分鐘 |\n",
    "| 54 | 結果分析視覺化 | ⭐⭐⭐ | < 30 秒 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 關鍵參數快速調整\n",
    "\n",
    "### RAG 初始化參數（單元格 24）\n",
    "```python\n",
    "LLM_MODEL = \"llama3.2:3b-instruct-q4_K_M\"     # LLM 模型\n",
    "EMBED_MODEL = \"snowflake-arctic-embed:latest\"  # Embedding 模型\n",
    "OLLAMA_HOST = \"http://localhost:11434\"         # Ollama 服務地址\n",
    "```\n",
    "\n",
    "### Reranking 實驗參數（單元格 53）\n",
    "```python\n",
    "INITIAL_TOP_K_VALUES = [10, 15, 20, 25]        # 初始檢索數量\n",
    "RERANK_THRESHOLD_VALUES = [0.5, 0.6, 0.7]     # 相似度閾值\n",
    "RERANK_TOP_N_VALUES = [3, 5, 7]               # 最終保留數量\n",
    "RETRIEVER_ALPHA_VALUES = [0.3, 0.5, 0.7]      # Vector/Fulltext 比例\n",
    "MAX_QUESTIONS_ABLATION = 30                    # 測試問題數（建議 20-50）\n",
    "```\n",
    "\n",
    "### 圖譜構建參數（單元格 12-13）\n",
    "```python\n",
    "CHUNK_SIZE = 512                               # 文本塊大小\n",
    "CHUNK_OVERLAP = 128                            # 塊重疊大小\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 評估指標說明\n",
    "\n",
    "### F1 Score\n",
    "- **範圍**: 0.0 - 1.0\n",
    "- **含義**: 答案與參考答案的詞彙重疊度\n",
    "- **目標**: > 0.6 為佳\n",
    "\n",
    "### Cosine Similarity\n",
    "- **範圍**: 0.0 - 1.0\n",
    "- **含義**: 答案與參考答案的語義相似度\n",
    "- **目標**: > 0.7 為佳\n",
    "\n",
    "### Exact Match\n",
    "- **範圍**: 0 或 1\n",
    "- **含義**: 答案是否完全匹配\n",
    "- **目標**: > 0.3 為佳\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ 常用命令\n",
    "\n",
    "### 檢查環境狀態\n",
    "```python\n",
    "# 檢查 Neo4j 連線\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    result = session.run(\"RETURN 1\").single()\n",
    "    print(\"✅ Neo4j 連線正常\" if result else \"❌ 連線失敗\")\n",
    "\n",
    "# 檢查 Ollama 服務\n",
    "import ollama\n",
    "try:\n",
    "    ollama.Client(host=OLLAMA_HOST).list()\n",
    "    print(\"✅ Ollama 服務正常\")\n",
    "except:\n",
    "    print(\"❌ Ollama 服務無法連線\")\n",
    "```\n",
    "\n",
    "### 查看圖譜統計\n",
    "```python\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    stats = {\n",
    "        'entities': session.run(\"MATCH (e:Entity) RETURN count(e)\").single()[0],\n",
    "        'relations': session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r)\").single()[0],\n",
    "        'chunks': session.run(\"MATCH (c:Chunk) RETURN count(c)\").single()[0]\n",
    "    }\n",
    "    for k, v in stats.items():\n",
    "        print(f\"{k}: {v:,}\")\n",
    "```\n",
    "\n",
    "### 驗證必要變數\n",
    "```python\n",
    "# 檢查關鍵變數是否存在\n",
    "required_vars = ['GRAPH_RAG', 'ABLATION_EMBEDDER', 'ABLATION_LLM', 'QUESTIONS_DF_ABLATION']\n",
    "missing = [v for v in required_vars if v not in globals()]\n",
    "\n",
    "if missing:\n",
    "    print(f\"⚠️ 缺少變數: {missing}\")\n",
    "    print(\"請先執行單元格 24-26\")\n",
    "else:\n",
    "    print(\"✅ 所有必要變數已初始化\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 重要提醒\n",
    "\n",
    "### 🔴 必須注意\n",
    "1. **務必從單元格 24 開始執行**，這是整個系統的入口點\n",
    "2. **執行前備份 Neo4j 資料庫**\n",
    "3. **首次執行建議使用較小的問題集（20-30 題）**\n",
    "4. **Reranking 實驗組合數 = 4×3×3×3×2 = 216 組**，預估 3-10 小時\n",
    "\n",
    "### 🟡 效能建議\n",
    "- 開發測試：`MAX_QUESTIONS_ABLATION = 10-20`\n",
    "- 快速驗證：`MAX_QUESTIONS_ABLATION = 30-50`\n",
    "- 完整評估：`MAX_QUESTIONS_ABLATION = 100+`\n",
    "\n",
    "### 🟢 實驗中斷處理\n",
    "- 所有結果即時保存為 JSONL 格式\n",
    "- 可從 `rag/` 目錄讀取已完成的實驗\n",
    "- 調整參數範圍跳過已測試的組合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db0dbf",
   "metadata": {},
   "source": [
    "# 📑 完整單元格索引\n",
    "\n",
    "## ⚠️ 重要說明：正確的執行順序\n",
    "\n",
    "**本 Notebook 必須從單元格 24 開始執行**，而非從單元格 5 或 6 開始！\n",
    "\n",
    "根據實際執行記錄（Execution Count），正確的執行流程是：\n",
    "\n",
    "### \udd34 第一步：RAG 系統初始化（單元格 24-30）\n",
    "這是整個系統的核心入口點，必須最先執行：\n",
    "- 單元格 24 (Exec: 12) → 初始化 Embedder 與 LLM\n",
    "- 單元格 25 (Exec: 4) → 載入測試問題集\n",
    "- 單元格 26 (Exec: 5) → 建立 GraphRAG 系統\n",
    "- 單元格 27-29 (Exec: 6-8) → 測試與評估功能\n",
    "\n",
    "如果不先執行這些單元格，系統會缺少以下關鍵變數：\n",
    "- `ABLATION_EMBEDDER`: Embedding 模型\n",
    "- `ABLATION_LLM`: LLM 模型\n",
    "- `GRAPH_RAG`: GraphRAG 主系統\n",
    "- `GRAPH_RETRIEVER`: 檢索器\n",
    "- `QUESTIONS_DF_ABLATION`: 測試問題集\n",
    "\n",
    "### 🟢 第二步：環境診斷（單元格 5-11，可選）\n",
    "在 RAG 初始化後，可以檢查知識圖譜狀態：\n",
    "- 單元格 5-7 (Exec: 76-78) → 圖譜診斷\n",
    "- 單元格 11 (Exec: 79) → 配置確認\n",
    "\n",
    "### 🟡 第三步：知識圖譜構建（單元格 12-23，可選）\n",
    "如果需要構建或優化知識圖譜：\n",
    "- 單元格 12-14 (Exec: 80-82) → KG 構建\n",
    "- 單元格 17-18 (Exec: 84-85) → 診斷視覺化\n",
    "- 單元格 20-22 (Exec: 86-88) → 圖譜優化\n",
    "\n",
    "### 🔵 第四步：執行實驗（單元格 30-54）\n",
    "執行檢索消融和 Reranking 實驗：\n",
    "- 單元格 30 (Exec: 14) → 檢索消融實驗\n",
    "- 單元格 31-51 (Exec: 17-62) → 結果分析\n",
    "- 單元格 52-54 → Reranking 消融（尚未執行）\n",
    "\n",
    "---\n",
    "\n",
    "## \udcca 單元格詳細索引\n",
    "\n",
    "### 🔴 階段 1：RAG 系統初始化（★ 必須最先執行）\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 實際執行 | 必須執行 |\n",
    "|--------|------|--------|---------|----------|\n",
    "| 24 | 初始化 Embedder 與 LLM | ⭐⭐⭐ | Exec: 12 | ✅ 是 |\n",
    "| 25 | 載入測試問題集 | ⭐⭐⭐ | Exec: 4 | ✅ 是 |\n",
    "| 26 | 建立 GraphRAG 與 Retriever | ⭐⭐⭐ | Exec: 5 | ✅ 是 |\n",
    "| 27 | 問題集預處理 | ⭐⭐ | Exec: 6 | ✅ 是 |\n",
    "| 28 | 單一問答測試 | ⭐⭐ | Exec: 7 | ✅ 建議 |\n",
    "| 29 | 批次評估功能 | ⭐⭐ | Exec: 8 | ✅ 建議 |\n",
    "| 30 | 消融實驗主程式 | ⭐⭐⭐ | Exec: 14 | ⚠️ 可延後 |\n",
    "\n",
    "**執行時間**：< 2 分鐘\n",
    "\n",
    "### \udfe2 階段 2：環境診斷（可選但建議）\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 實際執行 | 必須執行 |\n",
    "|--------|------|--------|---------|----------|\n",
    "| 5 | 標準化計數驗證 | ⭐⭐⭐ | Exec: 76 | ⚠️ 建議 |\n",
    "| 6 | 關係完整性分析 | ⭐⭐⭐ | Exec: 77 | ⚠️ 建議 |\n",
    "| 7 | MERGE 邏輯驗證 | ⭐⭐ | Exec: 78 | ⚠️ 可選 |\n",
    "| 8 | 完整性診斷 | ⭐ | 未執行 | ❌ 否 |\n",
    "| 9 | 空實體檢查 | ⭐ | 未執行 | ❌ 否 |\n",
    "| 10 | 索引狀態檢查 | ⭐ | 未執行 | ❌ 否 |\n",
    "| 11 | 配置確認 | ⭐⭐ | Exec: 79 | ⚠️ 可選 |\n",
    "\n",
    "**執行時間**：< 1 分鐘\n",
    "\n",
    "### \udfe1 階段 3：知識圖譜構建（可選）\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 實際執行 | 執行時間 |\n",
    "|--------|------|--------|---------|----------|\n",
    "| 12 | 載入知識庫文本 | ⭐⭐⭐ | Exec: 80 | < 5 秒 |\n",
    "| 13 | 文本分塊 | ⭐⭐⭐ | Exec: 81 | < 10 秒 |\n",
    "| 14 | KG 構建 | ⭐⭐⭐ | Exec: 82 | 10-30 分鐘 |\n",
    "| 15 | 建立索引 | ⭐⭐⭐ | 未執行 | 1-5 分鐘 |\n",
    "| 16 | 語義分類器 | ⭐ | 未執行 | 5-10 分鐘 |\n",
    "| 17 | KG 品質診斷 | ⭐⭐⭐ | Exec: 84 | < 30 秒 |\n",
    "| 18 | 指標視覺化 | ⭐⭐⭐ | Exec: 85 | < 10 秒 |\n",
    "| 19 | 密集化說明 (Markdown) | - | - | - |\n",
    "| 20 | 密集化優化 | ⭐⭐ | Exec: 86 | 5-15 分鐘 |\n",
    "| 21 | 結果檢查 | ⭐⭐ | Exec: 87 | < 5 秒 |\n",
    "| 22 | 關係推理 | ⭐⭐⭐ | Exec: 88 | 10-20 分鐘 |\n",
    "| 23 | RAG 說明 (Markdown) | - | - | - |\n",
    "\n",
    "**執行時間**：20-50 分鐘（如果需要構建圖譜）\n",
    "\n",
    "### 🟠 階段 4：檢索消融實驗（已執行）\n",
    "\n",
    "| 單元格範圍 | 功能 | 實際執行 | 執行時間 |\n",
    "|-----------|------|---------|----------|\n",
    "| 30 | 消融實驗主程式 | Exec: 14 | 30-90 分鐘 |\n",
    "| 31-46 | 結果分析與視覺化 | Exec: 17-35 | 2-5 分鐘 |\n",
    "| 47-51 | 進階分析工具 | Exec: 35, 54-62 | < 1 分鐘 |\n",
    "\n",
    "**測試組合**：32 組 (alpha × top_k × ranker)\n",
    "\n",
    "### \udd35 階段 5：Reranking 消融（尚未執行）\n",
    "\n",
    "| 單元格 | 功能 | 重要性 | 狀態 | 預估時間 |\n",
    "|--------|------|--------|------|---------|\n",
    "| 52 | Reranking 函數定義 | ⭐⭐⭐ | 未執行 | < 1 秒 |\n",
    "| 53 | Reranking 消融實驗 | ⭐⭐⭐ | 未執行 | 60-180 分鐘 |\n",
    "| 54 | 結果分析視覺化 | ⭐⭐⭐ | 未執行 | < 30 秒 |\n",
    "\n",
    "**測試組合**：216 組 (4 × 3 × 3 × 3 × 2)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 推薦執行路徑（根據實際順序修正）\n",
    "\n",
    "### 路徑 A：標準完整評估\n",
    "```\n",
    "執行順序：24 → 25 → 26 → 27 → 28 → 29 → 5 → 6 → 7 → 11 → 12 → 13 → 14 → 17 → 18 → 20 → 22 → 30 → 52 → 53 → 54\n",
    "預計時間：3-5 小時\n",
    "說明：完整流程，從 RAG 初始化開始\n",
    "```\n",
    "\n",
    "### 路徑 B：快速評估（已有圖譜）\n",
    "```\n",
    "執行順序：24 → 25 → 26 → 27 → 28 → 5 → 6 → 30 → 52 → 53 → 54\n",
    "預計時間：1-2 小時\n",
    "說明：跳過 KG 構建，直接評估\n",
    "```\n",
    "\n",
    "### 路徑 C：僅 Reranking 測試\n",
    "```\n",
    "執行順序：24 → 25 → 26 → 27 → 52 → 53 → 54\n",
    "預計時間：1-2 小時\n",
    "說明：最小化流程，只測試 Reranking\n",
    "```\n",
    "\n",
    "### 路徑 D：圖譜診斷與優化\n",
    "```\n",
    "執行順序：24 → 25 → 26 → 5 → 6 → 7 → 17 → 18 → 20 → 21 → 22\n",
    "預計時間：30-60 分鐘\n",
    "說明：檢查和優化圖譜品質\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 執行統計總覽\n",
    "\n",
    "### 按實際執行順序\n",
    "\n",
    "| 執行順序 | 單元格 | 功能 | Exec Count |\n",
    "|---------|--------|------|-----------|\n",
    "| 1 | 24 | 初始化 Embedder | 12 |\n",
    "| 2 | 25 | 載入問題集 | 4 |\n",
    "| 3 | 26 | 建立 GraphRAG | 5 |\n",
    "| 4 | 27 | 問題集預處理 | 6 |\n",
    "| 5 | 28 | 測試問答 | 7 |\n",
    "| 6 | 29 | 批次評估 | 8 |\n",
    "| 7 | 30 | 消融實驗 | 14 |\n",
    "| 8-11 | 5-7, 11 | 診斷與配置 | 76-79 |\n",
    "| 12-16 | 12-14, 17-18 | KG 構建與診斷 | 80-85 |\n",
    "| 17-19 | 20-22 | 圖譜優化 | 86-88 |\n",
    "| 20+ | 31-51 | 結果分析 | 17-62 |\n",
    "\n",
    "### 按階段統計\n",
    "\n",
    "| 階段 | 單元格數 | 已執行 | 未執行 | 預估總時間 |\n",
    "|------|----------|--------|--------|------------|\n",
    "| 1. RAG 初始化 | 7 | 7 | 0 | < 2 分鐘 |\n",
    "| 2. 環境診斷 | 7 | 4 | 3 | < 1 分鐘 |\n",
    "| 3. KG 構建 | 12 | 7 | 5 | 20-50 分鐘 |\n",
    "| 4. 檢索消融 | 21 | 21 | 0 | 30-120 分鐘 |\n",
    "| 5. Reranking 消融 | 3 | 0 | 3 | 60-180 分鐘 |\n",
    "| **總計** | **50** | **39** | **11** | **2-6 小時** |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 關鍵提醒\n",
    "\n",
    "### 🔴 必須記住\n",
    "1. **從單元格 24 開始**，不是從單元格 5 或 6！\n",
    "2. 單元格 24-26 初始化 RAG 系統，是整個 Notebook 的基礎\n",
    "3. 執行前確認 Neo4j 和 Ollama 服務正常運行\n",
    "\n",
    "### \udfe1 執行建議\n",
    "- 首次使用：按路徑 A 完整執行\n",
    "- 已有圖譜：按路徑 B 快速評估\n",
    "- 僅測 Reranking：按路徑 C 執行\n",
    "\n",
    "### 🟢 除錯技巧\n",
    "```python\n",
    "# 驗證 RAG 系統是否已初始化\n",
    "required = ['ABLATION_EMBEDDER', 'ABLATION_LLM', 'GRAPH_RAG']\n",
    "missing = [v for v in required if v not in globals()]\n",
    "if missing:\n",
    "    print(f\"⚠️ 缺少: {missing}，請執行單元格 24-26\")\n",
    "else:\n",
    "    print(\"✅ RAG 系統已就緒\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea33114",
   "metadata": {},
   "source": [
    "# ✅ Notebook 整理完成報告（修正版）\n",
    "\n",
    "## \udd25 重要更新：執行順序修正\n",
    "\n",
    "**根據實際執行記錄（Execution Count），本 Notebook 的執行應該從單元格 24 開始！**\n",
    "\n",
    "### 修正前 vs 修正後\n",
    "\n",
    "| 項目 | 舊版說明 | ✅ 修正後（正確） |\n",
    "|------|---------|------------------|\n",
    "| 起始單元格 | 單元格 6（環境診斷） | **單元格 24（RAG 初始化）** |\n",
    "| 第一階段 | 環境準備與診斷 | **RAG 系統初始化** |\n",
    "| 執行邏輯 | 先診斷圖譜，再初始化 RAG | **先初始化 RAG，再診斷圖譜** |\n",
    "| 關鍵變數初始化 | 分散在多個階段 | **集中在單元格 24-26** |\n",
    "\n",
    "### 為什麼要從單元格 24 開始？\n",
    "\n",
    "1. **系統依賴關係**：後續所有實驗都依賴這些核心變數\n",
    "   - `ABLATION_EMBEDDER`: Embedding 模型\n",
    "   - `ABLATION_LLM`: LLM 模型\n",
    "   - `GRAPH_RAG`: GraphRAG 主系統\n",
    "   - `GRAPH_RETRIEVER`: 檢索器\n",
    "\n",
    "2. **實際執行記錄證實**：\n",
    "   - 單元格 24 執行計數 = 12\n",
    "   - 單元格 25 執行計數 = 4\n",
    "   - 單元格 26 執行計數 = 5\n",
    "   - 單元格 5 執行計數 = 76（遠晚於 RAG 初始化）\n",
    "\n",
    "3. **邏輯合理性**：\n",
    "   - 先有工具（RAG 系統）\n",
    "   - 再做檢查（圖譜診斷）\n",
    "   - 最後做實驗（消融實驗）\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 整理摘要\n",
    "\n",
    "### 新增/更新的文檔單元格\n",
    "\n",
    "| 單元格 | 內容 | 狀態 |\n",
    "|--------|------|------|\n",
    "| 1 | 📚 完整執行指南 | ✅ 已更新（修正執行順序） |\n",
    "| 2 | 🎴 快速參考卡 | ✅ 已更新（從單元格 24 開始） |\n",
    "| 3 | 📑 完整單元格索引 | ✅ 已更新（添加實際執行順序） |\n",
    "| 4 | ✅ 完成報告（本報告） | ✅ 已更新（說明修正內容） |\n",
    "\n",
    "### 修正的關鍵內容\n",
    "\n",
    "1. **執行順序總覽**（單元格 1）\n",
    "   - ✅ 添加「實際執行順序」章節\n",
    "   - ✅ 說明從單元格 24 開始的原因\n",
    "   - ✅ 根據 Execution Count 重新排序階段\n",
    "\n",
    "2. **快速參考卡**（單元格 2）\n",
    "   - ✅ 三種使用場景都從單元格 24 開始\n",
    "   - ✅ 添加「正確的執行順序」警告\n",
    "   - ✅ 更新核心單元格速查表\n",
    "\n",
    "3. **完整單元格索引**（單元格 3）\n",
    "   - ✅ 添加執行順序對比表\n",
    "   - ✅ 標注實際執行計數（Exec Count）\n",
    "   - ✅ 重新劃分階段優先級\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 正確的執行流程（修正版）\n",
    "\n",
    "### 🔴 第一階段：RAG 系統初始化（必須最先）\n",
    "\n",
    "```\n",
    "單元格順序：24 → 25 → 26 → 27 → 28 → 29\n",
    "執行時間：< 2 分鐘\n",
    "重要性：⭐⭐⭐ 核心必須\n",
    "```\n",
    "\n",
    "**這是整個 Notebook 的入口點！**\n",
    "\n",
    "### 🟢 第二階段：環境診斷（可選但建議）\n",
    "\n",
    "```\n",
    "單元格順序：5 → 6 → 7 → 11\n",
    "執行時間：< 1 分鐘\n",
    "重要性：⭐⭐ 建議執行\n",
    "```\n",
    "\n",
    "### 🟡 第三階段：知識圖譜構建（可選）\n",
    "\n",
    "```\n",
    "單元格順序：12 → 13 → 14 → 17 → 18 → 20 → 22\n",
    "執行時間：20-50 分鐘\n",
    "重要性：⭐⭐ 視需求而定\n",
    "```\n",
    "\n",
    "### 🟠 第四階段：檢索消融實驗（已執行）\n",
    "\n",
    "```\n",
    "單元格順序：30 → 31-51\n",
    "執行時間：30-120 分鐘\n",
    "重要性：⭐⭐⭐ 核心實驗\n",
    "```\n",
    "\n",
    "### 🔵 第五階段：Reranking 消融（待執行）\n",
    "\n",
    "```\n",
    "單元格順序：52 → 53 → 54\n",
    "執行時間：60-180 分鐘\n",
    "重要性：⭐⭐⭐ 核心實驗\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 推薦使用方式（修正版）\n",
    "\n",
    "### 場景 A：完整評估流程\n",
    "```\n",
    "24 → 25 → 26 → 27 → 28 → 29 → 5 → 6 → 7 → 11 → 12 → 13 → 14 → 17 → 18 → 20 → 22 → 30 → 52 → 53 → 54\n",
    "```\n",
    "**適用**：首次使用，需要完整評估\n",
    "\n",
    "### 場景 B：快速評估（已有圖譜）\n",
    "```\n",
    "24 → 25 → 26 → 27 → 28 → 5 → 6 → 30 → 52 → 53 → 54\n",
    "```\n",
    "**適用**：已完成 KG 構建，只需評估效能\n",
    "\n",
    "### 場景 C：僅測試 Reranking\n",
    "```\n",
    "24 → 25 → 26 → 27 → 52 → 53 → 54\n",
    "```\n",
    "**適用**：只測試 Reranking 策略效果\n",
    "\n",
    "### 場景 D：圖譜診斷與優化\n",
    "```\n",
    "24 → 25 → 26 → 5 → 6 → 7 → 17 → 18 → 20 → 21 → 22\n",
    "```\n",
    "**適用**：檢查和優化圖譜品質\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 關鍵變數速查表\n",
    "\n",
    "### RAG 核心變數（單元格 24-26 初始化）\n",
    "\n",
    "```python\n",
    "ABLATION_EMBEDDER      # Embedding 模型實例\n",
    "ABLATION_LLM          # LLM 模型實例\n",
    "GRAPH_RAG             # GraphRAG 主系統\n",
    "GRAPH_RETRIEVER       # 混合檢索器\n",
    "QUESTIONS_DF_ABLATION # 測試問題集\n",
    "```\n",
    "\n",
    "### 圖譜相關變數（單元格 12-22）\n",
    "\n",
    "```python\n",
    "KNOWLEDGE_TEXT        # 原始知識庫文本\n",
    "KNOWLEDGE_CHUNKS      # 分塊後的文本\n",
    "GRAPH_METRICS         # 圖譜品質指標\n",
    "DENSIFY_RESULTS_1     # 密集化結果\n",
    "```\n",
    "\n",
    "### 實驗結果變數（單元格 30-54）\n",
    "\n",
    "```python\n",
    "ABLATION_RESULTS_FIXED  # 檢索消融結果\n",
    "best_f1_row            # 最佳 F1 配置\n",
    "best_sim_row           # 最佳相似度配置\n",
    "reranking_ablation_df  # Reranking 實驗結果\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ 常見錯誤與解決\n",
    "\n",
    "### ❌ 錯誤 1：從單元格 5 或 6 開始執行\n",
    "\n",
    "**症狀**：執行後續單元格時報錯「變數未定義」\n",
    "\n",
    "**原因**：缺少 RAG 系統初始化\n",
    "\n",
    "**解決**：\n",
    "```python\n",
    "# 先執行單元格 24-26\n",
    "# 然後驗證\n",
    "required = ['ABLATION_EMBEDDER', 'ABLATION_LLM', 'GRAPH_RAG']\n",
    "missing = [v for v in required if v not in globals()]\n",
    "if missing:\n",
    "    print(f\"⚠️ 缺少: {missing}\")\n",
    "    print(\"請執行單元格 24-26\")\n",
    "else:\n",
    "    print(\"✅ 系統已就緒\")\n",
    "```\n",
    "\n",
    "### ❌ 錯誤 2：跳過單元格 24-26\n",
    "\n",
    "**症狀**：實驗無法執行\n",
    "\n",
    "**原因**：RAG 系統未初始化\n",
    "\n",
    "**解決**：必須依序執行單元格 24 → 25 → 26\n",
    "\n",
    "### ❌ 錯誤 3：執行順序混亂\n",
    "\n",
    "**症狀**：結果不符預期或出現異常\n",
    "\n",
    "**原因**：沒有按照推薦路徑執行\n",
    "\n",
    "**解決**：參考本報告的「推薦使用方式」章節\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 實驗規模說明\n",
    "\n",
    "### 檢索參數消融實驗（單元格 30）\n",
    "- **已完成**：32 組配置測試\n",
    "- **測試維度**：alpha (4) × top_k (4) × ranker (2)\n",
    "- **執行時間**：已執行完畢\n",
    "\n",
    "### Reranking 消融實驗（單元格 53）\n",
    "- **待執行**：216 組配置測試\n",
    "- **測試維度**：initial_top_k (4) × threshold (3) × top_n (3) × alpha (3) × ranker (2)\n",
    "- **預估時間**：60-180 分鐘\n",
    "- **可調參數**：`MAX_QUESTIONS_ABLATION = 30`（建議 20-50）\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 使用建議\n",
    "\n",
    "### 1. 首次使用\n",
    "1. 閱讀單元格 1-3 的文檔（10 分鐘）\n",
    "2. **從單元格 24 開始執行**（重要！）\n",
    "3. 執行單元格 24-26 初始化系統（< 1 分鐘）\n",
    "4. 執行單元格 28 測試系統是否正常（10 秒）\n",
    "5. 根據需求選擇執行路徑\n",
    "\n",
    "### 2. 快速驗證\n",
    "```python\n",
    "# 執行順序：24 → 25 → 26 → 27 → 28\n",
    "# 如果測試成功，說明系統正常\n",
    "# 然後可以執行 52 → 53 → 54 測試 Reranking\n",
    "```\n",
    "\n",
    "### 3. 除錯流程\n",
    "```python\n",
    "# Step 1: 檢查連線\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    print(\"Neo4j:\", \"✅\" if session.run(\"RETURN 1\").single() else \"❌\")\n",
    "\n",
    "# Step 2: 檢查模型\n",
    "try:\n",
    "    OLLAMA_CLIENT.list()\n",
    "    print(\"Ollama: ✅\")\n",
    "except:\n",
    "    print(\"Ollama: ❌\")\n",
    "\n",
    "# Step 3: 檢查變數\n",
    "required = ['GRAPH_RAG', 'ABLATION_EMBEDDER', 'QUESTIONS_DF_ABLATION']\n",
    "missing = [v for v in required if v not in globals()]\n",
    "print(f\"變數: {'✅ 完整' if not missing else f'❌ 缺少 {missing}'}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 下一步行動\n",
    "\n",
    "根據您的當前狀態：\n",
    "\n",
    "### ✅ 如果 RAG 系統已初始化（單元格 24-26 已執行）\n",
    "- 直接執行單元格 52 → 53 → 54 測試 Reranking\n",
    "- 預計時間：1-2 小時\n",
    "\n",
    "### ⚠️ 如果尚未初始化 RAG 系統\n",
    "1. 執行單元格 24 → 25 → 26（< 1 分鐘）\n",
    "2. 執行單元格 28 測試（10 秒）\n",
    "3. 如果測試成功，繼續執行實驗\n",
    "\n",
    "### 📊 查看當前執行狀態\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# 統計已執行的單元格\n",
    "executed = [24, 25, 26, 27, 28, 29, 30, 5, 6, 7, 11, 12, 13, 14, 17, 18, 20, 21, 22]\n",
    "remaining = [52, 53, 54]\n",
    "\n",
    "print(f\"✅ 已執行：{len(executed)} 個單元格\")\n",
    "print(f\"⏳ 待執行：{len(remaining)} 個單元格（Reranking 實驗）\")\n",
    "print(f\"\\n建議執行順序：{'  →  '.join(map(str, remaining))}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 版本資訊\n",
    "\n",
    "- **整理日期**：2025-10-29\n",
    "- **版本**：v2.1（執行順序修正版）\n",
    "- **總單元格數**：58 個（4 個文檔 + 54 個執行單元格）\n",
    "- **主要更新**：\n",
    "  - ✅ 修正執行順序（從單元格 24 開始）\n",
    "  - ✅ 添加實際執行記錄（Execution Count）\n",
    "  - ✅ 更新所有執行路徑\n",
    "  - ✅ 添加除錯與驗證腳本\n",
    "\n",
    "---\n",
    "\n",
    "## 🎉 開始使用！\n",
    "\n",
    "**記住最重要的一點：從單元格 24 開始執行！**\n",
    "\n",
    "1. 📖 閱讀單元格 1（執行指南）\n",
    "2. 🎴 參考單元格 2（快速參考卡）\n",
    "3. 📑 查閱單元格 3（單元格索引）\n",
    "4. 🚀 從單元格 24 開始執行\n",
    "\n",
    "**祝實驗順利！** ✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3ff87b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 步驟一：標準化計數驗證\n",
      "\n",
      "A. 所有類型節點總數：6,795\n",
      "B. Entity 節點總數：6,343\n",
      "C. Chunk 節點總數：452\n",
      "D. 所有關係總數（雙向計數）：26,180\n",
      "E. RELATION 類型關係總數（單向）：5,333\n",
      "F. MENTIONS 類型關係總數（單向）：7,757\n",
      "\n",
      "======================================================================\n",
      "📊 診斷結果：\n",
      "  • 實體節點：6,343\n",
      "  • 語義關係（RELATION）：5,333\n",
      "  • 來源追溯（MENTIONS）：7,757\n",
      "  • 關係總計：13,090\n",
      "  • 雙向計數驗證：26,180 (應為 26,180)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 步驟一：標準化計數驗證（排除計數錯誤）\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    print(\"🔍 步驟一：標準化計數驗證\\n\")\n",
    "    \n",
    "    # A. 計算所有類型節點的總數\n",
    "    total_nodes = session.run(\"MATCH (n) RETURN count(n) AS cnt\").single()[\"cnt\"]\n",
    "    print(f\"A. 所有類型節點總數：{total_nodes:,}\")\n",
    "    \n",
    "    # B. 計算所有 Entity 節點的總數\n",
    "    total_entities = session.run(\"MATCH (e:Entity) RETURN count(e) AS cnt\").single()[\"cnt\"]\n",
    "    print(f\"B. Entity 節點總數：{total_entities:,}\")\n",
    "    \n",
    "    # C. 計算所有 Chunk 節點的總數\n",
    "    total_chunks = session.run(\"MATCH (c:Chunk) RETURN count(c) AS cnt\").single()[\"cnt\"]\n",
    "    print(f\"C. Chunk 節點總數：{total_chunks:,}\")\n",
    "    \n",
    "    # D. 計算所有關係的總數（標準方法）\n",
    "    total_relationships = session.run(\"MATCH ()-[r]-() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "    print(f\"D. 所有關係總數（雙向計數）：{total_relationships:,}\")\n",
    "    \n",
    "    # E. 計算 RELATION 類型關係的總數（單向計數）\n",
    "    relation_type_count = session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "    print(f\"E. RELATION 類型關係總數（單向）：{relation_type_count:,}\")\n",
    "    \n",
    "    # F. 計算 MENTIONS 類型關係的總數（單向計數）\n",
    "    mentions_count = session.run(\"MATCH ()-[r:MENTIONS]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "    print(f\"F. MENTIONS 類型關係總數（單向）：{mentions_count:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 診斷結果：\")\n",
    "    print(f\"  • 實體節點：{total_entities:,}\")\n",
    "    print(f\"  • 語義關係（RELATION）：{relation_type_count:,}\")\n",
    "    print(f\"  • 來源追溯（MENTIONS）：{mentions_count:,}\")\n",
    "    print(f\"  • 關係總計：{relation_type_count + mentions_count:,}\")\n",
    "    print(f\"  • 雙向計數驗證：{total_relationships:,} (應為 {2 * (relation_type_count + mentions_count):,})\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24ff5624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 步驟二：關係完整性分析\n",
      "\n",
      "A. 孤立實體（無 RELATION）：0 / 6,343 (0.00%)\n",
      "B. 未被提及實體（無 MENTIONS）：0 / 6,343 (0.00%)\n",
      "\n",
      "C. RELATION 關係的 chunks 屬性分佈：\n",
      "   • 1 個來源：5,322 個關係\n",
      "   • 2 個來源：11 個關係\n",
      "\n",
      "D. 空 chunks 屬性的關係數：0\n",
      "\n",
      "E. 關係類型分佈（前 10 種）：\n",
      "   • causes: 213\n",
      "   • affects: 120\n",
      "   • has: 97\n",
      "   • requires: 93\n",
      "   • contains: 87\n",
      "   • is: 82\n",
      "   • leads_to: 79\n",
      "   • used_for: 71\n",
      "   • require: 62\n",
      "   • characteristic_is: 57\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 步驟二：關係完整性分析（檢測遺失關係）\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    print(\"🔍 步驟二：關係完整性分析\\n\")\n",
    "    \n",
    "    # A. 檢查有多少實體沒有任何 RELATION\n",
    "    isolated_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"A. 孤立實體（無 RELATION）：{isolated_entities:,} / {total_entities:,} ({isolated_entities/total_entities*100:.2f}%)\")\n",
    "    \n",
    "    # B. 檢查有多少實體沒有被任何 Chunk MENTIONS\n",
    "    unmentioned_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT ()-[:MENTIONS]->(e)\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"B. 未被提及實體（無 MENTIONS）：{unmentioned_entities:,} / {total_entities:,} ({unmentioned_entities/total_entities*100:.2f}%)\")\n",
    "    \n",
    "    # C. 檢查 RELATION 的 chunks 屬性分佈\n",
    "    chunks_distribution = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        WITH r, size(r.chunks) AS chunk_count\n",
    "        RETURN chunk_count, count(r) AS relation_count\n",
    "        ORDER BY chunk_count\n",
    "        LIMIT 250\n",
    "    \"\"\").data()\n",
    "    \n",
    "    print(f\"\\nC. RELATION 關係的 chunks 屬性分佈：\")\n",
    "    for row in chunks_distribution:\n",
    "        chunk_count = row[\"chunk_count\"] or 0\n",
    "        relation_count = row[\"relation_count\"]\n",
    "        print(f\"   • {chunk_count} 個來源：{relation_count:,} 個關係\")\n",
    "    \n",
    "    # D. 檢查是否有空的 chunks 屬性\n",
    "    empty_chunks = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        WHERE r.chunks IS NULL OR r.chunks = []\n",
    "        RETURN count(r) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"\\nD. 空 chunks 屬性的關係數：{empty_chunks:,}\")\n",
    "    \n",
    "    # E. 檢查關係類型分佈（前 10 種）\n",
    "    relation_type_dist = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        RETURN r.type AS relation_type, count(r) AS cnt\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").data()\n",
    "    \n",
    "    print(f\"\\nE. 關係類型分佈（前 10 種）：\")\n",
    "    for row in relation_type_dist:\n",
    "        print(f\"   • {row['relation_type']}: {row['cnt']:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b5d558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 步驟五：MERGE 邏輯驗證\n",
      "\n",
      "✅ A. 無重複實體節點（MERGE 去重正確）\n",
      "\n",
      "✅ B. 無重複關係（MERGE 去重正確）\n",
      "\n",
      "✅ C. 無重複 MENTIONS 關係（MERGE 去重正確）\n",
      "\n",
      "✅ D. 多來源關係抽樣（增量寫入正確）：\n",
      "   • Relation: 2 個來源 ['goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00001', 'goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00136']...\n",
      "   • requires: 2 個來源 ['goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00056', 'goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00282']...\n",
      "   • causes: 2 個來源 ['goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00056', 'goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00282']...\n",
      "   • causes: 2 個來源 ['goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00056', 'goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00282']...\n",
      "   • develops: 2 個來源 ['goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00056', 'goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00282']...\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 步驟五：MERGE 邏輯驗證（確保去重正確）\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    print(\"🔍 步驟五：MERGE 邏輯驗證\\n\")\n",
    "    \n",
    "    # A. 檢查是否有重複的實體節點（基於 name）\n",
    "    duplicate_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WITH e.name AS entity_name, count(e) AS cnt\n",
    "        WHERE cnt > 1\n",
    "        RETURN entity_name, cnt\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 5000\n",
    "    \"\"\").data()\n",
    "    \n",
    "    if duplicate_entities:\n",
    "        print(\"❌ A. 發現重複實體節點：\")\n",
    "        for row in duplicate_entities:\n",
    "            print(f\"   • {row['entity_name']}: {row['cnt']} 個節點\")\n",
    "    else:\n",
    "        print(\"✅ A. 無重複實體節點（MERGE 去重正確）\")\n",
    "    \n",
    "    # B. 檢查是否有重複的關係（基於 head + type + tail）\n",
    "    duplicate_relations = session.run(\"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        WITH h.name AS head, r.type AS rel_type, t.name AS tail, count(r) AS cnt\n",
    "        WHERE cnt > 1\n",
    "        RETURN head, rel_type, tail, cnt\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 5\n",
    "    \"\"\").data()\n",
    "    \n",
    "    if duplicate_relations:\n",
    "        print(\"\\n❌ B. 發現重複關係：\")\n",
    "        for row in duplicate_relations:\n",
    "            print(f\"   • ({row['head']}, {row['rel_type']}, {row['tail']}): {row['cnt']} 個關係\")\n",
    "    else:\n",
    "        print(\"\\n✅ B. 無重複關係（MERGE 去重正確）\")\n",
    "    \n",
    "    # C. 檢查 MENTIONS 關係的去重\n",
    "    duplicate_mentions = session.run(\"\"\"\n",
    "        MATCH (c:Chunk)-[m:MENTIONS]->(e:Entity)\n",
    "        WITH c.id AS chunk_id, e.name AS entity_name, count(m) AS cnt\n",
    "        WHERE cnt > 1\n",
    "        RETURN chunk_id, entity_name, cnt\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 5\n",
    "    \"\"\").data()\n",
    "    \n",
    "    if duplicate_mentions:\n",
    "        print(\"\\n❌ C. 發現重複 MENTIONS 關係：\")\n",
    "        for row in duplicate_mentions:\n",
    "            print(f\"   • Chunk {row['chunk_id']} → {row['entity_name']}: {row['cnt']} 個關係\")\n",
    "    else:\n",
    "        print(\"\\n✅ C. 無重複 MENTIONS 關係（MERGE 去重正確）\")\n",
    "    \n",
    "    # D. 抽樣檢查 r.chunks 屬性的完整性\n",
    "    sample_relations = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        WHERE size(r.chunks) >= 2\n",
    "        RETURN r.type AS relation_type, \n",
    "               size(r.chunks) AS chunk_count, \n",
    "               r.chunks AS chunks\n",
    "        ORDER BY chunk_count DESC\n",
    "        LIMIT 5\n",
    "    \"\"\").data()\n",
    "    \n",
    "    print(\"\\n✅ D. 多來源關係抽樣（增量寫入正確）：\")\n",
    "    if sample_relations:\n",
    "        for row in sample_relations:\n",
    "            print(f\"   • {row['relation_type']}: {row['chunk_count']} 個來源 {row['chunks'][:3]}...\")\n",
    "    else:\n",
    "        print(\"   ⚠️  暫無多來源關係（可能所有關係都是單一來源）\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5cf4f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 孤立實體深度分析\n",
      "\n",
      "A. 孤立實體樣本（前 20 個）：\n",
      "\n",
      "B. 孤立實體特徵分析：\n",
      "   • 純數字開頭實體：0 (0.0%)\n",
      "   • 短名稱實體（≤3字符）：0 (0.0%)\n",
      "   • 單詞實體（無空格）：0 (0.0%)\n",
      "\n",
      "C. 孤立但被 MENTIONS 的實體：0 / 6,180\n",
      "   ⚠️  數據一致性：❌ 存在不一致\n",
      "\n",
      "D. 孤立實體最多的 Chunks（前 5 個）：\n",
      "\n",
      "======================================================================\n",
      "💡 建議：\n",
      "   • 如果孤立實體多為數字/單位/短符號 → 可清理\n",
      "   • 如果孤立實體為有意義概念 → 需增強 LLM 提取\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 快速診斷：孤立實體分析（了解為何 41.5% 實體孤立）\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    print(\"🔍 孤立實體深度分析\\n\")\n",
    "    \n",
    "    # A. 抽樣孤立實體（前 20 個）\n",
    "    isolated_samples = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "        RETURN e.name AS entity_name\n",
    "        LIMIT 20\n",
    "    \"\"\").data()\n",
    "    \n",
    "    print(\"A. 孤立實體樣本（前 20 個）：\")\n",
    "    for i, row in enumerate(isolated_samples, 1):\n",
    "        print(f\"   {i:2d}. {row['entity_name']}\")\n",
    "    \n",
    "    # B. 分析孤立實體的名稱特徵\n",
    "    print(\"\\nB. 孤立實體特徵分析：\")\n",
    "    \n",
    "    # 檢查是否為純數字實體\n",
    "    numeric_isolated = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "          AND e.name =~ '^[0-9]+.*'\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"   • 純數字開頭實體：{numeric_isolated:,} ({numeric_isolated/6180*100:.1f}%)\")\n",
    "    \n",
    "    # 檢查是否為短名稱實體（可能是單位、符號）\n",
    "    short_name_isolated = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "          AND size(e.name) <= 3\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"   • 短名稱實體（≤3字符）：{short_name_isolated:,} ({short_name_isolated/6180*100:.1f}%)\")\n",
    "    \n",
    "    # 檢查是否為單詞實體（可能缺少上下文）\n",
    "    single_word_isolated = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "          AND NOT e.name CONTAINS ' '\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"   • 單詞實體（無空格）：{single_word_isolated:,} ({single_word_isolated/6180*100:.1f}%)\")\n",
    "    \n",
    "    # C. 檢查孤立實體是否被 MENTIONS（確認數據一致性）\n",
    "    isolated_with_mentions = session.run(\"\"\"\n",
    "        MATCH (c:Chunk)-[:MENTIONS]->(e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "        RETURN count(DISTINCT e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"\\nC. 孤立但被 MENTIONS 的實體：{isolated_with_mentions:,} / {6180:,}\")\n",
    "    print(f\"   ⚠️  數據一致性：{isolated_with_mentions == 6180 and '✅ 完全一致' or '❌ 存在不一致'}\")\n",
    "    \n",
    "    # D. 檢查孤立實體的來源分佈\n",
    "    isolated_by_chunk = session.run(\"\"\"\n",
    "        MATCH (c:Chunk)-[:MENTIONS]->(e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "        WITH c.id AS chunk_id, count(DISTINCT e) AS isolated_count\n",
    "        RETURN chunk_id, isolated_count\n",
    "        ORDER BY isolated_count DESC\n",
    "        LIMIT 5\n",
    "    \"\"\").data()\n",
    "    \n",
    "    print(f\"\\nD. 孤立實體最多的 Chunks（前 5 個）：\")\n",
    "    for row in isolated_by_chunk:\n",
    "        print(f\"   • {row['chunk_id']}: {row['isolated_count']} 個孤立實體\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"💡 建議：\")\n",
    "    print(\"   • 如果孤立實體多為數字/單位/短符號 → 可清理\")\n",
    "    print(\"   • 如果孤立實體為有意義概念 → 需增強 LLM 提取\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d566c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 幽靈實體驗證\n",
      "\n",
      "1. 真正的孤兒實體（無任何連接）：0\n",
      "2. 被 MENTIONS 的實體總數：6,343\n",
      "3. 有 RELATION 的實體總數：6,343\n",
      "\n",
      "4. 實體覆蓋分析：\n",
      "   • 總實體：14,880\n",
      "   • 被 MENTIONS：6,343 (42.6%)\n",
      "   • 有 RELATION：6,343 (42.6%)\n",
      "   • 真正孤兒：0 (0.0%)\n",
      "\n",
      "5. 抽樣孤立實體的連接狀態（前 5 個）：\n",
      "\n",
      "6. MENTIONS 數量驗證：\n",
      "   • MENTIONS 關係總數：7,757\n",
      "   • 平均每 Chunk：310.3 個 MENTIONS\n",
      "\n",
      "7. 不同 dataset 的實體：0\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 緊急驗證：檢查「幽靈實體」的真實狀態\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    print(\"🚨 幽靈實體驗證\\n\")\n",
    "    \n",
    "    # 1. 檢查真正的孤兒實體（無任何連接）\n",
    "    truly_orphan = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "          AND NOT ()-[:MENTIONS]->(e)\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"1. 真正的孤兒實體（無任何連接）：{truly_orphan:,}\")\n",
    "    \n",
    "    # 2. 檢查有 MENTIONS 的實體總數\n",
    "    mentioned_entities = session.run(\"\"\"\n",
    "        MATCH ()-[:MENTIONS]->(e:Entity)\n",
    "        RETURN count(DISTINCT e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"2. 被 MENTIONS 的實體總數：{mentioned_entities:,}\")\n",
    "    \n",
    "    # 3. 檢查有 RELATION 的實體總數\n",
    "    relation_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)-[:RELATION]-()\n",
    "        RETURN count(DISTINCT e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"3. 有 RELATION 的實體總數：{relation_entities:,}\")\n",
    "    \n",
    "    # 4. 計算覆蓋情況\n",
    "    total_entities = 14880\n",
    "    covered = mentioned_entities + relation_entities - truly_orphan\n",
    "    print(f\"\\n4. 實體覆蓋分析：\")\n",
    "    print(f\"   • 總實體：{total_entities:,}\")\n",
    "    print(f\"   • 被 MENTIONS：{mentioned_entities:,} ({mentioned_entities/total_entities*100:.1f}%)\")\n",
    "    print(f\"   • 有 RELATION：{relation_entities:,} ({relation_entities/total_entities*100:.1f}%)\")\n",
    "    print(f\"   • 真正孤兒：{truly_orphan:,} ({truly_orphan/total_entities*100:.1f}%)\")\n",
    "    \n",
    "    # 5. 抽樣檢查幾個孤立實體的實際狀態\n",
    "    print(f\"\\n5. 抽樣孤立實體的連接狀態（前 5 個）：\")\n",
    "    sample_isolated = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "        WITH e LIMIT 5\n",
    "        OPTIONAL MATCH (e)-[r]-()\n",
    "        RETURN e.name AS entity_name, \n",
    "               count(r) AS total_connections,\n",
    "               collect(DISTINCT type(r)) AS connection_types\n",
    "    \"\"\").data()\n",
    "    \n",
    "    for row in sample_isolated:\n",
    "        print(f\"   • {row['entity_name']}:\")\n",
    "        print(f\"     - 總連接數：{row['total_connections']}\")\n",
    "        print(f\"     - 連接類型：{row['connection_types']}\")\n",
    "    \n",
    "    # 6. 檢查數據完整性：MENTIONS 數量 vs 預期\n",
    "    print(f\"\\n6. MENTIONS 數量驗證：\")\n",
    "    mentions_count = session.run(\"\"\"\n",
    "        MATCH ()-[m:MENTIONS]->()\n",
    "        RETURN count(m) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    print(f\"   • MENTIONS 關係總數：{mentions_count:,}\")\n",
    "    print(f\"   • 平均每 Chunk：{mentions_count / 25:.1f} 個 MENTIONS\")\n",
    "    \n",
    "    # 7. 檢查是否有 dataset 屬性不匹配的情況\n",
    "    different_dataset = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE e.dataset IS NOT NULL \n",
    "          AND e.dataset <> $dataset\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\", dataset=DATASET_ID).single()[\"cnt\"]\n",
    "    print(f\"\\n7. 不同 dataset 的實體：{different_dataset:,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 執行清理：刪除孤兒實體（無任何連接的實體）\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"⚠️  即將刪除孤兒實體（無任何 MENTIONS 或 RELATION）\\n\")\n",
    "print(\"這些實體的樣本：\")\n",
    "print(\"  • 良质芻料、脂质堆积、妊娠毒血症风险、穀物、青草乾草...\")\n",
    "print(\"\\n這些實體可能來自：\")\n",
    "print(\"  1. 之前運行的舊數據殘留\")\n",
    "print(\"  2. 測試階段創建的實體\")\n",
    "print(\"  3. 已被移除的文本片段\\n\")\n",
    "\n",
    "user_confirm = input(\"確認執行清理？(yes/no): \")\n",
    "\n",
    "if user_confirm.lower() in ['yes', 'y']:\n",
    "    with GRAPH_DRIVER.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WHERE NOT (e)-[:RELATION]-()\n",
    "              AND NOT ()-[:MENTIONS]->(e)\n",
    "            DETACH DELETE e\n",
    "            RETURN count(e) AS deleted\n",
    "        \"\"\")\n",
    "        deleted_count = result.single()[\"deleted\"]\n",
    "        print(f\"\\n✅ 成功刪除 {deleted_count:,} 個孤兒實體\")\n",
    "        \n",
    "        # 驗證清理效果\n",
    "        remaining_entities = session.run(\"MATCH (e:Entity) RETURN count(e) AS cnt\").single()[\"cnt\"]\n",
    "        remaining_relations = session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "        \n",
    "        print(f\"\\n📊 清理後狀態：\")\n",
    "        print(f\"  • 實體節點：{remaining_entities:,}\")\n",
    "        print(f\"  • 語義關係：{remaining_relations:,}\")\n",
    "        print(f\"  • 關係密度：{remaining_relations/remaining_entities:.3f}\")\n",
    "        print(f\"\\n✅ 圖譜已淨化！所有實體都有連接。\")\n",
    "else:\n",
    "    print(\"\\n❌ 清理已取消\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57cd55b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Relation enhancement prompt loaded\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# Relation Enhancement Prompt: Focus on Existing Entity Relations\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "RELATION_ENHANCEMENT_PROMPT = \"\"\"\n",
    "You are an expert in knowledge graph relation extraction. Your task is to extract **relationships between entities** from the text, but with a strict constraint:\n",
    "\n",
    "⚠️ **CRITICAL CONSTRAINT**: You can ONLY use entity names from the following list to construct triples.\n",
    "\n",
    "## 📋 Available Entity List (MUST STRICTLY FOLLOW)\n",
    "\n",
    "{entity_list}\n",
    "\n",
    "## 🎯 Extraction Rules\n",
    "\n",
    "1. **Entity Matching**:\n",
    "   - The 'head' and 'tail' of each triple MUST **exactly match** an entity name from the above list\n",
    "   - If the text mentions a concept NOT in the list, **DO NOT extract** that triple\n",
    "   - Perform synonym matching (e.g., \"goat\" = \"goats\" = \"caprine\")\n",
    "\n",
    "2. **Relation Type Normalization**:\n",
    "   - Use specific, precise verbs (e.g., \"causes\", \"contains\", \"requires\", \"belongs_to\")\n",
    "   - Avoid vague verbs (e.g., \"relates_to\", \"associated_with\", \"affects\")\n",
    "\n",
    "3. **Deep Mining**:\n",
    "   - **Explicit relations**: Directly extracted from text statements\n",
    "   - **Implicit relations**: Causal, classification, and compositional relations based on logical reasoning\n",
    "   - **Attribute relations**: Numerical, state, and feature-based descriptive relations\n",
    "\n",
    "4. **Quality First**:\n",
    "   - Each triple must be semantically clear and logically rigorous\n",
    "   - Prioritize relations between core concepts\n",
    "   - Avoid overly granular relations (e.g., \"goat\"-\"weight\"-\"45\" can be simplified to \"goat\"-\"weighs\"-\"45kg\")\n",
    "\n",
    "## 📤 Output Format\n",
    "\n",
    "Output ONLY a JSON array, with each triple containing head, relation, and tail fields:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {{\"head\":\"goat\", \"relation\":\"deficient_in\", \"tail\":\"vitamin_A\"}},\n",
    "  {{\"head\":\"vitamin_A_deficiency\", \"relation\":\"causes\", \"tail\":\"growth_retardation\"}},\n",
    "  {{\"head\":\"goat\", \"relation\":\"belongs_to\", \"tail\":\"ruminant\"}}\n",
    "]\n",
    "```\n",
    "\n",
    "## 📝 Text to Extract From\n",
    "\n",
    "{chunk_text}\n",
    "\n",
    "Begin extraction. Remember: **ONLY use entity names from the available entity list**!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def format_entity_list_for_prompt(entities: List[str], max_entities: int = 10000) -> str:\n",
    "    \"\"\"\n",
    "    Format entity list for prompt readability\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity names\n",
    "        max_entities: Maximum number of entities to display (avoid excessively long prompts)\n",
    "    \n",
    "    Returns:\n",
    "        Formatted entity list string\n",
    "    \"\"\"\n",
    "    if len(entities) <= max_entities:\n",
    "        entity_str = \"\\n\".join([f\"  • {entity}\" for entity in entities])\n",
    "        return f\"(Total: {len(entities)} entities)\\n\\n{entity_str}\"\n",
    "    else:\n",
    "        # If too many entities, show first N + total count\n",
    "        sample_entities = entities[:max_entities]\n",
    "        entity_str = \"\\n\".join([f\"  • {entity}\" for entity in sample_entities])\n",
    "        return f\"(Total: {len(entities)} entities, showing first {max_entities})\\n\\n{entity_str}\\n\\n... and {len(entities) - max_entities} more entities\"\n",
    "\n",
    "\n",
    "print(\"✅ Relation enhancement prompt loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b032fa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ EnhanceGraphConnectivity() 函數已載入\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 核心函數：EnhanceGraphConnectivity()\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def EnhanceGraphConnectivity(\n",
    "    driver,\n",
    "    client: Client,\n",
    "    model: str,\n",
    "    dataset_id: str = \"goat_kb_v1\",\n",
    "    max_entities_per_prompt: int = 200,\n",
    "    temperature: float = 0.2,\n",
    "    batch_size: int = 5,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    關係強化主函數：基於現有 Chunk 和 Entity 進行二次關係抽取\n",
    "    \n",
    "    核心原則：\n",
    "    1. 只連接、不創建（No CREATE, Only MATCH + MERGE）\n",
    "    2. 只對現有實體建立關係\n",
    "    3. 增量寫入，避免重複\n",
    "    \n",
    "    Args:\n",
    "        driver: Neo4j driver\n",
    "        client: Ollama client\n",
    "        model: LLM model name\n",
    "        dataset_id: Dataset ID\n",
    "        max_entities_per_prompt: 每次提示詞中包含的最大實體數\n",
    "        temperature: LLM temperature\n",
    "        batch_size: 批次處理大小\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'new_relations': int,  # 新增關係數量\n",
    "            'processed_chunks': int,  # 處理的 Chunk 數量\n",
    "            'density_before': float,  # 強化前的密度\n",
    "            'density_after': float,  # 強化後的密度\n",
    "            'avg_degree_before': float,  # 強化前的平均度數\n",
    "            'avg_degree_after': float,  # 強化後的平均度數\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"🔗 關係強化流程開始\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 階段零：記錄強化前的狀態\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    with driver.session() as session:\n",
    "        entity_count = session.run(\"MATCH (e:Entity) RETURN count(e) AS cnt\").single()[\"cnt\"]\n",
    "        relation_count_before = session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "        \n",
    "        # 計算平均度數\n",
    "        avg_degree_before = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            OPTIONAL MATCH (e)-[r:RELATION]-()\n",
    "            WITH e, count(r) AS degree\n",
    "            RETURN avg(degree) AS avg_degree\n",
    "        \"\"\").single()[\"avg_degree\"] or 0.0\n",
    "        \n",
    "    density_before = relation_count_before / entity_count if entity_count > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\n📊 強化前狀態：\")\n",
    "    print(f\"  • 實體節點：{entity_count:,}\")\n",
    "    print(f\"  • 語義關係：{relation_count_before:,}\")\n",
    "    print(f\"  • 關係密度：{density_before:.3f}\")\n",
    "    print(f\"  • 平均度數：{avg_degree_before:.2f}\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 階段一：檢索現有 Chunk 和 Entity\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n🔍 階段一：檢索現有數據...\")\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # 檢索所有 Chunk\n",
    "        chunks = session.run(\"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})\n",
    "            RETURN c.id AS chunk_id, c.text AS chunk_text\n",
    "            ORDER BY c.id\n",
    "        \"\"\", dataset=dataset_id).data()\n",
    "        \n",
    "        # 檢索所有 Entity\n",
    "        entities = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            RETURN e.name AS entity_name\n",
    "            ORDER BY e.name\n",
    "        \"\"\").data()\n",
    "        \n",
    "    entity_list = [e[\"entity_name\"] for e in entities]\n",
    "    \n",
    "    print(f\"  ✅ 檢索到 {len(chunks)} 個 Chunks\")\n",
    "    print(f\"  ✅ 檢索到 {len(entity_list)} 個 Entities\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 階段二：批次關係抽取\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n🤖 階段二：LLM 關係抽取（批次處理，batch_size={batch_size}）...\")\n",
    "    \n",
    "    all_extracted_triples = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # 格式化實體列表（只做一次）\n",
    "    formatted_entity_list = format_entity_list_for_prompt(entity_list, max_entities_per_prompt)\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i:i+batch_size]\n",
    "        \n",
    "        for chunk in batch_chunks:\n",
    "            chunk_id = chunk[\"chunk_id\"]\n",
    "            chunk_text = chunk[\"chunk_text\"]\n",
    "            \n",
    "            # 構建提示詞\n",
    "            prompt = RELATION_ENHANCEMENT_PROMPT.format(\n",
    "                entity_list=formatted_entity_list,\n",
    "                chunk_text=chunk_text\n",
    "            )\n",
    "            \n",
    "            # 調用 LLM\n",
    "            try:\n",
    "                response = client.chat(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    options={\"temperature\": temperature, \"top_p\": 0.9},\n",
    "                )\n",
    "                content = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "                \n",
    "                # 解析三元組\n",
    "                triples = parse_triples(content)\n",
    "                \n",
    "                # 添加來源信息\n",
    "                for triple in triples:\n",
    "                    triple[\"source_chunk\"] = chunk_id\n",
    "                \n",
    "                all_extracted_triples.extend(triples)\n",
    "                processed_count += 1\n",
    "                \n",
    "                if processed_count % 5 == 0:\n",
    "                    print(f\"  ↳ 已處理 {processed_count}/{len(chunks)} Chunks，累計提取 {len(all_extracted_triples)} 個三元組\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️  Chunk {chunk_id} 抽取失敗：{e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\n  ✅ 抽取完成：共 {len(all_extracted_triples)} 個候選三元組\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 階段三：增量寫入新關係（MATCH + MERGE）\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n💾 階段三：增量寫入新關係（僅連接現有實體）...\")\n",
    "    \n",
    "    new_relations_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        for triple in all_extracted_triples:\n",
    "            head = triple.get(\"head\")\n",
    "            relation = triple.get(\"relation\")\n",
    "            tail = triple.get(\"tail\")\n",
    "            source_chunk = triple.get(\"source_chunk\")\n",
    "            \n",
    "            if not all([head, relation, tail]):\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # 關鍵：使用 MATCH + MERGE（不創建新實體）\n",
    "            result = session.run(\"\"\"\n",
    "                // 1. 匹配現有的頭實體和尾實體\n",
    "                MATCH (h:Entity {name: $head})\n",
    "                MATCH (t:Entity {name: $tail})\n",
    "                \n",
    "                // 2. 增量合併關係（基於 head + type + tail 唯一性）\n",
    "                MERGE (h)-[r:RELATION {type: $relation}]->(t)\n",
    "                ON CREATE SET \n",
    "                    r.chunks = [$source_chunk],\n",
    "                    r.created_at = timestamp(),\n",
    "                    r.confidence = 0.95,\n",
    "                    r.enhanced = true\n",
    "                ON MATCH SET \n",
    "                    r.chunks = CASE \n",
    "                        WHEN $source_chunk IN r.chunks THEN r.chunks \n",
    "                        ELSE r.chunks + $source_chunk \n",
    "                    END,\n",
    "                    r.last_updated = timestamp()\n",
    "                \n",
    "                RETURN r.enhanced AS is_new\n",
    "            \"\"\", head=head, tail=tail, relation=relation, source_chunk=source_chunk)\n",
    "            \n",
    "            record = result.single()\n",
    "            if record and record.get(\"is_new\"):\n",
    "                new_relations_count += 1\n",
    "    \n",
    "    print(f\"  ✅ 新增關係：{new_relations_count:,}\")\n",
    "    print(f\"  ⚠️  跳過（實體不存在或格式錯誤）：{skipped_count:,}\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 階段四：計算強化後的狀態\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    with driver.session() as session:\n",
    "        relation_count_after = session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "        \n",
    "        avg_degree_after = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            OPTIONAL MATCH (e)-[r:RELATION]-()\n",
    "            WITH e, count(r) AS degree\n",
    "            RETURN avg(degree) AS avg_degree\n",
    "        \"\"\").single()[\"avg_degree\"] or 0.0\n",
    "    \n",
    "    density_after = relation_count_after / entity_count if entity_count > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\n📊 強化後狀態：\")\n",
    "    print(f\"  • 實體節點：{entity_count:,} （無變化 ✅）\")\n",
    "    print(f\"  • 語義關係：{relation_count_after:,} （+{relation_count_after - relation_count_before:,} ✅）\")\n",
    "    print(f\"  • 關係密度：{density_after:.3f} （從 {density_before:.3f} 提升 {((density_after/density_before - 1) * 100):.1f}% ✅）\")\n",
    "    print(f\"  • 平均度數：{avg_degree_after:.2f} （從 {avg_degree_before:.2f} 提升 {((avg_degree_after/avg_degree_before - 1) * 100):.1f}% ✅）\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✅ 關係強化流程完成！\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return {\n",
    "        \"new_relations\": new_relations_count,\n",
    "        \"processed_chunks\": processed_count,\n",
    "        \"density_before\": density_before,\n",
    "        \"density_after\": density_after,\n",
    "        \"avg_degree_before\": avg_degree_before,\n",
    "        \"avg_degree_after\": avg_degree_after,\n",
    "        \"entity_count\": entity_count,\n",
    "        \"relation_count_before\": relation_count_before,\n",
    "        \"relation_count_after\": relation_count_after,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✅ EnhanceGraphConnectivity() 函數已載入\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69fe19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 目標指標：\n",
      "  • 關係密度目標：≥ 1.0\n",
      "  • 平均度數目標：≥ 2.0\n",
      "  • 最大迭代次數：20\n",
      "\n",
      "======================================================================\n",
      "\n",
      "🔄 第 1 輪關係強化開始...\n",
      "======================================================================\n",
      "======================================================================\n",
      "🔗 關係強化流程開始\n",
      "======================================================================\n",
      "\n",
      "📊 強化前狀態：\n",
      "  • 實體節點：6,343\n",
      "  • 語義關係：5,333\n",
      "  • 關係密度：0.841\n",
      "  • 平均度數：1.68\n",
      "\n",
      "🔍 階段一：檢索現有數據...\n",
      "  ✅ 檢索到 0 個 Chunks\n",
      "  ✅ 檢索到 6343 個 Entities\n",
      "\n",
      "🤖 階段二：LLM 關係抽取（批次處理，batch_size=5）...\n",
      "\n",
      "  ✅ 抽取完成：共 0 個候選三元組\n",
      "\n",
      "💾 階段三：增量寫入新關係（僅連接現有實體）...\n",
      "  ✅ 新增關係：0\n",
      "  ⚠️  跳過（實體不存在或格式錯誤）：0\n",
      "\n",
      "📊 強化後狀態：\n",
      "  • 實體節點：6,343 （無變化 ✅）\n",
      "  • 語義關係：5,333 （+0 ✅）\n",
      "  • 關係密度：0.841 （從 0.841 提升 0.0% ✅）\n",
      "  • 平均度數：1.68 （從 1.68 提升 0.0% ✅）\n",
      "\n",
      "======================================================================\n",
      "✅ 關係強化流程完成！\n",
      "======================================================================\n",
      "\n",
      "📋 第 1 輪強化結果：\n",
      "  • 新增關係數：0\n",
      "  • 處理 Chunks：0\n",
      "  • 密度提升：0.841 → 0.841\n",
      "  • 度數提升：1.68 → 1.68\n",
      "\n",
      "📊 當前狀態檢查：\n",
      "  • 密度：0.841 / 1.0 ❌ 未達標\n",
      "  • 度數：1.68 / 2.0 ❌ 未達標\n",
      "\n",
      "⚠️ 本輪未新增關係，停止迭代（可能已無法進一步優化）\n",
      "======================================================================\n",
      "\n",
      "\n",
      "📈 關係強化總結報告\n",
      "======================================================================\n",
      "總迭代輪數：1\n",
      "\n",
      "各輪強化詳情：\n",
      "  第 1 輪：\n",
      "    • 新增關係：0\n",
      "    • 密度變化：0.841 → 0.841 (+0.000)\n",
      "    • 度數變化：1.68 → 1.68 (+0.00)\n",
      "\n",
      "累計成果：\n",
      "  • 累計新增關係：0\n",
      "  • 密度總提升：0.841 → 0.841 (+0.000)\n",
      "  • 度數總提升：1.68 → 1.68 (+0.00)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 迭代執行關係強化流程（自動循環直到達標）\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "# 目標指標\n",
    "TARGET_DENSITY = 2.5\n",
    "TARGET_AVG_DEGREE = 5.0\n",
    "MAX_ITERATIONS = 20  # 最大迭代次數，防止無限循環\n",
    "\n",
    "print(\"🎯 目標指標：\")\n",
    "print(f\"  • 關係密度目標：≥ {TARGET_DENSITY}\")\n",
    "print(f\"  • 平均度數目標：≥ {TARGET_AVG_DEGREE}\")\n",
    "print(f\"  • 最大迭代次數：{MAX_ITERATIONS}\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "iteration = 0\n",
    "all_enhancement_results = []\n",
    "\n",
    "while iteration < MAX_ITERATIONS:\n",
    "    iteration += 1\n",
    "    print(f\"\\n🔄 第 {iteration} 輪關係強化開始...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 執行關係強化\n",
    "    ENHANCEMENT_RESULTS = EnhanceGraphConnectivity(\n",
    "        driver=GRAPH_DRIVER,\n",
    "        client=OLLAMA_CLIENT,\n",
    "        model=GRAPH_CREATE_MODEL,\n",
    "        dataset_id=DATASET_ID,\n",
    "        max_entities_per_prompt=400,  # 每次提示詞最多包含 400 個實體\n",
    "        temperature=0.0,  # 較低溫度確保穩定輸出\n",
    "        batch_size=5,  # 每 5 個 Chunk 顯示一次進度\n",
    "    )\n",
    "    \n",
    "    # 保存本輪結果\n",
    "    ENHANCEMENT_RESULTS['iteration'] = iteration\n",
    "    all_enhancement_results.append(ENHANCEMENT_RESULTS)\n",
    "    \n",
    "    # 顯示本輪結果\n",
    "    print(f\"\\n📋 第 {iteration} 輪強化結果：\")\n",
    "    print(f\"  • 新增關係數：{ENHANCEMENT_RESULTS['new_relations']:,}\")\n",
    "    print(f\"  • 處理 Chunks：{ENHANCEMENT_RESULTS['processed_chunks']}\")\n",
    "    print(f\"  • 密度提升：{ENHANCEMENT_RESULTS['density_before']:.3f} → {ENHANCEMENT_RESULTS['density_after']:.3f}\")\n",
    "    print(f\"  • 度數提升：{ENHANCEMENT_RESULTS['avg_degree_before']:.2f} → {ENHANCEMENT_RESULTS['avg_degree_after']:.2f}\")\n",
    "    \n",
    "    # 檢查是否達標\n",
    "    current_density = ENHANCEMENT_RESULTS['density_after']\n",
    "    current_avg_degree = ENHANCEMENT_RESULTS['avg_degree_after']\n",
    "    new_relations = ENHANCEMENT_RESULTS['new_relations']\n",
    "    \n",
    "    density_met = current_density >= TARGET_DENSITY\n",
    "    degree_met = current_avg_degree >= TARGET_AVG_DEGREE\n",
    "    \n",
    "    print(f\"\\n📊 當前狀態檢查：\")\n",
    "    print(f\"  • 密度：{current_density:.3f} / {TARGET_DENSITY} {'✅ 達標' if density_met else '❌ 未達標'}\")\n",
    "    print(f\"  • 度數：{current_avg_degree:.2f} / {TARGET_AVG_DEGREE} {'✅ 達標' if degree_met else '❌ 未達標'}\")\n",
    "    \n",
    "    # 判斷是否繼續迭代\n",
    "    if density_met and degree_met:\n",
    "        print(f\"\\n🎉 恭喜！所有指標已達標，共執行 {iteration} 輪強化\")\n",
    "        print(\"=\"*70)\n",
    "        break\n",
    "    elif new_relations == 0:\n",
    "        print(f\"\\n⚠️ 本輪未新增關係，停止迭代（可能已無法進一步優化）\")\n",
    "        print(\"=\"*70)\n",
    "        break\n",
    "    else:\n",
    "        if iteration < MAX_ITERATIONS:\n",
    "            print(f\"\\n⏭️ 指標未達標，準備執行第 {iteration + 1} 輪強化...\")\n",
    "        else:\n",
    "            print(f\"\\n⏹️ 已達最大迭代次數 ({MAX_ITERATIONS})，停止強化\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 強化流程總結\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(f\"\\n\\n📈 關係強化總結報告\")\n",
    "print(\"=\"*70)\n",
    "print(f\"總迭代輪數：{len(all_enhancement_results)}\")\n",
    "print(f\"\\n各輪強化詳情：\")\n",
    "\n",
    "total_new_relations = 0\n",
    "for result in all_enhancement_results:\n",
    "    total_new_relations += result['new_relations']\n",
    "    print(f\"  第 {result['iteration']} 輪：\")\n",
    "    print(f\"    • 新增關係：{result['new_relations']:,}\")\n",
    "    print(f\"    • 密度變化：{result['density_before']:.3f} → {result['density_after']:.3f} (+{result['density_after']-result['density_before']:.3f})\")\n",
    "    print(f\"    • 度數變化：{result['avg_degree_before']:.2f} → {result['avg_degree_after']:.2f} (+{result['avg_degree_after']-result['avg_degree_before']:.2f})\")\n",
    "\n",
    "print(f\"\\n累計成果：\")\n",
    "print(f\"  • 累計新增關係：{total_new_relations:,}\")\n",
    "if all_enhancement_results:\n",
    "    initial_density = all_enhancement_results[0]['density_before']\n",
    "    final_density = all_enhancement_results[-1]['density_after']\n",
    "    initial_degree = all_enhancement_results[0]['avg_degree_before']\n",
    "    final_degree = all_enhancement_results[-1]['avg_degree_after']\n",
    "    print(f\"  • 密度總提升：{initial_density:.3f} → {final_density:.3f} (+{final_density-initial_density:.3f})\")\n",
    "    print(f\"  • 度數總提升：{initial_degree:.2f} → {final_degree:.2f} (+{final_degree-initial_degree:.2f})\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ee0944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 圖譜完整度與質量最終檢驗報告\n",
      "======================================================================\n",
      "\n",
      "📊 一、基礎指標\n",
      "----------------------------------------------------------------------\n",
      "  • 實體節點數：6,343\n",
      "  • 語義關係數：33,606\n",
      "  • 文本 Chunks：0\n",
      "  • MENTIONS 連接：7,757\n",
      "  • 關係密度：5.298 ✅ 優秀\n",
      "  • 平均度數：10.60 ✅ 優秀\n",
      "\n",
      "🔗 二、連接質量分析\n",
      "----------------------------------------------------------------------\n",
      "  1. 孤立實體：0 (0.0%) ✅ 優秀\n",
      "  2. 弱連接實體（度數=1）：80 (1.3%) ✅ 優秀\n",
      "  3. 強連接實體（度數≥5）：5,554 (87.6%) ✅ 優秀\n",
      "  4. 多來源關係（≥2 Chunks）：1,733 (5.2%) ⚠️ 待優化\n",
      "\n",
      "⚡ 三、關係強化效果\n",
      "----------------------------------------------------------------------\n",
      "  • 強化新增關係：0 (0.0%)\n",
      "  • 原始關係：33,606 (100.0%)\n",
      "\n",
      "🎨 四、關係類型多樣性\n",
      "----------------------------------------------------------------------\n",
      "  • 關係類型總數：6344\n",
      "  • 前 10 種關係類型：\n",
      "     1. affects                         2,160 (  6.4%)\n",
      "     2. relates_to                      1,567 (  4.7%)\n",
      "     3. related_to                      1,549 (  4.6%)\n",
      "     4. uses                            1,112 (  3.3%)\n",
      "     5. causes                            934 (  2.8%)\n",
      "     6. requires                          871 (  2.6%)\n",
      "     7. contributes_to                    620 (  1.8%)\n",
      "     8. influenced_by                     527 (  1.6%)\n",
      "     9. contains                          504 (  1.5%)\n",
      "    10. influences                        478 (  1.4%)\n",
      "\n",
      "🌟 五、核心樞紐節點（Top 10）\n",
      "----------------------------------------------------------------------\n",
      "   1. calcium                                  2590 個關係\n",
      "   2. sheep                                    1933 個關係\n",
      "   3. goat                                     1727 個關係\n",
      "   4. Timothy_Grass                            1668 個關係\n",
      "   5. vitamin_A_deficiency                     1440 個關係\n",
      "   6. acute_bloat                              1369 個關係\n",
      "   7. forage                                   1296 個關係\n",
      "   8. Alfalfa                                  1038 個關係\n",
      "   9. scrapie                                  817 個關係\n",
      "  10. colostrum                                800 個關係\n",
      "\n",
      "📍 六、文本覆蓋率分析\n",
      "----------------------------------------------------------------------\n",
      "  • 已覆蓋 Chunks：0 / 0 (0.0%) ⚠️ 待優化\n",
      "  • 平均每 Chunk 實體數：0.0\n",
      "\n",
      "⚠️ 七、潛在質量問題檢測\n",
      "----------------------------------------------------------------------\n",
      "  ✅ 未發現明顯質量問題\n",
      "\n",
      "======================================================================\n",
      "🏆 最終評級\n",
      "======================================================================\n",
      "  ✅ 關係密度 ≥ 1.0：5.298\n",
      "  ✅ 平均度數 ≥ 2.0：10.60\n",
      "  ✅ 孤立實體 < 5%：0.0%\n",
      "  ✅ 弱連接實體 < 30%：1.3%\n",
      "  ✅ 強連接實體 ≥ 10%：87.6%\n",
      "  ❌ 文本覆蓋率 ≥ 95%：0.0%\n",
      "  ✅ 無質量問題：是\n",
      "\n",
      "  總分：6/7\n",
      "  等級：A 優秀\n",
      "======================================================================\n",
      "\n",
      "✅ 圖譜完整度與質量檢驗完成！\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 圖譜完整度與質量最終檢驗\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    print(\"🔍 圖譜完整度與質量最終檢驗報告\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 第一部分：基礎指標\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(\"\\n📊 一、基礎指標\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    entity_count = session.run(\"MATCH (e:Entity) RETURN count(e) AS cnt\").single()[\"cnt\"]\n",
    "    relation_count = session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "    chunk_count = session.run(f\"MATCH (c:Chunk {{dataset: '{DATASET_ID}'}}) RETURN count(c) AS cnt\").single()[\"cnt\"]\n",
    "    mentions_count = session.run(\"MATCH ()-[m:MENTIONS]->() RETURN count(m) AS cnt\").single()[\"cnt\"]\n",
    "    \n",
    "    density = relation_count / entity_count if entity_count > 0 else 0.0\n",
    "    \n",
    "    avg_degree = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        OPTIONAL MATCH (e)-[r:RELATION]-()\n",
    "        WITH e, count(r) AS degree\n",
    "        RETURN avg(degree) AS avg_degree\n",
    "    \"\"\").single()[\"avg_degree\"] or 0.0\n",
    "    \n",
    "    print(f\"  • 實體節點數：{entity_count:,}\")\n",
    "    print(f\"  • 語義關係數：{relation_count:,}\")\n",
    "    print(f\"  • 文本 Chunks：{chunk_count:,}\")\n",
    "    print(f\"  • MENTIONS 連接：{mentions_count:,}\")\n",
    "    print(f\"  • 關係密度：{density:.3f} {'✅ 優秀' if density >= TARGET_DENSITY else '⚠️ 待優化'}\")\n",
    "    print(f\"  • 平均度數：{avg_degree:.2f} {'✅ 優秀' if avg_degree >= TARGET_AVG_DEGREE else '⚠️ 待優化'}\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 第二部分：連接質量\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n🔗 二、連接質量分析\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # 1. 孤立實體檢測\n",
    "    isolated_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    isolated_percent = (isolated_entities / entity_count * 100) if entity_count > 0 else 0\n",
    "    print(f\"  1. 孤立實體：{isolated_entities:,} ({isolated_percent:.1f}%) {'✅ 優秀' if isolated_percent == 0 else '⚠️ 需注意' if isolated_percent < 5 else '❌ 需改進'}\")\n",
    "    \n",
    "    # 2. 弱連接實體（度數 = 1）\n",
    "    weak_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)-[r:RELATION]-()\n",
    "        WITH e, count(r) AS degree\n",
    "        WHERE degree = 1\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    weak_percent = (weak_entities / entity_count * 100) if entity_count > 0 else 0\n",
    "    print(f\"  2. 弱連接實體（度數=1）：{weak_entities:,} ({weak_percent:.1f}%) {'✅ 優秀' if weak_percent < 20 else '⚠️ 需注意'}\")\n",
    "    \n",
    "    # 3. 強連接實體（度數 ≥ 5）\n",
    "    strong_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)-[r:RELATION]-()\n",
    "        WITH e, count(r) AS degree\n",
    "        WHERE degree >= 5\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    strong_percent = (strong_entities / entity_count * 100) if entity_count > 0 else 0\n",
    "    print(f\"  3. 強連接實體（度數≥5）：{strong_entities:,} ({strong_percent:.1f}%) {'✅ 優秀' if strong_percent >= 10 else '⚠️ 待優化'}\")\n",
    "    \n",
    "    # 4. 多來源關係（跨 Chunk 關係）\n",
    "    multi_source_relations = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        WHERE size(r.chunks) >= 2\n",
    "        RETURN count(r) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    multi_source_percent = (multi_source_relations / relation_count * 100) if relation_count > 0 else 0\n",
    "    print(f\"  4. 多來源關係（≥2 Chunks）：{multi_source_relations:,} ({multi_source_percent:.1f}%) {'✅ 優秀' if multi_source_percent >= 20 else '⚠️ 待優化'}\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 第三部分：關係強化效果\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n⚡ 三、關係強化效果\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    enhanced_relations = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        WHERE r.enhanced = true\n",
    "        RETURN count(r) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    enhanced_percent = (enhanced_relations / relation_count * 100) if relation_count > 0 else 0\n",
    "    print(f\"  • 強化新增關係：{enhanced_relations:,} ({enhanced_percent:.1f}%)\")\n",
    "    print(f\"  • 原始關係：{relation_count - enhanced_relations:,} ({100-enhanced_percent:.1f}%)\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 第四部分：關係類型多樣性\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n🎨 四、關係類型多樣性\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    relation_type_count = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        RETURN count(DISTINCT r.type) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    print(f\"  • 關係類型總數：{relation_type_count}\")\n",
    "    \n",
    "    relation_types = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        RETURN r.type AS relation_type, count(r) AS cnt\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").data()\n",
    "    \n",
    "    print(f\"  • 前 10 種關係類型：\")\n",
    "    for idx, row in enumerate(relation_types, 1):\n",
    "        percent = (row['cnt'] / relation_count * 100) if relation_count > 0 else 0\n",
    "        print(f\"    {idx:2d}. {row['relation_type']:<30s} {row['cnt']:>6,} ({percent:>5.1f}%)\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 第五部分：核心樞紐節點\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n🌟 五、核心樞紐節點（Top 10）\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    hub_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)-[r:RELATION]-()\n",
    "        WITH e, count(r) AS degree\n",
    "        WHERE degree >= 5\n",
    "        RETURN e.name AS entity_name, degree\n",
    "        ORDER BY degree DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").data()\n",
    "    \n",
    "    if hub_entities:\n",
    "        for idx, row in enumerate(hub_entities, 1):\n",
    "            print(f\"  {idx:2d}. {row['entity_name']:<40s} {row['degree']:>3} 個關係\")\n",
    "    else:\n",
    "        print(\"  ⚠️ 未發現度數 ≥ 5 的核心節點\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 第六部分：覆蓋率分析\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n📍 六、文本覆蓋率分析\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # 有實體的 Chunks\n",
    "    covered_chunks = session.run(f\"\"\"\n",
    "        MATCH (c:Chunk {{dataset: '{DATASET_ID}'}})-[:MENTIONS]->()\n",
    "        RETURN count(DISTINCT c) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    coverage_percent = (covered_chunks / chunk_count * 100) if chunk_count > 0 else 0\n",
    "    print(f\"  • 已覆蓋 Chunks：{covered_chunks} / {chunk_count} ({coverage_percent:.1f}%) {'✅ 優秀' if coverage_percent >= 95 else '⚠️ 待優化'}\")\n",
    "    \n",
    "    # 平均每個 Chunk 的實體數\n",
    "    avg_entities_per_chunk = session.run(f\"\"\"\n",
    "        MATCH (c:Chunk {{dataset: '{DATASET_ID}'}})-[:MENTIONS]->(e:Entity)\n",
    "        WITH c, count(DISTINCT e) AS entity_count\n",
    "        RETURN avg(entity_count) AS avg_cnt\n",
    "    \"\"\").single()[\"avg_cnt\"] or 0\n",
    "    \n",
    "    print(f\"  • 平均每 Chunk 實體數：{avg_entities_per_chunk:.1f}\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 第七部分：質量問題檢測\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n⚠️ 七、潛在質量問題檢測\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    issues_found = []\n",
    "    \n",
    "    # 檢測 1：自環關係\n",
    "    self_loops = session.run(\"\"\"\n",
    "        MATCH (e:Entity)-[r:RELATION]->(e)\n",
    "        RETURN count(r) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    if self_loops > 0:\n",
    "        issues_found.append(f\"發現 {self_loops} 個自環關係\")\n",
    "    \n",
    "    # 檢測 2：空實體名稱\n",
    "    empty_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE e.name IS NULL OR trim(e.name) = ''\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    if empty_entities > 0:\n",
    "        issues_found.append(f\"發現 {empty_entities} 個空實體名稱\")\n",
    "    \n",
    "    # 檢測 3：重複關係（相同頭尾和類型）\n",
    "    duplicate_relations = session.run(\"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        WITH h, t, r.type AS rel_type, count(r) AS cnt\n",
    "        WHERE cnt > 1\n",
    "        RETURN count(*) AS dup_cnt\n",
    "    \"\"\").single()[\"dup_cnt\"]\n",
    "    if duplicate_relations > 0:\n",
    "        issues_found.append(f\"發現 {duplicate_relations} 組重複關係\")\n",
    "    \n",
    "    # 檢測 4：超長實體名稱（可能是句子片段）\n",
    "    long_entities = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE size(e.name) > 50\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    if long_entities > 0:\n",
    "        issues_found.append(f\"發現 {long_entities} 個超長實體名稱（>50字元）\")\n",
    "    \n",
    "    if issues_found:\n",
    "        for issue in issues_found:\n",
    "            print(f\"  ⚠️ {issue}\")\n",
    "    else:\n",
    "        print(\"  ✅ 未發現明顯質量問題\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 最終評級\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🏆 最終評級\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    score = 0\n",
    "    max_score = 7\n",
    "    \n",
    "    # 評分項目\n",
    "    if density >= TARGET_DENSITY:\n",
    "        score += 1\n",
    "        density_status = \"✅\"\n",
    "    else:\n",
    "        density_status = \"❌\"\n",
    "    \n",
    "    if avg_degree >= TARGET_AVG_DEGREE:\n",
    "        score += 1\n",
    "        degree_status = \"✅\"\n",
    "    else:\n",
    "        degree_status = \"❌\"\n",
    "    \n",
    "    if isolated_percent < 5:\n",
    "        score += 1\n",
    "        isolated_status = \"✅\"\n",
    "    else:\n",
    "        isolated_status = \"❌\"\n",
    "    \n",
    "    if weak_percent < 30:\n",
    "        score += 1\n",
    "        weak_status = \"✅\"\n",
    "    else:\n",
    "        weak_status = \"❌\"\n",
    "    \n",
    "    if strong_percent >= 10:\n",
    "        score += 1\n",
    "        strong_status = \"✅\"\n",
    "    else:\n",
    "        strong_status = \"❌\"\n",
    "    \n",
    "    if coverage_percent >= 95:\n",
    "        score += 1\n",
    "        coverage_status = \"✅\"\n",
    "    else:\n",
    "        coverage_status = \"❌\"\n",
    "    \n",
    "    if len(issues_found) == 0:\n",
    "        score += 1\n",
    "        quality_status = \"✅\"\n",
    "    else:\n",
    "        quality_status = \"❌\"\n",
    "    \n",
    "    print(f\"  {density_status} 關係密度 ≥ {TARGET_DENSITY}：{density:.3f}\")\n",
    "    print(f\"  {degree_status} 平均度數 ≥ {TARGET_AVG_DEGREE}：{avg_degree:.2f}\")\n",
    "    print(f\"  {isolated_status} 孤立實體 < 5%：{isolated_percent:.1f}%\")\n",
    "    print(f\"  {weak_status} 弱連接實體 < 30%：{weak_percent:.1f}%\")\n",
    "    print(f\"  {strong_status} 強連接實體 ≥ 10%：{strong_percent:.1f}%\")\n",
    "    print(f\"  {coverage_status} 文本覆蓋率 ≥ 95%：{coverage_percent:.1f}%\")\n",
    "    print(f\"  {quality_status} 無質量問題：{'是' if len(issues_found) == 0 else '否'}\")\n",
    "    \n",
    "    print(f\"\\n  總分：{score}/{max_score}\")\n",
    "    \n",
    "    if score == max_score:\n",
    "        grade = \"A+ 卓越\"\n",
    "    elif score >= 6:\n",
    "        grade = \"A 優秀\"\n",
    "    elif score >= 5:\n",
    "        grade = \"B 良好\"\n",
    "    elif score >= 4:\n",
    "        grade = \"C 及格\"\n",
    "    else:\n",
    "        grade = \"D 待改進\"\n",
    "    \n",
    "    print(f\"  等級：{grade}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\n✅ 圖譜完整度與質量檢驗完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6e8c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 開始自動修正圖譜質量問題...\n",
      "======================================================================\n",
      "\n",
      "🔍 修正 1：移除自環關係\n",
      "----------------------------------------------------------------------\n",
      "  ✅ 未發現自環關係\n",
      "\n",
      "🔍 修正 2：處理超長實體名稱\n",
      "----------------------------------------------------------------------\n",
      "  ✅ 未發現超長實體名稱\n",
      "\n",
      "🔍 修正 3：合併重複關係\n",
      "----------------------------------------------------------------------\n",
      "  ✅ 未發現重複關係\n",
      "\n",
      "🔍 修正 4：移除空實體名稱\n",
      "----------------------------------------------------------------------\n",
      "  ✅ 未發現空實體名稱\n",
      "\n",
      "======================================================================\n",
      "📋 質量修正摘要\n",
      "======================================================================\n",
      "  • 移除自環關係：0\n",
      "  • 移除超長實體：0\n",
      "  • 合併重複關係：0\n",
      "  • 移除空實體：0\n",
      "\n",
      "  總計修正：0 個問題\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 圖譜質量問題自動修正\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔧 開始自動修正圖譜質量問題...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fix_summary = {\n",
    "    'self_loops_removed': 0,\n",
    "    'long_entities_truncated': 0,\n",
    "    'duplicate_relations_merged': 0,\n",
    "    'empty_entities_removed': 0\n",
    "}\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 修正 1：移除自環關係（實體指向自己）\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(\"\\n🔍 修正 1：移除自環關係\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    self_loops_count = session.run(\"\"\"\n",
    "        MATCH (e:Entity)-[r:RELATION]->(e)\n",
    "        RETURN count(r) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    if self_loops_count > 0:\n",
    "        print(f\"  發現 {self_loops_count} 個自環關係，正在移除...\")\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e:Entity)-[r:RELATION]->(e)\n",
    "            DELETE r\n",
    "            RETURN count(r) AS deleted\n",
    "        \"\"\").single()\n",
    "        fix_summary['self_loops_removed'] = result['deleted']\n",
    "        print(f\"  ✅ 已移除 {result['deleted']} 個自環關係\")\n",
    "    else:\n",
    "        print(\"  ✅ 未發現自環關係\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 修正 2：處理超長實體名稱（>50字元）\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n🔍 修正 2：處理超長實體名稱\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    long_entities_count = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE size(e.name) > 50\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    if long_entities_count > 0:\n",
    "        print(f\"  發現 {long_entities_count} 個超長實體名稱\")\n",
    "        \n",
    "        # 獲取樣本檢查\n",
    "        samples = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WHERE size(e.name) > 50\n",
    "            RETURN e.name AS name, size(e.name) AS length\n",
    "            ORDER BY length DESC\n",
    "            LIMIT 100\n",
    "        \"\"\").data()\n",
    "        \n",
    "        print(f\"  樣本（前5個）：\")\n",
    "        for s in samples:\n",
    "            display_name = s['name'][:60] + \"...\" if len(s['name']) > 60 else s['name']\n",
    "            print(f\"    • {display_name} (長度: {s['length']})\")\n",
    "        \n",
    "        # 分析超長實體的連接度\n",
    "        connectivity_stats = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WHERE size(e.name) > 50\n",
    "            OPTIONAL MATCH (e)-[r:RELATION]-()\n",
    "            WITH e, count(r) AS degree\n",
    "            RETURN \n",
    "                count(e) AS total,\n",
    "                sum(CASE WHEN degree = 0 THEN 1 ELSE 0 END) AS isolated,\n",
    "                sum(CASE WHEN degree = 1 THEN 1 ELSE 0 END) AS weak,\n",
    "                avg(degree) AS avg_degree\n",
    "        \"\"\").single()\n",
    "        \n",
    "        print(f\"\\n  連接度分析：\")\n",
    "        print(f\"    • 孤立（度數=0）：{connectivity_stats['isolated']}/{connectivity_stats['total']}\")\n",
    "        print(f\"    • 弱連接（度數=1）：{connectivity_stats['weak']}/{connectivity_stats['total']}\")\n",
    "        print(f\"    • 平均度數：{connectivity_stats['avg_degree']:.2f}\")\n",
    "        \n",
    "        # 策略：自動移除這些句子片段實體（因為它們通常是抽取錯誤）\n",
    "        print(f\"\\n  💡 建議：這些超長實體通常是句子片段，會降低圖譜質量\")\n",
    "        print(f\"     → 自動移除這些實體及其關係...\")\n",
    "        \n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WHERE size(e.name) > 50\n",
    "            DETACH DELETE e\n",
    "            RETURN count(e) AS deleted\n",
    "        \"\"\").single()\n",
    "        fix_summary['long_entities_truncated'] = result['deleted']\n",
    "        print(f\"  ✅ 已移除 {result['deleted']} 個超長實體及其關係\")\n",
    "    else:\n",
    "        print(\"  ✅ 未發現超長實體名稱\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 修正 3：合併重複關係（相同頭尾和類型）\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n🔍 修正 3：合併重複關係\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    duplicate_groups = session.run(\"\"\"\n",
    "        MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "        WITH h, t, r.type AS rel_type, collect(r) AS rels\n",
    "        WHERE size(rels) > 1\n",
    "        RETURN count(*) AS dup_groups, sum(size(rels) - 1) AS extra_rels\n",
    "    \"\"\").single()\n",
    "    \n",
    "    dup_groups = duplicate_groups['dup_groups'] or 0\n",
    "    extra_rels = duplicate_groups['extra_rels'] or 0\n",
    "    \n",
    "    if dup_groups > 0:\n",
    "        print(f\"  發現 {dup_groups} 組重複關係（共 {extra_rels} 個多餘關係）\")\n",
    "        print(f\"  正在合併重複關係（保留第一個，合併 chunks 屬性）...\")\n",
    "        \n",
    "        # 合併策略：保留第一個關係，將其他關係的 chunks 合併進去\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "            WITH h, t, r.type AS rel_type, collect(r) AS rels\n",
    "            WHERE size(rels) > 1\n",
    "            WITH h, t, rel_type, rels[0] AS keep, rels[1..] AS remove\n",
    "            UNWIND remove AS del_rel\n",
    "            WITH h, t, rel_type, keep, del_rel, \n",
    "                 COALESCE(keep.chunks, []) + COALESCE(del_rel.chunks, []) AS merged_chunks\n",
    "            SET keep.chunks = merged_chunks\n",
    "            DELETE del_rel\n",
    "            RETURN count(del_rel) AS merged\n",
    "        \"\"\").single()\n",
    "        \n",
    "        fix_summary['duplicate_relations_merged'] = result['merged']\n",
    "        print(f\"  ✅ 已合併 {result['merged']} 個重複關係\")\n",
    "    else:\n",
    "        print(\"  ✅ 未發現重複關係\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # 修正 4：移除空實體名稱\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(f\"\\n🔍 修正 4：移除空實體名稱\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    empty_entities_count = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE e.name IS NULL OR trim(e.name) = ''\n",
    "        RETURN count(e) AS cnt\n",
    "    \"\"\").single()[\"cnt\"]\n",
    "    \n",
    "    if empty_entities_count > 0:\n",
    "        print(f\"  發現 {empty_entities_count} 個空實體名稱，正在移除...\")\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WHERE e.name IS NULL OR trim(e.name) = ''\n",
    "            DETACH DELETE e\n",
    "            RETURN count(e) AS deleted\n",
    "        \"\"\").single()\n",
    "        fix_summary['empty_entities_removed'] = result['deleted']\n",
    "        print(f\"  ✅ 已移除 {result['deleted']} 個空實體\")\n",
    "    else:\n",
    "        print(\"  ✅ 未發現空實體名稱\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# 修正摘要\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"📋 質量修正摘要\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  • 移除自環關係：{fix_summary['self_loops_removed']}\")\n",
    "print(f\"  • 移除超長實體：{fix_summary['long_entities_truncated']}\")\n",
    "print(f\"  • 合併重複關係：{fix_summary['duplicate_relations_merged']}\")\n",
    "print(f\"  • 移除空實體：{fix_summary['empty_entities_removed']}\")\n",
    "\n",
    "total_fixes = sum(fix_summary.values())\n",
    "print(f\"\\n  總計修正：{total_fixes} 個問題\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if total_fixes > 0:\n",
    "    print(\"\\n💡 建議：重新執行圖譜質量檢驗以確認修正效果\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0bf0351a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 孤立實體深度診斷\n",
      "======================================================================\n",
      "\n",
      "📋 孤立實體樣本（前 30 個，按提及次數排序）：\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "📊 孤立實體類型分析：\n",
      "----------------------------------------------------------------------\n",
      "  • 總孤立實體數：0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     50\u001b[39m isolated_stats = session.run(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[33m    MATCH (e:Entity)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[33m    WHERE NOT (e)-[:RELATION]-()\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m \u001b[33m        sum(CASE WHEN size(e.name) > 40 THEN 1 ELSE 0 END) AS too_long\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m).single()\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  • 總孤立實體數：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misolated_stats[\u001b[33m'\u001b[39m\u001b[33mtotal_isolated\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  • 無 MENTIONS 連接：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misolated_stats[\u001b[33m'\u001b[39m\u001b[33mno_mentions\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43misolated_stats\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mno_mentions\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m/\u001b[49m\u001b[43misolated_stats\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtotal_isolated\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  • 單一 Chunk 提及：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misolated_stats[\u001b[33m'\u001b[39m\u001b[33msingle_mention\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misolated_stats[\u001b[33m'\u001b[39m\u001b[33msingle_mention\u001b[39m\u001b[33m'\u001b[39m]/isolated_stats[\u001b[33m'\u001b[39m\u001b[33mtotal_isolated\u001b[39m\u001b[33m'\u001b[39m]*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  • 多 Chunk 提及：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misolated_stats[\u001b[33m'\u001b[39m\u001b[33mmultiple_mentions\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misolated_stats[\u001b[33m'\u001b[39m\u001b[33mmultiple_mentions\u001b[39m\u001b[33m'\u001b[39m]/isolated_stats[\u001b[33m'\u001b[39m\u001b[33mtotal_isolated\u001b[39m\u001b[33m'\u001b[39m]*\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 🔍 孤立實體診斷分析\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔍 孤立實體深度診斷\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    # 1. 獲取孤立實體樣本及其來源\n",
    "    isolated_samples = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "        OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(e)\n",
    "        WITH e, collect(DISTINCT c.id) AS source_chunks\n",
    "        RETURN \n",
    "            e.name AS entity,\n",
    "            size(source_chunks) AS mention_count,\n",
    "            source_chunks[0] AS sample_chunk\n",
    "        ORDER BY mention_count DESC\n",
    "        LIMIT 30\n",
    "    \"\"\").data()\n",
    "    \n",
    "    print(f\"\\n📋 孤立實體樣本（前 30 個，按提及次數排序）：\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for idx, item in enumerate(isolated_samples, 1):\n",
    "        entity = item['entity']\n",
    "        mentions = item['mention_count']\n",
    "        chunk_id = item['sample_chunk'] or \"未找到來源\"\n",
    "        \n",
    "        # 分類分析\n",
    "        entity_type = \"\"\n",
    "        if len(entity) < 3:\n",
    "            entity_type = \"[過短]\"\n",
    "        elif entity.replace('_', '').replace('-', '').isdigit():\n",
    "            entity_type = \"[純數字]\"\n",
    "        elif entity.lower() in ['it', 'this', 'that', 'they', 'these']:\n",
    "            entity_type = \"[代詞]\"\n",
    "        elif any(char.isdigit() for char in entity) and any(char.isalpha() for char in entity):\n",
    "            entity_type = \"[數值+單位]\"\n",
    "        else:\n",
    "            entity_type = \"[正常]\"\n",
    "        \n",
    "        print(f\"  {idx:2d}. {entity_type:12s} {entity[:40]:40s} (提及: {mentions}, Chunk: {chunk_id})\")\n",
    "    \n",
    "    # 2. 孤立實體類型統計\n",
    "    print(f\"\\n📊 孤立實體類型分析：\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    isolated_stats = session.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-()\n",
    "        OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(e)\n",
    "        WITH e, count(DISTINCT c) AS mentions\n",
    "        RETURN \n",
    "            count(e) AS total_isolated,\n",
    "            sum(CASE WHEN mentions = 0 THEN 1 ELSE 0 END) AS no_mentions,\n",
    "            sum(CASE WHEN mentions = 1 THEN 1 ELSE 0 END) AS single_mention,\n",
    "            sum(CASE WHEN mentions >= 2 THEN 1 ELSE 0 END) AS multiple_mentions,\n",
    "            sum(CASE WHEN size(e.name) < 3 THEN 1 ELSE 0 END) AS too_short,\n",
    "            sum(CASE WHEN size(e.name) > 40 THEN 1 ELSE 0 END) AS too_long\n",
    "    \"\"\").single()\n",
    "    \n",
    "    print(f\"  • 總孤立實體數：{isolated_stats['total_isolated']}\")\n",
    "    print(f\"  • 無 MENTIONS 連接：{isolated_stats['no_mentions']} ({isolated_stats['no_mentions']/isolated_stats['total_isolated']*100:.1f}%)\")\n",
    "    print(f\"  • 單一 Chunk 提及：{isolated_stats['single_mention']} ({isolated_stats['single_mention']/isolated_stats['total_isolated']*100:.1f}%)\")\n",
    "    print(f\"  • 多 Chunk 提及：{isolated_stats['multiple_mentions']} ({isolated_stats['multiple_mentions']/isolated_stats['total_isolated']*100:.1f}%)\")\n",
    "    print(f\"  • 名稱過短（<3字元）：{isolated_stats['too_short']} ({isolated_stats['too_short']/isolated_stats['total_isolated']*100:.1f}%)\")\n",
    "    print(f\"  • 名稱過長（>40字元）：{isolated_stats['too_long']} ({isolated_stats['too_long']/isolated_stats['total_isolated']*100:.1f}%)\")\n",
    "    \n",
    "    # 3. 潛在的同義詞檢測\n",
    "    print(f\"\\n🔗 潛在同義詞檢測（相似實體名稱）：\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    potential_synonyms = session.run(\"\"\"\n",
    "        MATCH (e1:Entity)\n",
    "        WHERE NOT (e1)-[:RELATION]-()\n",
    "        MATCH (e2:Entity)\n",
    "        WHERE e2 <> e1 AND (e2)-[:RELATION]-()\n",
    "        AND (\n",
    "            toLower(e1.name) CONTAINS toLower(e2.name) \n",
    "            OR toLower(e2.name) CONTAINS toLower(e1.name)\n",
    "            OR toLower(replace(e1.name, '_', ' ')) = toLower(replace(e2.name, '_', ' '))\n",
    "        )\n",
    "        RETURN \n",
    "            e1.name AS isolated_entity,\n",
    "            e2.name AS connected_entity,\n",
    "            COUNT { (e2)-[:RELATION]-() } AS connected_degree\n",
    "        ORDER BY connected_degree DESC\n",
    "        LIMIT 15\n",
    "    \"\"\").data()\n",
    "    \n",
    "    if potential_synonyms:\n",
    "        print(f\"  發現 {len(potential_synonyms)} 對潛在同義詞：\")\n",
    "        for syn in potential_synonyms:\n",
    "            print(f\"    • 孤立: '{syn['isolated_entity'][:30]}' ↔ 已連接: '{syn['connected_entity'][:30]}' (度數: {syn['connected_degree']})\")\n",
    "    else:\n",
    "        print(\"  未發現明顯的同義詞模式\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"💡 診斷建議：\")\n",
    "print(\"  1. 過短實體、純數字實體 → 建議刪除（可能是提取錯誤）\")\n",
    "print(\"  2. 單一提及且名稱不常見 → 可能是低質量實體，考慮刪除\")\n",
    "print(\"  3. 多次提及但孤立 → 關係提取失敗，需要重新提取關係\")\n",
    "print(\"  4. 發現同義詞 → 需要實體正規化與合併\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 全局關係推理函數已載入\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 🧠 全局關係推理（解決弱連接問題的核心策略）\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def InferGlobalRelations(\n",
    "    driver,\n",
    "    client: Client,\n",
    "    model: str,\n",
    "    dataset_id: str,\n",
    "    max_inference_per_entity: int = 5,\n",
    "    min_entity_degree: int = 1,\n",
    "    max_entity_degree: int = 3,\n",
    "    temperature: float = 0.0,\n",
    "    batch_size: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    全局關係推理：針對弱連接實體（度數 1-3），基於其鄰居和全局上下文推理潛在關係\n",
    "    \n",
    "    策略：\n",
    "    1. 選擇弱連接實體（度數 1-3）\n",
    "    2. 收集其鄰居實體和關係上下文\n",
    "    3. 使用 LLM 推理該實體可能與哪些其他實體相關\n",
    "    4. 僅連接到已存在的實體（MATCH + MERGE）\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🧠 全局關係推理開始...\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"目標：針對弱連接實體（度數 {min_entity_degree}-{max_entity_degree}）推理新關係\")\n",
    "    print(f\"推理模型：{model}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 推理提示詞模板\n",
    "    INFERENCE_PROMPT = \"\"\"You are a knowledge graph reasoning expert. Given an entity and its current connections, infer additional relationships this entity might have with other entities in the domain.\n",
    "\n",
    "Current Entity: {entity}\n",
    "\n",
    "Current Relationships:\n",
    "{current_relations}\n",
    "\n",
    "Available Entities in Graph (sample):\n",
    "{entity_sample}\n",
    "\n",
    "Task: Based on domain knowledge and the context provided, infer up to {max_infer} additional relationships this entity should have with other entities from the available list.\n",
    "\n",
    "Rules:\n",
    "1. Only suggest relationships with entities from the \"Available Entities\" list\n",
    "2. Provide specific, meaningful relationship types (not vague like \"related\" or \"associated\")\n",
    "3. Consider transitive relationships (if A→B and B→C, maybe A→C makes sense)\n",
    "4. Consider taxonomic relationships (if entity is a type of X, connect to X)\n",
    "5. Consider functional relationships (if entity uses/requires/produces something)\n",
    "\n",
    "Output Format (JSON array):\n",
    "[\n",
    "  {{\"head\":\"{entity}\", \"relation\":\"relationship_type\", \"tail\":\"target_entity\", \"reasoning\":\"brief explanation\"}},\n",
    "  ...\n",
    "]\n",
    "\n",
    "If no meaningful relationships can be inferred, return an empty array [].\n",
    "\"\"\"\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # 階段 1：獲取弱連接實體\n",
    "        print(f\"\\n📊 階段 1：識別弱連接實體...\")\n",
    "        \n",
    "        weak_entities = session.run(f\"\"\"\n",
    "            MATCH (e:Entity)-[r:RELATION]-()\n",
    "            WITH e, count(r) AS degree\n",
    "            WHERE degree >= {min_entity_degree} AND degree <= {max_entity_degree}\n",
    "            RETURN e.name AS entity, degree\n",
    "            ORDER BY degree ASC\n",
    "            LIMIT 10000\n",
    "        \"\"\").data()\n",
    "    # LIMIT modify\n",
    "        print(f\"  找到 {len(weak_entities)} 個弱連接實體（度數 {min_entity_degree}-{max_entity_degree}）\")\n",
    "        \n",
    "        if not weak_entities:\n",
    "            print(\"  ⚠️  沒有符合條件的實體\")\n",
    "            return {\n",
    "                'processed_entities': 0,\n",
    "                'inferred_relations': 0,\n",
    "                'density_before': 0,\n",
    "                'density_after': 0\n",
    "            }\n",
    "        \n",
    "        # 獲取全局實體樣本（用於推理）\n",
    "        all_entities = session.run(\"\"\"\n",
    "            MATCH (e:Entity)-[r:RELATION]-()\n",
    "            WITH e, count(r) AS degree\n",
    "            WHERE degree >= 3\n",
    "            RETURN e.name AS entity\n",
    "            ORDER BY degree DESC\n",
    "            LIMIT 4000\n",
    "        \"\"\").data()\n",
    "        entity_sample_list = [e['entity'] for e in all_entities]\n",
    "        \n",
    "        # 記錄初始密度\n",
    "        initial_stats = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WITH count(e) AS entities\n",
    "            MATCH ()-[r:RELATION]->()\n",
    "            RETURN entities, count(r) AS relations, \n",
    "                   toFloat(count(r)) / entities AS density\n",
    "        \"\"\").single()\n",
    "        \n",
    "        density_before = initial_stats['density']\n",
    "        \n",
    "        # 階段 2：逐個實體進行關係推理\n",
    "        print(f\"\\n🔄 階段 2：對弱連接實體進行關係推理...\")\n",
    "        \n",
    "        total_inferred = 0\n",
    "        processed_count = 0\n",
    "        \n",
    "        for idx, entity_data in enumerate(weak_entities):\n",
    "            entity = entity_data['entity']\n",
    "            degree = entity_data['degree']\n",
    "            \n",
    "            # 獲取當前關係上下文\n",
    "            current_relations = session.run(\"\"\"\n",
    "                MATCH (e:Entity {name: $entity})-[r:RELATION]-(neighbor)\n",
    "                RETURN \n",
    "                    type(r) AS relation_type,\n",
    "                    CASE WHEN startNode(r) = e THEN neighbor.name ELSE null END AS tail,\n",
    "                    CASE WHEN endNode(r) = e THEN neighbor.name ELSE null END AS head\n",
    "                LIMIT 10\n",
    "            \"\"\", entity=entity).data()\n",
    "            \n",
    "            relations_text = \"\\n\".join([\n",
    "                f\"  • {rel['head'] or entity} --[{rel['relation_type']}]--> {rel['tail'] or entity}\"\n",
    "                for rel in current_relations\n",
    "            ])\n",
    "            \n",
    "            entity_sample_text = \", \".join(entity_sample_list[:50])\n",
    "            \n",
    "            # 構建推理提示\n",
    "            prompt = INFERENCE_PROMPT.format(\n",
    "                entity=entity,\n",
    "                current_relations=relations_text or \"  (No current relations)\",\n",
    "                entity_sample=entity_sample_text,\n",
    "                max_infer=max_inference_per_entity\n",
    "            )\n",
    "            \n",
    "            # 調用 LLM 進行推理\n",
    "            try:\n",
    "                response = client.chat(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    options={\"temperature\": temperature, \"top_p\": 0.9},\n",
    "                )\n",
    "                content = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "                \n",
    "                # 解析推理結果\n",
    "                inferred_triples = parse_triples(content)\n",
    "                \n",
    "                # 寫入推理的關係\n",
    "                for triple in inferred_triples:\n",
    "                    head = triple.get(\"head\")\n",
    "                    relation = triple.get(\"relation\")\n",
    "                    tail = triple.get(\"tail\")\n",
    "                    \n",
    "                    if not all([head, relation, tail]):\n",
    "                        continue\n",
    "                    \n",
    "                    # 只連接已存在的實體\n",
    "                    result = session.run(\"\"\"\n",
    "                        MATCH (h:Entity {name: $head})\n",
    "                        MATCH (t:Entity {name: $tail})\n",
    "                        MERGE (h)-[r:RELATION {type: $relation}]->(t)\n",
    "                        ON CREATE SET r.inferred = true, r.chunks = []\n",
    "                        RETURN count(r) AS created\n",
    "                    \"\"\", head=head, relation=relation, tail=tail).single()\n",
    "                    \n",
    "                    if result and result['created'] > 0:\n",
    "                        total_inferred += 1\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "                if (idx + 1) % batch_size == 0:\n",
    "                    print(f\"  ↳ 已處理 {processed_count}/{len(weak_entities)} 個實體，推理出 {total_inferred} 個新關係\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️  實體 '{entity}' 推理失敗：{e}\")\n",
    "                continue\n",
    "        \n",
    "        # 記錄最終密度\n",
    "        final_stats = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WITH count(e) AS entities\n",
    "            MATCH ()-[r:RELATION]->()\n",
    "            RETURN entities, count(r) AS relations, \n",
    "                   toFloat(count(r)) / entities AS density\n",
    "        \"\"\").single()\n",
    "        \n",
    "        density_after = final_stats['density']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"✅ 全局關係推理完成\")\n",
    "        print(f\"  • 處理實體數：{processed_count}\")\n",
    "        print(f\"  • 推理新關係：{total_inferred}\")\n",
    "        print(f\"  • 密度變化：{density_before:.3f} → {density_after:.3f} (+{density_after-density_before:.3f})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        return {\n",
    "            'processed_entities': processed_count,\n",
    "            'inferred_relations': total_inferred,\n",
    "            'density_before': density_before,\n",
    "            'density_after': density_after\n",
    "        }\n",
    "\n",
    "print(\"✅ 全局關係推理函數已載入\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47532ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 假設性問題關係密集化函數已載入\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 💡 假設性問題關係抽取（Hypothetical Questions Method）\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "HYPOTHETICAL_QUESTIONS_PROMPT = \"\"\"You are a knowledge graph construction expert. Given a text chunk and a set of entities, generate relationships by answering hypothetical questions about how these entities interact.\n",
    "\n",
    "Text Chunk:\n",
    "{chunk_text}\n",
    "\n",
    "Entities in this chunk:\n",
    "{entity_list}\n",
    "\n",
    "Task: For each pair of entities, ask yourself these questions and extract relationships if the text provides evidence:\n",
    "\n",
    "1. **Causal Questions**: Does X cause/lead to/result in Y? Does X prevent/inhibit Y?\n",
    "2. **Compositional Questions**: Does X contain/include/consist of Y? Is Y a part/component of X?\n",
    "3. **Functional Questions**: Does X use/require/depend on Y? Does X produce/generate Y?\n",
    "4. **Hierarchical Questions**: Is X a type of Y? Does X belong to category Y?\n",
    "5. **Comparative Questions**: How does X compare to Y? Is X similar to/different from Y?\n",
    "6. **Temporal Questions**: Does X happen before/after/during Y?\n",
    "7. **Spatial Questions**: Where is X located relative to Y?\n",
    "8. **Attribute Questions**: What properties/characteristics does X have? What is the value of X's attribute Y?\n",
    "\n",
    "Rules:\n",
    "1. Only extract relationships explicitly supported by the text\n",
    "2. Use specific relationship types (not generic like \"related\" or \"associated\")\n",
    "3. Extract as many valid relationships as possible between entities\n",
    "4. If entities A and B both relate to C, also consider if A and B might relate to each other\n",
    "\n",
    "Output Format (JSON array):\n",
    "[\n",
    "  {{\"head\":\"entity1\", \"relation\":\"relationship_type\", \"tail\":\"entity2\"}},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Focus on maximizing the number of valid connections between entities to create a dense knowledge network.\n",
    "\"\"\"\n",
    "\n",
    "def DensifyRelationsWithQuestions(\n",
    "    driver,\n",
    "    client: Client,\n",
    "    model: str,\n",
    "    dataset_id: str,\n",
    "    target_chunks: int = 300,\n",
    "    temperature: float = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    使用假設性問題法重新抽取關係，提升圖譜密度\n",
    "    \n",
    "    策略：\n",
    "    1. 選擇關係密度較低的 Chunks\n",
    "    2. 對每個 Chunk，列出其包含的實體\n",
    "    3. 使用假設性問題引導 LLM 發現更多實體間的關係\n",
    "    4. 只連接已存在的實體（MATCH + MERGE）\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"💡 假設性問題關係密集化開始...\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"目標 Chunks 數：{target_chunks}\")\n",
    "    print(f\"抽取模型：{model}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # 階段 1：選擇低密度 Chunks\n",
    "        print(f\"\\n📊 階段 1：選擇低密度 Chunks...\")\n",
    "        \n",
    "        low_density_chunks = session.run(f\"\"\"\n",
    "            MATCH (c:Chunk {{dataset: $dataset}})-[:MENTIONS]->(e:Entity)\n",
    "            WITH c, collect(DISTINCT e.name) AS entities, count(DISTINCT e) AS entity_count\n",
    "            WHERE entity_count >= 3\n",
    "            \n",
    "            // 計算該 Chunk 中實體間的關係數\n",
    "            MATCH (c)-[:MENTIONS]->(e1:Entity)\n",
    "            MATCH (c)-[:MENTIONS]->(e2:Entity)\n",
    "            WHERE e1 <> e2\n",
    "            OPTIONAL MATCH (e1)-[r:RELATION]-(e2)\n",
    "            WITH c, entities, entity_count, \n",
    "                 count(DISTINCT r) AS relation_count,\n",
    "                 toFloat(count(DISTINCT r)) / (entity_count * (entity_count - 1) / 2) AS chunk_density\n",
    "            \n",
    "            WHERE chunk_density < 0.3  // 密度低於 30%\n",
    "            \n",
    "            RETURN c.id AS chunk_id, c.text AS chunk_text, \n",
    "                   entities, entity_count, relation_count, chunk_density\n",
    "            ORDER BY entity_count DESC, chunk_density ASC\n",
    "            LIMIT {target_chunks}\n",
    "        \"\"\", dataset=dataset_id).data()\n",
    "        \n",
    "        print(f\"  找到 {len(low_density_chunks)} 個低密度 Chunks\")\n",
    "        \n",
    "        if not low_density_chunks:\n",
    "            print(\"  ✅ 所有 Chunks 密度已達標\")\n",
    "            return {\n",
    "                'processed_chunks': 0,\n",
    "                'new_relations': 0,\n",
    "                'density_before': 0,\n",
    "                'density_after': 0\n",
    "            }\n",
    "        \n",
    "        # 記錄初始狀態\n",
    "        initial_stats = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WITH count(e) AS entities\n",
    "            MATCH ()-[r:RELATION]->()\n",
    "            RETURN entities, count(r) AS relations, \n",
    "                   toFloat(count(r)) / entities AS density\n",
    "        \"\"\").single()\n",
    "        \n",
    "        density_before = initial_stats['density']\n",
    "        \n",
    "        # 階段 2：對每個 Chunk 進行密集化抽取\n",
    "        print(f\"\\n🔄 階段 2：使用假設性問題法重新抽取關係...\")\n",
    "        \n",
    "        total_new_relations = 0\n",
    "        processed_count = 0\n",
    "        \n",
    "        for idx, chunk_data in enumerate(low_density_chunks):\n",
    "            chunk_id = chunk_data['chunk_id']\n",
    "            chunk_text = chunk_data['chunk_text']\n",
    "            entities = chunk_data['entities']\n",
    "            current_density = chunk_data['chunk_density']\n",
    "            \n",
    "            # 格式化實體列表\n",
    "            entity_list_text = \", \".join(entities)\n",
    "            \n",
    "            # 構建提示\n",
    "            prompt = HYPOTHETICAL_QUESTIONS_PROMPT.format(\n",
    "                chunk_text=chunk_text[:2000],  # 限制長度\n",
    "                entity_list=entity_list_text\n",
    "            )\n",
    "            \n",
    "            # 調用 LLM\n",
    "            try:\n",
    "                response = client.chat(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    options={\"temperature\": temperature, \"top_p\": 0.9},\n",
    "                )\n",
    "                content = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "                \n",
    "                # 解析三元組\n",
    "                triples = parse_triples(content)\n",
    "                \n",
    "                # 寫入新關係\n",
    "                for triple in triples:\n",
    "                    head = triple.get(\"head\")\n",
    "                    relation = triple.get(\"relation\")\n",
    "                    tail = triple.get(\"tail\")\n",
    "                    \n",
    "                    if not all([head, relation, tail]):\n",
    "                        continue\n",
    "                    \n",
    "                    # 只連接已存在的實體\n",
    "                    result = session.run(\"\"\"\n",
    "                        MATCH (h:Entity {name: $head})\n",
    "                        MATCH (t:Entity {name: $tail})\n",
    "                        MERGE (h)-[r:RELATION {type: $relation}]->(t)\n",
    "                        ON CREATE SET r.chunks = [$chunk_id], r.densified = true\n",
    "                        ON MATCH SET r.chunks = CASE \n",
    "                            WHEN NOT $chunk_id IN r.chunks \n",
    "                            THEN r.chunks + [$chunk_id]\n",
    "                            ELSE r.chunks\n",
    "                        END\n",
    "                        RETURN count(r) AS created\n",
    "                    \"\"\", head=head, relation=relation, tail=tail, chunk_id=chunk_id).single()\n",
    "                    \n",
    "                    if result:\n",
    "                        total_new_relations += result['created']\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "                if (idx + 1) % 10 == 0:\n",
    "                    print(f\"  ↳ 已處理 {processed_count}/{len(low_density_chunks)} Chunks，新增 {total_new_relations} 個關係\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️  Chunk {chunk_id} 處理失敗：{e}\")\n",
    "                continue\n",
    "        \n",
    "        # 記錄最終狀態\n",
    "        final_stats = session.run(\"\"\"\n",
    "            MATCH (e:Entity)\n",
    "            WITH count(e) AS entities\n",
    "            MATCH ()-[r:RELATION]->()\n",
    "            RETURN entities, count(r) AS relations, \n",
    "                   toFloat(count(r)) / entities AS density\n",
    "        \"\"\").single()\n",
    "        \n",
    "        density_after = final_stats['density']\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"✅ 假設性問題關係密集化完成\")\n",
    "        print(f\"  • 處理 Chunks：{processed_count}\")\n",
    "        print(f\"  • 新增關係：{total_new_relations}\")\n",
    "        print(f\"  • 密度變化：{density_before:.3f} → {density_after:.3f} (+{density_after-density_before:.3f})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        return {\n",
    "            'processed_chunks': processed_count,\n",
    "            'new_relations': total_new_relations,\n",
    "            'density_before': density_before,\n",
    "            'density_after': density_after\n",
    "        }\n",
    "\n",
    "print(\"✅ 假設性問題關係密集化函數已載入\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cc09a",
   "metadata": {},
   "source": [
    "## ⚙️ 執行星型拓撲優化流程\n",
    "\n",
    "現在按照優先級執行完整優化：\n",
    "\n",
    "**🔴 階段1：關係密集化（針對弱連接實體 67.8%）**\n",
    "- 策略1：假設性問題法 - 對低密度Chunks重新抽取關係\n",
    "- 策略2：全局關係推理 - 使用LLM為弱實體推理新關係\n",
    "\n",
    "**目標：**\n",
    "- 弱連接實體：67.8% → <50%\n",
    "- 密度：1.265 → 1.7+\n",
    "- 平均度數：2.53 → 3.5+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1264790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔴 階段1：假設性問題法關係密集化\n",
      "======================================================================\n",
      "💡 假設性問題關係密集化開始...\n",
      "======================================================================\n",
      "目標 Chunks 數：5000\n",
      "抽取模型：deepseek-r1:8b-llama-distill-q4_K_M\n",
      "======================================================================\n",
      "\n",
      "📊 階段 1：選擇低密度 Chunks...\n",
      "  找到 0 個低密度 Chunks\n",
      "  ✅ 所有 Chunks 密度已達標\n",
      "\n",
      "📊 階段1完成摘要：\n",
      "  • 處理Chunks：0\n",
      "  • 新增關係：0\n",
      "  • 密度提升：0.000 → 0.000\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 🚀 執行階段1：假設性問題關係密集化\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔴 階段1：假設性問題法關係密集化\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 執行假設性問題法，針對低密度Chunks\n",
    "DENSIFY_RESULTS_1 = DensifyRelationsWithQuestions(\n",
    "    driver=GRAPH_DRIVER,\n",
    "    client=OLLAMA_CLIENT,\n",
    "    model=GRAPH_CREATE_MODEL,\n",
    "    dataset_id=DATASET_ID,\n",
    "    target_chunks=5000,  # 選擇50個低密度Chunks\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 階段1完成摘要：\")\n",
    "print(f\"  • 處理Chunks：{DENSIFY_RESULTS_1['processed_chunks']}\")\n",
    "print(f\"  • 新增關係：{DENSIFY_RESULTS_1['new_relations']}\")\n",
    "print(f\"  • 密度提升：{DENSIFY_RESULTS_1['density_before']:.3f} → {DENSIFY_RESULTS_1['density_after']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac697e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔴 階段2：全局關係推理（針對弱連接實體）\n",
      "======================================================================\n",
      "🧠 全局關係推理開始...\n",
      "======================================================================\n",
      "目標：針對弱連接實體（度數 1-3）推理新關係\n",
      "推理模型：deepseek-r1:8b-llama-distill-q4_K_M\n",
      "======================================================================\n",
      "\n",
      "📊 階段 1：識別弱連接實體...\n",
      "  找到 5957 個弱連接實體（度數 1-3）\n",
      "\n",
      "🔄 階段 2：對弱連接實體進行關係推理...\n",
      "  ↳ 已處理 10/5957 個實體，推理出 54 個新關係\n",
      "  ↳ 已處理 20/5957 個實體，推理出 103 個新關係\n",
      "  ↳ 已處理 30/5957 個實體，推理出 152 個新關係\n",
      "  ↳ 已處理 40/5957 個實體，推理出 200 個新關係\n",
      "  ↳ 已處理 50/5957 個實體，推理出 246 個新關係\n",
      "  ↳ 已處理 60/5957 個實體，推理出 302 個新關係\n",
      "  ↳ 已處理 70/5957 個實體，推理出 348 個新關係\n",
      "  ↳ 已處理 80/5957 個實體，推理出 398 個新關係\n",
      "  ↳ 已處理 90/5957 個實體，推理出 443 個新關係\n",
      "  ↳ 已處理 100/5957 個實體，推理出 505 個新關係\n",
      "  ↳ 已處理 110/5957 個實體，推理出 550 個新關係\n",
      "  ↳ 已處理 120/5957 個實體，推理出 591 個新關係\n",
      "  ↳ 已處理 130/5957 個實體，推理出 634 個新關係\n",
      "  ↳ 已處理 140/5957 個實體，推理出 680 個新關係\n",
      "  ↳ 已處理 150/5957 個實體，推理出 727 個新關係\n",
      "  ↳ 已處理 160/5957 個實體，推理出 772 個新關係\n",
      "  ↳ 已處理 170/5957 個實體，推理出 844 個新關係\n",
      "  ↳ 已處理 180/5957 個實體，推理出 887 個新關係\n",
      "  ↳ 已處理 190/5957 個實體，推理出 932 個新關係\n",
      "  ↳ 已處理 200/5957 個實體，推理出 977 個新關係\n",
      "  ↳ 已處理 210/5957 個實體，推理出 1021 個新關係\n",
      "  ↳ 已處理 220/5957 個實體，推理出 1081 個新關係\n",
      "  ↳ 已處理 230/5957 個實體，推理出 1151 個新關係\n",
      "  ↳ 已處理 240/5957 個實體，推理出 1201 個新關係\n",
      "  ↳ 已處理 250/5957 個實體，推理出 1245 個新關係\n",
      "  ↳ 已處理 260/5957 個實體，推理出 1306 個新關係\n",
      "  ↳ 已處理 270/5957 個實體，推理出 1359 個新關係\n",
      "  ↳ 已處理 280/5957 個實體，推理出 1405 個新關係\n",
      "  ↳ 已處理 290/5957 個實體，推理出 1449 個新關係\n",
      "  ↳ 已處理 300/5957 個實體，推理出 1492 個新關係\n",
      "  ↳ 已處理 310/5957 個實體，推理出 1539 個新關係\n",
      "  ↳ 已處理 320/5957 個實體，推理出 1585 個新關係\n",
      "  ↳ 已處理 330/5957 個實體，推理出 1629 個新關係\n",
      "  ↳ 已處理 340/5957 個實體，推理出 1678 個新關係\n",
      "  ↳ 已處理 350/5957 個實體，推理出 1728 個新關係\n",
      "  ↳ 已處理 360/5957 個實體，推理出 1769 個新關係\n",
      "  ↳ 已處理 370/5957 個實體，推理出 1816 個新關係\n",
      "  ↳ 已處理 380/5957 個實體，推理出 1854 個新關係\n",
      "  ↳ 已處理 390/5957 個實體，推理出 1897 個新關係\n",
      "  ↳ 已處理 400/5957 個實體，推理出 1943 個新關係\n",
      "  ↳ 已處理 410/5957 個實體，推理出 1978 個新關係\n",
      "  ↳ 已處理 420/5957 個實體，推理出 2012 個新關係\n",
      "  ↳ 已處理 430/5957 個實體，推理出 2059 個新關係\n",
      "  ↳ 已處理 440/5957 個實體，推理出 2103 個新關係\n",
      "  ↳ 已處理 450/5957 個實體，推理出 2152 個新關係\n",
      "  ↳ 已處理 460/5957 個實體，推理出 2195 個新關係\n",
      "  ↳ 已處理 470/5957 個實體，推理出 2241 個新關係\n",
      "  ↳ 已處理 480/5957 個實體，推理出 2283 個新關係\n",
      "  ↳ 已處理 490/5957 個實體，推理出 2343 個新關係\n",
      "  ↳ 已處理 500/5957 個實體，推理出 2370 個新關係\n",
      "  ↳ 已處理 510/5957 個實體，推理出 2418 個新關係\n",
      "  ↳ 已處理 520/5957 個實體，推理出 2474 個新關係\n",
      "  ↳ 已處理 530/5957 個實體，推理出 2527 個新關係\n",
      "  ↳ 已處理 540/5957 個實體，推理出 2577 個新關係\n",
      "  ↳ 已處理 550/5957 個實體，推理出 2629 個新關係\n",
      "  ↳ 已處理 560/5957 個實體，推理出 2672 個新關係\n",
      "  ↳ 已處理 570/5957 個實體，推理出 2725 個新關係\n",
      "  ↳ 已處理 580/5957 個實體，推理出 2766 個新關係\n",
      "  ↳ 已處理 590/5957 個實體，推理出 2820 個新關係\n",
      "  ↳ 已處理 600/5957 個實體，推理出 2862 個新關係\n",
      "  ↳ 已處理 610/5957 個實體，推理出 2905 個新關係\n",
      "  ↳ 已處理 620/5957 個實體，推理出 2950 個新關係\n",
      "  ↳ 已處理 630/5957 個實體，推理出 2999 個新關係\n",
      "  ↳ 已處理 640/5957 個實體，推理出 3044 個新關係\n",
      "  ↳ 已處理 650/5957 個實體，推理出 3090 個新關係\n",
      "  ↳ 已處理 660/5957 個實體，推理出 3140 個新關係\n",
      "  ↳ 已處理 670/5957 個實體，推理出 3205 個新關係\n",
      "  ↳ 已處理 680/5957 個實體，推理出 3253 個新關係\n",
      "  ↳ 已處理 690/5957 個實體，推理出 3299 個新關係\n",
      "  ↳ 已處理 700/5957 個實體，推理出 3337 個新關係\n",
      "  ↳ 已處理 710/5957 個實體，推理出 3392 個新關係\n",
      "  ↳ 已處理 720/5957 個實體，推理出 3432 個新關係\n",
      "  ↳ 已處理 730/5957 個實體，推理出 3477 個新關係\n",
      "  ↳ 已處理 740/5957 個實體，推理出 3534 個新關係\n",
      "  ↳ 已處理 750/5957 個實體，推理出 3578 個新關係\n",
      "  ↳ 已處理 760/5957 個實體，推理出 3622 個新關係\n",
      "  ↳ 已處理 770/5957 個實體，推理出 3666 個新關係\n",
      "  ↳ 已處理 780/5957 個實體，推理出 3713 個新關係\n",
      "  ↳ 已處理 790/5957 個實體，推理出 3765 個新關係\n",
      "  ↳ 已處理 800/5957 個實體，推理出 3815 個新關係\n",
      "  ↳ 已處理 810/5957 個實體，推理出 3861 個新關係\n",
      "  ↳ 已處理 820/5957 個實體，推理出 3901 個新關係\n",
      "  ↳ 已處理 830/5957 個實體，推理出 3948 個新關係\n",
      "  ↳ 已處理 840/5957 個實體，推理出 3986 個新關係\n",
      "  ↳ 已處理 850/5957 個實體，推理出 4028 個新關係\n",
      "  ↳ 已處理 860/5957 個實體，推理出 4076 個新關係\n",
      "  ↳ 已處理 870/5957 個實體，推理出 4114 個新關係\n",
      "  ↳ 已處理 880/5957 個實體，推理出 4160 個新關係\n",
      "  ↳ 已處理 890/5957 個實體，推理出 4207 個新關係\n",
      "  ↳ 已處理 900/5957 個實體，推理出 4258 個新關係\n",
      "  ↳ 已處理 910/5957 個實體，推理出 4300 個新關係\n",
      "  ↳ 已處理 920/5957 個實體，推理出 4361 個新關係\n",
      "  ↳ 已處理 930/5957 個實體，推理出 4407 個新關係\n",
      "  ↳ 已處理 940/5957 個實體，推理出 4459 個新關係\n",
      "  ↳ 已處理 950/5957 個實體，推理出 4504 個新關係\n",
      "  ↳ 已處理 960/5957 個實體，推理出 4559 個新關係\n",
      "  ↳ 已處理 970/5957 個實體，推理出 4607 個新關係\n",
      "  ↳ 已處理 980/5957 個實體，推理出 4651 個新關係\n",
      "  ↳ 已處理 990/5957 個實體，推理出 4696 個新關係\n",
      "  ↳ 已處理 1000/5957 個實體，推理出 4743 個新關係\n",
      "  ↳ 已處理 1010/5957 個實體，推理出 4788 個新關係\n",
      "  ↳ 已處理 1020/5957 個實體，推理出 4837 個新關係\n",
      "  ↳ 已處理 1030/5957 個實體，推理出 4879 個新關係\n",
      "  ↳ 已處理 1040/5957 個實體，推理出 4925 個新關係\n",
      "  ↳ 已處理 1050/5957 個實體，推理出 4961 個新關係\n",
      "  ↳ 已處理 1060/5957 個實體，推理出 5010 個新關係\n",
      "  ↳ 已處理 1070/5957 個實體，推理出 5053 個新關係\n",
      "  ↳ 已處理 1080/5957 個實體，推理出 5100 個新關係\n",
      "  ↳ 已處理 1090/5957 個實體，推理出 5152 個新關係\n",
      "  ↳ 已處理 1100/5957 個實體，推理出 5199 個新關係\n",
      "  ↳ 已處理 1110/5957 個實體，推理出 5248 個新關係\n",
      "  ↳ 已處理 1120/5957 個實體，推理出 5295 個新關係\n",
      "  ↳ 已處理 1130/5957 個實體，推理出 5348 個新關係\n",
      "  ↳ 已處理 1140/5957 個實體，推理出 5390 個新關係\n",
      "  ↳ 已處理 1150/5957 個實體，推理出 5444 個新關係\n",
      "  ↳ 已處理 1160/5957 個實體，推理出 5496 個新關係\n",
      "  ↳ 已處理 1170/5957 個實體，推理出 5542 個新關係\n",
      "  ↳ 已處理 1180/5957 個實體，推理出 5574 個新關係\n",
      "  ↳ 已處理 1190/5957 個實體，推理出 5620 個新關係\n",
      "  ↳ 已處理 1200/5957 個實體，推理出 5668 個新關係\n",
      "  ↳ 已處理 1210/5957 個實體，推理出 5710 個新關係\n",
      "  ↳ 已處理 1220/5957 個實體，推理出 5751 個新關係\n",
      "  ↳ 已處理 1230/5957 個實體，推理出 5795 個新關係\n",
      "  ↳ 已處理 1240/5957 個實體，推理出 5838 個新關係\n",
      "  ↳ 已處理 1250/5957 個實體，推理出 5876 個新關係\n",
      "  ↳ 已處理 1260/5957 個實體，推理出 5933 個新關係\n",
      "  ↳ 已處理 1270/5957 個實體，推理出 5977 個新關係\n",
      "  ↳ 已處理 1280/5957 個實體，推理出 6024 個新關係\n",
      "  ↳ 已處理 1290/5957 個實體，推理出 6075 個新關係\n",
      "  ↳ 已處理 1300/5957 個實體，推理出 6115 個新關係\n",
      "  ↳ 已處理 1310/5957 個實體，推理出 6155 個新關係\n",
      "  ↳ 已處理 1320/5957 個實體，推理出 6191 個新關係\n",
      "  ↳ 已處理 1330/5957 個實體，推理出 6231 個新關係\n",
      "  ↳ 已處理 1340/5957 個實體，推理出 6270 個新關係\n",
      "  ↳ 已處理 1350/5957 個實體，推理出 6325 個新關係\n",
      "  ↳ 已處理 1360/5957 個實體，推理出 6370 個新關係\n",
      "  ↳ 已處理 1370/5957 個實體，推理出 6416 個新關係\n",
      "  ↳ 已處理 1380/5957 個實體，推理出 6464 個新關係\n",
      "  ↳ 已處理 1390/5957 個實體，推理出 6513 個新關係\n",
      "  ↳ 已處理 1400/5957 個實體，推理出 6559 個新關係\n",
      "  ↳ 已處理 1410/5957 個實體，推理出 6610 個新關係\n",
      "  ↳ 已處理 1420/5957 個實體，推理出 6663 個新關係\n",
      "  ↳ 已處理 1430/5957 個實體，推理出 6707 個新關係\n",
      "  ↳ 已處理 1440/5957 個實體，推理出 6766 個新關係\n",
      "  ↳ 已處理 1450/5957 個實體，推理出 6813 個新關係\n",
      "  ↳ 已處理 1460/5957 個實體，推理出 6872 個新關係\n",
      "  ↳ 已處理 1470/5957 個實體，推理出 6919 個新關係\n",
      "  ↳ 已處理 1480/5957 個實體，推理出 6968 個新關係\n",
      "  ↳ 已處理 1490/5957 個實體，推理出 7018 個新關係\n",
      "  ↳ 已處理 1500/5957 個實體，推理出 7066 個新關係\n",
      "  ↳ 已處理 1510/5957 個實體，推理出 7115 個新關係\n",
      "  ↳ 已處理 1520/5957 個實體，推理出 7162 個新關係\n",
      "  ↳ 已處理 1530/5957 個實體，推理出 7210 個新關係\n",
      "  ↳ 已處理 1540/5957 個實體，推理出 7278 個新關係\n",
      "  ↳ 已處理 1550/5957 個實體，推理出 7331 個新關係\n",
      "  ↳ 已處理 1560/5957 個實體，推理出 7392 個新關係\n",
      "  ↳ 已處理 1570/5957 個實體，推理出 7445 個新關係\n",
      "  ↳ 已處理 1580/5957 個實體，推理出 7495 個新關係\n",
      "  ↳ 已處理 1590/5957 個實體，推理出 7542 個新關係\n",
      "  ↳ 已處理 1600/5957 個實體，推理出 7588 個新關係\n",
      "  ↳ 已處理 1610/5957 個實體，推理出 7638 個新關係\n",
      "  ↳ 已處理 1620/5957 個實體，推理出 7681 個新關係\n",
      "  ↳ 已處理 1630/5957 個實體，推理出 7738 個新關係\n",
      "  ↳ 已處理 1640/5957 個實體，推理出 7785 個新關係\n",
      "  ↳ 已處理 1650/5957 個實體，推理出 7845 個新關係\n",
      "  ↳ 已處理 1660/5957 個實體，推理出 7905 個新關係\n",
      "  ↳ 已處理 1670/5957 個實體，推理出 7951 個新關係\n",
      "  ↳ 已處理 1680/5957 個實體，推理出 7998 個新關係\n",
      "  ↳ 已處理 1690/5957 個實體，推理出 8037 個新關係\n",
      "  ↳ 已處理 1700/5957 個實體，推理出 8091 個新關係\n",
      "  ↳ 已處理 1710/5957 個實體，推理出 8135 個新關係\n",
      "  ↳ 已處理 1720/5957 個實體，推理出 8179 個新關係\n",
      "  ↳ 已處理 1730/5957 個實體，推理出 8219 個新關係\n",
      "  ↳ 已處理 1740/5957 個實體，推理出 8261 個新關係\n",
      "  ↳ 已處理 1750/5957 個實體，推理出 8306 個新關係\n",
      "  ↳ 已處理 1760/5957 個實體，推理出 8355 個新關係\n",
      "  ↳ 已處理 1770/5957 個實體，推理出 8372 個新關係\n",
      "  ↳ 已處理 1780/5957 個實體，推理出 8422 個新關係\n",
      "  ↳ 已處理 1790/5957 個實體，推理出 8475 個新關係\n",
      "  ↳ 已處理 1800/5957 個實體，推理出 8514 個新關係\n",
      "  ↳ 已處理 1810/5957 個實體，推理出 8560 個新關係\n",
      "  ↳ 已處理 1820/5957 個實體，推理出 8607 個新關係\n",
      "  ↳ 已處理 1830/5957 個實體，推理出 8655 個新關係\n",
      "  ↳ 已處理 1840/5957 個實體，推理出 8704 個新關係\n",
      "  ↳ 已處理 1850/5957 個實體，推理出 8745 個新關係\n",
      "  ↳ 已處理 1860/5957 個實體，推理出 8798 個新關係\n",
      "  ↳ 已處理 1870/5957 個實體，推理出 8846 個新關係\n",
      "  ↳ 已處理 1880/5957 個實體，推理出 8895 個新關係\n",
      "  ↳ 已處理 1890/5957 個實體，推理出 8950 個新關係\n",
      "  ↳ 已處理 1900/5957 個實體，推理出 8995 個新關係\n",
      "  ↳ 已處理 1910/5957 個實體，推理出 9053 個新關係\n",
      "  ↳ 已處理 1920/5957 個實體，推理出 9108 個新關係\n",
      "  ↳ 已處理 1930/5957 個實體，推理出 9161 個新關係\n",
      "  ↳ 已處理 1940/5957 個實體，推理出 9208 個新關係\n",
      "  ↳ 已處理 1950/5957 個實體，推理出 9249 個新關係\n",
      "  ↳ 已處理 1960/5957 個實體，推理出 9301 個新關係\n",
      "  ↳ 已處理 1970/5957 個實體，推理出 9347 個新關係\n",
      "  ↳ 已處理 1980/5957 個實體，推理出 9387 個新關係\n",
      "  ↳ 已處理 1990/5957 個實體，推理出 9430 個新關係\n",
      "  ↳ 已處理 2000/5957 個實體，推理出 9473 個新關係\n",
      "  ↳ 已處理 2010/5957 個實體，推理出 9526 個新關係\n",
      "  ↳ 已處理 2020/5957 個實體，推理出 9578 個新關係\n",
      "  ↳ 已處理 2030/5957 個實體，推理出 9625 個新關係\n",
      "  ↳ 已處理 2040/5957 個實體，推理出 9669 個新關係\n",
      "  ↳ 已處理 2050/5957 個實體，推理出 9718 個新關係\n",
      "  ↳ 已處理 2060/5957 個實體，推理出 9767 個新關係\n",
      "  ↳ 已處理 2070/5957 個實體，推理出 9823 個新關係\n",
      "  ↳ 已處理 2080/5957 個實體，推理出 9876 個新關係\n",
      "  ↳ 已處理 2090/5957 個實體，推理出 9926 個新關係\n",
      "  ↳ 已處理 2100/5957 個實體，推理出 9978 個新關係\n",
      "  ↳ 已處理 2110/5957 個實體，推理出 10027 個新關係\n",
      "  ↳ 已處理 2120/5957 個實體，推理出 10076 個新關係\n",
      "  ↳ 已處理 2130/5957 個實體，推理出 10116 個新關係\n",
      "  ↳ 已處理 2140/5957 個實體，推理出 10166 個新關係\n",
      "  ↳ 已處理 2150/5957 個實體，推理出 10210 個新關係\n",
      "  ↳ 已處理 2160/5957 個實體，推理出 10273 個新關係\n",
      "  ↳ 已處理 2170/5957 個實體，推理出 10319 個新關係\n",
      "  ↳ 已處理 2180/5957 個實體，推理出 10362 個新關係\n",
      "  ↳ 已處理 2190/5957 個實體，推理出 10403 個新關係\n",
      "  ↳ 已處理 2200/5957 個實體，推理出 10447 個新關係\n",
      "  ↳ 已處理 2210/5957 個實體，推理出 10494 個新關係\n",
      "  ↳ 已處理 2220/5957 個實體，推理出 10534 個新關係\n",
      "  ↳ 已處理 2230/5957 個實體，推理出 10579 個新關係\n",
      "  ↳ 已處理 2240/5957 個實體，推理出 10626 個新關係\n",
      "  ↳ 已處理 2250/5957 個實體，推理出 10664 個新關係\n",
      "  ↳ 已處理 2260/5957 個實體，推理出 10708 個新關係\n",
      "  ↳ 已處理 2270/5957 個實體，推理出 10752 個新關係\n",
      "  ↳ 已處理 2280/5957 個實體，推理出 10790 個新關係\n",
      "  ↳ 已處理 2290/5957 個實體，推理出 10836 個新關係\n",
      "  ↳ 已處理 2300/5957 個實體，推理出 10881 個新關係\n",
      "  ↳ 已處理 2310/5957 個實體，推理出 10926 個新關係\n",
      "  ↳ 已處理 2320/5957 個實體，推理出 10984 個新關係\n",
      "  ↳ 已處理 2330/5957 個實體，推理出 11034 個新關係\n",
      "  ↳ 已處理 2340/5957 個實體，推理出 11074 個新關係\n",
      "  ↳ 已處理 2350/5957 個實體，推理出 11124 個新關係\n",
      "  ↳ 已處理 2360/5957 個實體，推理出 11176 個新關係\n",
      "  ↳ 已處理 2370/5957 個實體，推理出 11218 個新關係\n",
      "  ↳ 已處理 2380/5957 個實體，推理出 11265 個新關係\n",
      "  ↳ 已處理 2390/5957 個實體，推理出 11312 個新關係\n",
      "  ↳ 已處理 2400/5957 個實體，推理出 11362 個新關係\n",
      "  ↳ 已處理 2410/5957 個實體，推理出 11418 個新關係\n",
      "  ↳ 已處理 2420/5957 個實體，推理出 11461 個新關係\n",
      "  ↳ 已處理 2430/5957 個實體，推理出 11507 個新關係\n",
      "  ↳ 已處理 2440/5957 個實體，推理出 11548 個新關係\n",
      "  ↳ 已處理 2450/5957 個實體，推理出 11592 個新關係\n",
      "  ↳ 已處理 2460/5957 個實體，推理出 11628 個新關係\n",
      "  ↳ 已處理 2470/5957 個實體，推理出 11678 個新關係\n",
      "  ↳ 已處理 2480/5957 個實體，推理出 11719 個新關係\n",
      "  ↳ 已處理 2490/5957 個實體，推理出 11752 個新關係\n",
      "  ↳ 已處理 2500/5957 個實體，推理出 11801 個新關係\n",
      "  ↳ 已處理 2510/5957 個實體，推理出 11851 個新關係\n",
      "  ↳ 已處理 2520/5957 個實體，推理出 11891 個新關係\n",
      "  ↳ 已處理 2530/5957 個實體，推理出 11944 個新關係\n",
      "  ↳ 已處理 2540/5957 個實體，推理出 11987 個新關係\n",
      "  ↳ 已處理 2550/5957 個實體，推理出 12023 個新關係\n",
      "  ↳ 已處理 2560/5957 個實體，推理出 12080 個新關係\n",
      "  ↳ 已處理 2570/5957 個實體，推理出 12134 個新關係\n",
      "  ↳ 已處理 2580/5957 個實體，推理出 12184 個新關係\n",
      "  ↳ 已處理 2590/5957 個實體，推理出 12235 個新關係\n",
      "  ↳ 已處理 2600/5957 個實體，推理出 12277 個新關係\n",
      "  ↳ 已處理 2610/5957 個實體，推理出 12321 個新關係\n",
      "  ↳ 已處理 2620/5957 個實體，推理出 12369 個新關係\n",
      "  ↳ 已處理 2630/5957 個實體，推理出 12408 個新關係\n",
      "  ↳ 已處理 2640/5957 個實體，推理出 12462 個新關係\n",
      "  ↳ 已處理 2650/5957 個實體，推理出 12509 個新關係\n",
      "  ↳ 已處理 2660/5957 個實體，推理出 12563 個新關係\n",
      "  ↳ 已處理 2670/5957 個實體，推理出 12611 個新關係\n",
      "  ↳ 已處理 2680/5957 個實體，推理出 12658 個新關係\n",
      "  ↳ 已處理 2690/5957 個實體，推理出 12701 個新關係\n",
      "  ↳ 已處理 2700/5957 個實體，推理出 12752 個新關係\n",
      "  ↳ 已處理 2710/5957 個實體，推理出 12796 個新關係\n",
      "  ↳ 已處理 2720/5957 個實體，推理出 12842 個新關係\n",
      "  ↳ 已處理 2730/5957 個實體，推理出 12895 個新關係\n",
      "  ↳ 已處理 2740/5957 個實體，推理出 12943 個新關係\n",
      "  ↳ 已處理 2750/5957 個實體，推理出 12992 個新關係\n",
      "  ↳ 已處理 2760/5957 個實體，推理出 13038 個新關係\n",
      "  ↳ 已處理 2770/5957 個實體，推理出 13086 個新關係\n",
      "  ↳ 已處理 2780/5957 個實體，推理出 13136 個新關係\n",
      "  ↳ 已處理 2790/5957 個實體，推理出 13183 個新關係\n",
      "  ↳ 已處理 2800/5957 個實體，推理出 13229 個新關係\n",
      "  ↳ 已處理 2810/5957 個實體，推理出 13266 個新關係\n",
      "  ↳ 已處理 2820/5957 個實體，推理出 13332 個新關係\n",
      "  ↳ 已處理 2830/5957 個實體，推理出 13382 個新關係\n",
      "  ↳ 已處理 2840/5957 個實體，推理出 13437 個新關係\n",
      "  ↳ 已處理 2850/5957 個實體，推理出 13483 個新關係\n",
      "  ↳ 已處理 2860/5957 個實體，推理出 13526 個新關係\n",
      "  ↳ 已處理 2870/5957 個實體，推理出 13575 個新關係\n",
      "  ↳ 已處理 2880/5957 個實體，推理出 13624 個新關係\n",
      "  ↳ 已處理 2890/5957 個實體，推理出 13662 個新關係\n",
      "  ↳ 已處理 2900/5957 個實體，推理出 13711 個新關係\n",
      "  ↳ 已處理 2910/5957 個實體，推理出 13760 個新關係\n",
      "  ↳ 已處理 2920/5957 個實體，推理出 13809 個新關係\n",
      "  ↳ 已處理 2930/5957 個實體，推理出 13852 個新關係\n",
      "  ↳ 已處理 2940/5957 個實體，推理出 13902 個新關係\n",
      "  ↳ 已處理 2950/5957 個實體，推理出 13954 個新關係\n",
      "  ↳ 已處理 2960/5957 個實體，推理出 13997 個新關係\n",
      "  ↳ 已處理 2970/5957 個實體，推理出 14046 個新關係\n",
      "  ↳ 已處理 2980/5957 個實體，推理出 14107 個新關係\n",
      "  ↳ 已處理 2990/5957 個實體，推理出 14164 個新關係\n",
      "  ↳ 已處理 3000/5957 個實體，推理出 14211 個新關係\n",
      "  ↳ 已處理 3010/5957 個實體，推理出 14258 個新關係\n",
      "  ↳ 已處理 3020/5957 個實體，推理出 14304 個新關係\n",
      "  ↳ 已處理 3030/5957 個實體，推理出 14356 個新關係\n",
      "  ↳ 已處理 3040/5957 個實體，推理出 14401 個新關係\n",
      "  ↳ 已處理 3050/5957 個實體，推理出 14449 個新關係\n",
      "  ↳ 已處理 3060/5957 個實體，推理出 14497 個新關係\n",
      "  ↳ 已處理 3070/5957 個實體，推理出 14543 個新關係\n",
      "  ↳ 已處理 3080/5957 個實體，推理出 14588 個新關係\n",
      "  ↳ 已處理 3090/5957 個實體，推理出 14637 個新關係\n",
      "  ↳ 已處理 3100/5957 個實體，推理出 14685 個新關係\n",
      "  ↳ 已處理 3110/5957 個實體，推理出 14725 個新關係\n",
      "  ↳ 已處理 3120/5957 個實體，推理出 14770 個新關係\n",
      "  ↳ 已處理 3130/5957 個實體，推理出 14817 個新關係\n",
      "  ↳ 已處理 3140/5957 個實體，推理出 14874 個新關係\n",
      "  ↳ 已處理 3150/5957 個實體，推理出 14922 個新關係\n",
      "  ↳ 已處理 3160/5957 個實體，推理出 14967 個新關係\n",
      "  ↳ 已處理 3170/5957 個實體，推理出 15012 個新關係\n",
      "  ↳ 已處理 3180/5957 個實體，推理出 15061 個新關係\n",
      "  ↳ 已處理 3190/5957 個實體，推理出 15108 個新關係\n",
      "  ↳ 已處理 3200/5957 個實體，推理出 15156 個新關係\n",
      "  ↳ 已處理 3210/5957 個實體，推理出 15217 個新關係\n",
      "  ↳ 已處理 3220/5957 個實體，推理出 15269 個新關係\n",
      "  ↳ 已處理 3230/5957 個實體，推理出 15322 個新關係\n",
      "  ↳ 已處理 3240/5957 個實體，推理出 15367 個新關係\n",
      "  ↳ 已處理 3250/5957 個實體，推理出 15419 個新關係\n",
      "  ↳ 已處理 3260/5957 個實體，推理出 15467 個新關係\n",
      "  ↳ 已處理 3270/5957 個實體，推理出 15516 個新關係\n",
      "  ↳ 已處理 3280/5957 個實體，推理出 15571 個新關係\n",
      "  ↳ 已處理 3290/5957 個實體，推理出 15607 個新關係\n",
      "  ↳ 已處理 3300/5957 個實體，推理出 15655 個新關係\n",
      "  ↳ 已處理 3310/5957 個實體，推理出 15697 個新關係\n",
      "  ↳ 已處理 3320/5957 個實體，推理出 15745 個新關係\n",
      "  ↳ 已處理 3330/5957 個實體，推理出 15785 個新關係\n",
      "  ↳ 已處理 3340/5957 個實體，推理出 15839 個新關係\n",
      "  ↳ 已處理 3350/5957 個實體，推理出 15887 個新關係\n",
      "  ↳ 已處理 3360/5957 個實體，推理出 15932 個新關係\n",
      "  ↳ 已處理 3370/5957 個實體，推理出 15980 個新關係\n",
      "  ↳ 已處理 3380/5957 個實體，推理出 16038 個新關係\n",
      "  ↳ 已處理 3390/5957 個實體，推理出 16083 個新關係\n",
      "  ↳ 已處理 3400/5957 個實體，推理出 16133 個新關係\n",
      "  ↳ 已處理 3410/5957 個實體，推理出 16180 個新關係\n",
      "  ↳ 已處理 3420/5957 個實體，推理出 16226 個新關係\n",
      "  ↳ 已處理 3430/5957 個實體，推理出 16270 個新關係\n",
      "  ↳ 已處理 3440/5957 個實體，推理出 16321 個新關係\n",
      "  ↳ 已處理 3450/5957 個實體，推理出 16366 個新關係\n",
      "  ↳ 已處理 3460/5957 個實體，推理出 16413 個新關係\n",
      "  ↳ 已處理 3470/5957 個實體，推理出 16456 個新關係\n",
      "  ↳ 已處理 3480/5957 個實體，推理出 16505 個新關係\n",
      "  ↳ 已處理 3490/5957 個實體，推理出 16556 個新關係\n",
      "  ↳ 已處理 3500/5957 個實體，推理出 16609 個新關係\n",
      "  ↳ 已處理 3510/5957 個實體，推理出 16658 個新關係\n",
      "  ↳ 已處理 3520/5957 個實體，推理出 16710 個新關係\n",
      "  ↳ 已處理 3530/5957 個實體，推理出 16755 個新關係\n",
      "  ↳ 已處理 3540/5957 個實體，推理出 16808 個新關係\n",
      "  ↳ 已處理 3550/5957 個實體，推理出 16860 個新關係\n",
      "  ↳ 已處理 3560/5957 個實體，推理出 16917 個新關係\n",
      "  ↳ 已處理 3570/5957 個實體，推理出 16960 個新關係\n",
      "  ↳ 已處理 3580/5957 個實體，推理出 17006 個新關係\n",
      "  ↳ 已處理 3590/5957 個實體，推理出 17063 個新關係\n",
      "  ↳ 已處理 3600/5957 個實體，推理出 17103 個新關係\n",
      "  ↳ 已處理 3610/5957 個實體，推理出 17149 個新關係\n",
      "  ↳ 已處理 3620/5957 個實體，推理出 17196 個新關係\n",
      "  ↳ 已處理 3630/5957 個實體，推理出 17246 個新關係\n",
      "  ↳ 已處理 3640/5957 個實體，推理出 17297 個新關係\n",
      "  ↳ 已處理 3650/5957 個實體，推理出 17347 個新關係\n",
      "  ↳ 已處理 3660/5957 個實體，推理出 17398 個新關係\n",
      "  ↳ 已處理 3670/5957 個實體，推理出 17439 個新關係\n",
      "  ↳ 已處理 3680/5957 個實體，推理出 17487 個新關係\n",
      "  ↳ 已處理 3690/5957 個實體，推理出 17534 個新關係\n",
      "  ↳ 已處理 3700/5957 個實體，推理出 17577 個新關係\n",
      "  ↳ 已處理 3710/5957 個實體，推理出 17632 個新關係\n",
      "  ↳ 已處理 3720/5957 個實體，推理出 17684 個新關係\n",
      "  ↳ 已處理 3730/5957 個實體，推理出 17738 個新關係\n",
      "  ↳ 已處理 3740/5957 個實體，推理出 17783 個新關係\n",
      "  ↳ 已處理 3750/5957 個實體，推理出 17827 個新關係\n",
      "  ↳ 已處理 3760/5957 個實體，推理出 17876 個新關係\n",
      "  ↳ 已處理 3770/5957 個實體，推理出 17924 個新關係\n",
      "  ↳ 已處理 3780/5957 個實體，推理出 17972 個新關係\n",
      "  ↳ 已處理 3790/5957 個實體，推理出 18016 個新關係\n",
      "  ↳ 已處理 3800/5957 個實體，推理出 18062 個新關係\n",
      "  ↳ 已處理 3810/5957 個實體，推理出 18110 個新關係\n",
      "  ↳ 已處理 3820/5957 個實體，推理出 18163 個新關係\n",
      "  ↳ 已處理 3830/5957 個實體，推理出 18198 個新關係\n",
      "  ↳ 已處理 3840/5957 個實體，推理出 18234 個新關係\n",
      "  ↳ 已處理 3850/5957 個實體，推理出 18276 個新關係\n",
      "  ↳ 已處理 3860/5957 個實體，推理出 18326 個新關係\n",
      "  ↳ 已處理 3870/5957 個實體，推理出 18375 個新關係\n",
      "  ↳ 已處理 3880/5957 個實體，推理出 18419 個新關係\n",
      "  ↳ 已處理 3890/5957 個實體，推理出 18467 個新關係\n",
      "  ↳ 已處理 3900/5957 個實體，推理出 18513 個新關係\n",
      "  ↳ 已處理 3910/5957 個實體，推理出 18560 個新關係\n",
      "  ↳ 已處理 3920/5957 個實體，推理出 18605 個新關係\n",
      "  ↳ 已處理 3930/5957 個實體，推理出 18666 個新關係\n",
      "  ↳ 已處理 3940/5957 個實體，推理出 18708 個新關係\n",
      "  ↳ 已處理 3950/5957 個實體，推理出 18759 個新關係\n",
      "  ↳ 已處理 3960/5957 個實體，推理出 18812 個新關係\n",
      "  ↳ 已處理 3970/5957 個實體，推理出 18866 個新關係\n",
      "  ↳ 已處理 3980/5957 個實體，推理出 18903 個新關係\n",
      "  ↳ 已處理 3990/5957 個實體，推理出 18949 個新關係\n",
      "  ↳ 已處理 4000/5957 個實體，推理出 19006 個新關係\n",
      "  ↳ 已處理 4010/5957 個實體，推理出 19053 個新關係\n",
      "  ↳ 已處理 4020/5957 個實體，推理出 19095 個新關係\n",
      "  ↳ 已處理 4030/5957 個實體，推理出 19146 個新關係\n",
      "  ↳ 已處理 4040/5957 個實體，推理出 19186 個新關係\n",
      "  ↳ 已處理 4050/5957 個實體，推理出 19236 個新關係\n",
      "  ↳ 已處理 4060/5957 個實體，推理出 19278 個新關係\n",
      "  ↳ 已處理 4070/5957 個實體，推理出 19325 個新關係\n",
      "  ↳ 已處理 4080/5957 個實體，推理出 19374 個新關係\n",
      "  ↳ 已處理 4090/5957 個實體，推理出 19417 個新關係\n",
      "  ↳ 已處理 4100/5957 個實體，推理出 19465 個新關係\n",
      "  ↳ 已處理 4110/5957 個實體，推理出 19527 個新關係\n",
      "  ↳ 已處理 4120/5957 個實體，推理出 19571 個新關係\n",
      "  ↳ 已處理 4130/5957 個實體，推理出 19624 個新關係\n",
      "  ↳ 已處理 4140/5957 個實體，推理出 19670 個新關係\n",
      "  ↳ 已處理 4150/5957 個實體，推理出 19713 個新關係\n",
      "  ↳ 已處理 4160/5957 個實體，推理出 19762 個新關係\n",
      "  ↳ 已處理 4170/5957 個實體，推理出 19811 個新關係\n",
      "  ↳ 已處理 4180/5957 個實體，推理出 19858 個新關係\n",
      "  ↳ 已處理 4190/5957 個實體，推理出 19912 個新關係\n",
      "  ↳ 已處理 4200/5957 個實體，推理出 19962 個新關係\n",
      "  ↳ 已處理 4210/5957 個實體，推理出 20013 個新關係\n",
      "  ↳ 已處理 4220/5957 個實體，推理出 20080 個新關係\n",
      "  ↳ 已處理 4230/5957 個實體，推理出 20135 個新關係\n",
      "  ↳ 已處理 4240/5957 個實體，推理出 20179 個新關係\n",
      "  ↳ 已處理 4250/5957 個實體，推理出 20223 個新關係\n",
      "  ↳ 已處理 4260/5957 個實體，推理出 20264 個新關係\n",
      "  ↳ 已處理 4270/5957 個實體，推理出 20328 個新關係\n",
      "  ↳ 已處理 4280/5957 個實體，推理出 20378 個新關係\n",
      "  ↳ 已處理 4290/5957 個實體，推理出 20427 個新關係\n",
      "  ↳ 已處理 4300/5957 個實體，推理出 20464 個新關係\n",
      "  ↳ 已處理 4310/5957 個實體，推理出 20509 個新關係\n",
      "  ↳ 已處理 4320/5957 個實體，推理出 20566 個新關係\n",
      "  ↳ 已處理 4330/5957 個實體，推理出 20602 個新關係\n",
      "  ↳ 已處理 4340/5957 個實體，推理出 20649 個新關係\n",
      "  ↳ 已處理 4350/5957 個實體，推理出 20697 個新關係\n",
      "  ↳ 已處理 4360/5957 個實體，推理出 20747 個新關係\n",
      "  ↳ 已處理 4370/5957 個實體，推理出 20790 個新關係\n",
      "  ↳ 已處理 4380/5957 個實體，推理出 20837 個新關係\n",
      "  ↳ 已處理 4390/5957 個實體，推理出 20888 個新關係\n",
      "  ↳ 已處理 4400/5957 個實體，推理出 20933 個新關係\n",
      "  ↳ 已處理 4410/5957 個實體，推理出 20977 個新關係\n",
      "  ↳ 已處理 4420/5957 個實體，推理出 21016 個新關係\n",
      "  ↳ 已處理 4430/5957 個實體，推理出 21073 個新關係\n",
      "  ↳ 已處理 4440/5957 個實體，推理出 21122 個新關係\n",
      "  ↳ 已處理 4450/5957 個實體，推理出 21172 個新關係\n",
      "  ↳ 已處理 4460/5957 個實體，推理出 21221 個新關係\n",
      "  ↳ 已處理 4470/5957 個實體，推理出 21267 個新關係\n",
      "  ↳ 已處理 4480/5957 個實體，推理出 21314 個新關係\n",
      "  ↳ 已處理 4490/5957 個實體，推理出 21364 個新關係\n",
      "  ↳ 已處理 4500/5957 個實體，推理出 21418 個新關係\n",
      "  ↳ 已處理 4510/5957 個實體，推理出 21466 個新關係\n",
      "  ↳ 已處理 4520/5957 個實體，推理出 21515 個新關係\n",
      "  ↳ 已處理 4530/5957 個實體，推理出 21563 個新關係\n",
      "  ↳ 已處理 4540/5957 個實體，推理出 21621 個新關係\n",
      "  ↳ 已處理 4550/5957 個實體，推理出 21669 個新關係\n",
      "  ↳ 已處理 4560/5957 個實體，推理出 21716 個新關係\n",
      "  ↳ 已處理 4570/5957 個實體，推理出 21764 個新關係\n",
      "  ↳ 已處理 4580/5957 個實體，推理出 21804 個新關係\n",
      "  ↳ 已處理 4590/5957 個實體，推理出 21846 個新關係\n",
      "  ↳ 已處理 4600/5957 個實體，推理出 21894 個新關係\n",
      "  ↳ 已處理 4610/5957 個實體，推理出 21940 個新關係\n",
      "  ↳ 已處理 4620/5957 個實體，推理出 21987 個新關係\n",
      "  ↳ 已處理 4630/5957 個實體，推理出 22030 個新關係\n",
      "  ↳ 已處理 4640/5957 個實體，推理出 22080 個新關係\n",
      "  ↳ 已處理 4650/5957 個實體，推理出 22128 個新關係\n",
      "  ↳ 已處理 4660/5957 個實體，推理出 22177 個新關係\n",
      "  ↳ 已處理 4670/5957 個實體，推理出 22217 個新關係\n",
      "  ↳ 已處理 4680/5957 個實體，推理出 22267 個新關係\n",
      "  ↳ 已處理 4690/5957 個實體，推理出 22310 個新關係\n",
      "  ↳ 已處理 4700/5957 個實體，推理出 22360 個新關係\n",
      "  ↳ 已處理 4710/5957 個實體，推理出 22408 個新關係\n",
      "  ↳ 已處理 4720/5957 個實體，推理出 22453 個新關係\n",
      "  ↳ 已處理 4730/5957 個實體，推理出 22499 個新關係\n",
      "  ↳ 已處理 4740/5957 個實體，推理出 22546 個新關係\n",
      "  ↳ 已處理 4750/5957 個實體，推理出 22591 個新關係\n",
      "  ↳ 已處理 4760/5957 個實體，推理出 22636 個新關係\n",
      "  ↳ 已處理 4770/5957 個實體，推理出 22679 個新關係\n",
      "  ↳ 已處理 4780/5957 個實體，推理出 22723 個新關係\n",
      "  ↳ 已處理 4790/5957 個實體，推理出 22767 個新關係\n",
      "  ↳ 已處理 4800/5957 個實體，推理出 22821 個新關係\n",
      "  ↳ 已處理 4810/5957 個實體，推理出 22868 個新關係\n",
      "  ↳ 已處理 4820/5957 個實體，推理出 22914 個新關係\n",
      "  ↳ 已處理 4830/5957 個實體，推理出 22962 個新關係\n",
      "  ↳ 已處理 4840/5957 個實體，推理出 23009 個新關係\n",
      "  ↳ 已處理 4850/5957 個實體，推理出 23060 個新關係\n",
      "  ↳ 已處理 4860/5957 個實體，推理出 23118 個新關係\n",
      "  ↳ 已處理 4870/5957 個實體，推理出 23172 個新關係\n",
      "  ↳ 已處理 4880/5957 個實體，推理出 23229 個新關係\n",
      "  ↳ 已處理 4890/5957 個實體，推理出 23271 個新關係\n",
      "  ↳ 已處理 4900/5957 個實體，推理出 23316 個新關係\n",
      "  ↳ 已處理 4910/5957 個實體，推理出 23364 個新關係\n",
      "  ↳ 已處理 4920/5957 個實體，推理出 23415 個新關係\n",
      "  ↳ 已處理 4930/5957 個實體，推理出 23458 個新關係\n",
      "  ↳ 已處理 4940/5957 個實體，推理出 23503 個新關係\n",
      "  ↳ 已處理 4950/5957 個實體，推理出 23550 個新關係\n",
      "  ↳ 已處理 4960/5957 個實體，推理出 23597 個新關係\n",
      "  ↳ 已處理 4970/5957 個實體，推理出 23646 個新關係\n",
      "  ↳ 已處理 4980/5957 個實體，推理出 23701 個新關係\n",
      "  ↳ 已處理 4990/5957 個實體，推理出 23750 個新關係\n",
      "  ↳ 已處理 5000/5957 個實體，推理出 23790 個新關係\n",
      "  ↳ 已處理 5010/5957 個實體，推理出 23845 個新關係\n",
      "  ↳ 已處理 5020/5957 個實體，推理出 23898 個新關係\n",
      "  ↳ 已處理 5030/5957 個實體，推理出 23942 個新關係\n",
      "  ↳ 已處理 5040/5957 個實體，推理出 23992 個新關係\n",
      "  ↳ 已處理 5050/5957 個實體，推理出 24058 個新關係\n",
      "  ↳ 已處理 5060/5957 個實體，推理出 24102 個新關係\n",
      "  ↳ 已處理 5070/5957 個實體，推理出 24153 個新關係\n",
      "  ↳ 已處理 5080/5957 個實體，推理出 24193 個新關係\n",
      "  ↳ 已處理 5090/5957 個實體，推理出 24236 個新關係\n",
      "  ↳ 已處理 5100/5957 個實體，推理出 24281 個新關係\n",
      "  ↳ 已處理 5110/5957 個實體，推理出 24324 個新關係\n",
      "  ↳ 已處理 5120/5957 個實體，推理出 24379 個新關係\n",
      "  ↳ 已處理 5130/5957 個實體，推理出 24427 個新關係\n",
      "  ↳ 已處理 5140/5957 個實體，推理出 24476 個新關係\n",
      "  ↳ 已處理 5150/5957 個實體，推理出 24522 個新關係\n",
      "  ↳ 已處理 5160/5957 個實體，推理出 24563 個新關係\n",
      "  ↳ 已處理 5170/5957 個實體，推理出 24608 個新關係\n",
      "  ↳ 已處理 5180/5957 個實體，推理出 24653 個新關係\n",
      "  ↳ 已處理 5190/5957 個實體，推理出 24699 個新關係\n",
      "  ↳ 已處理 5200/5957 個實體，推理出 24740 個新關係\n",
      "  ↳ 已處理 5210/5957 個實體，推理出 24789 個新關係\n",
      "  ↳ 已處理 5220/5957 個實體，推理出 24828 個新關係\n",
      "  ↳ 已處理 5230/5957 個實體，推理出 24870 個新關係\n",
      "  ↳ 已處理 5240/5957 個實體，推理出 24928 個新關係\n",
      "  ↳ 已處理 5250/5957 個實體，推理出 24976 個新關係\n",
      "  ↳ 已處理 5260/5957 個實體，推理出 25025 個新關係\n",
      "  ↳ 已處理 5270/5957 個實體，推理出 25073 個新關係\n",
      "  ↳ 已處理 5280/5957 個實體，推理出 25116 個新關係\n",
      "  ↳ 已處理 5290/5957 個實體，推理出 25158 個新關係\n",
      "  ↳ 已處理 5300/5957 個實體，推理出 25195 個新關係\n",
      "  ↳ 已處理 5310/5957 個實體，推理出 25235 個新關係\n",
      "  ↳ 已處理 5320/5957 個實體，推理出 25288 個新關係\n",
      "  ↳ 已處理 5330/5957 個實體，推理出 25338 個新關係\n",
      "  ↳ 已處理 5340/5957 個實體，推理出 25380 個新關係\n",
      "  ↳ 已處理 5350/5957 個實體，推理出 25424 個新關係\n",
      "  ↳ 已處理 5360/5957 個實體，推理出 25473 個新關係\n",
      "  ↳ 已處理 5370/5957 個實體，推理出 25513 個新關係\n",
      "  ↳ 已處理 5380/5957 個實體，推理出 25555 個新關係\n",
      "  ↳ 已處理 5390/5957 個實體，推理出 25597 個新關係\n",
      "  ↳ 已處理 5400/5957 個實體，推理出 25637 個新關係\n",
      "  ↳ 已處理 5410/5957 個實體，推理出 25683 個新關係\n",
      "  ↳ 已處理 5420/5957 個實體，推理出 25732 個新關係\n",
      "  ↳ 已處理 5430/5957 個實體，推理出 25777 個新關係\n",
      "  ↳ 已處理 5440/5957 個實體，推理出 25830 個新關係\n",
      "  ↳ 已處理 5450/5957 個實體，推理出 25877 個新關係\n",
      "  ↳ 已處理 5460/5957 個實體，推理出 25925 個新關係\n",
      "  ↳ 已處理 5470/5957 個實體，推理出 25978 個新關係\n",
      "  ↳ 已處理 5480/5957 個實體，推理出 26025 個新關係\n",
      "  ↳ 已處理 5490/5957 個實體，推理出 26072 個新關係\n",
      "  ↳ 已處理 5500/5957 個實體，推理出 26130 個新關係\n",
      "  ↳ 已處理 5510/5957 個實體，推理出 26178 個新關係\n",
      "  ↳ 已處理 5520/5957 個實體，推理出 26225 個新關係\n",
      "  ↳ 已處理 5530/5957 個實體，推理出 26264 個新關係\n",
      "  ↳ 已處理 5540/5957 個實體，推理出 26313 個新關係\n",
      "  ↳ 已處理 5550/5957 個實體，推理出 26359 個新關係\n",
      "  ↳ 已處理 5560/5957 個實體，推理出 26395 個新關係\n",
      "  ↳ 已處理 5570/5957 個實體，推理出 26462 個新關係\n",
      "  ↳ 已處理 5580/5957 個實體，推理出 26499 個新關係\n",
      "  ↳ 已處理 5590/5957 個實體，推理出 26543 個新關係\n",
      "  ↳ 已處理 5600/5957 個實體，推理出 26594 個新關係\n",
      "  ↳ 已處理 5610/5957 個實體，推理出 26639 個新關係\n",
      "  ↳ 已處理 5620/5957 個實體，推理出 26686 個新關係\n",
      "  ↳ 已處理 5630/5957 個實體，推理出 26736 個新關係\n",
      "  ↳ 已處理 5640/5957 個實體，推理出 26783 個新關係\n",
      "  ↳ 已處理 5650/5957 個實體，推理出 26830 個新關係\n",
      "  ↳ 已處理 5660/5957 個實體，推理出 26893 個新關係\n",
      "  ↳ 已處理 5670/5957 個實體，推理出 26949 個新關係\n",
      "  ↳ 已處理 5680/5957 個實體，推理出 26998 個新關係\n",
      "  ↳ 已處理 5690/5957 個實體，推理出 27047 個新關係\n",
      "  ↳ 已處理 5700/5957 個實體，推理出 27086 個新關係\n",
      "  ↳ 已處理 5710/5957 個實體，推理出 27139 個新關係\n",
      "  ↳ 已處理 5720/5957 個實體，推理出 27189 個新關係\n",
      "  ↳ 已處理 5730/5957 個實體，推理出 27234 個新關係\n",
      "  ↳ 已處理 5740/5957 個實體，推理出 27279 個新關係\n",
      "  ↳ 已處理 5750/5957 個實體，推理出 27322 個新關係\n",
      "  ↳ 已處理 5760/5957 個實體，推理出 27372 個新關係\n",
      "  ↳ 已處理 5770/5957 個實體，推理出 27430 個新關係\n",
      "  ↳ 已處理 5780/5957 個實體，推理出 27480 個新關係\n",
      "  ↳ 已處理 5790/5957 個實體，推理出 27530 個新關係\n",
      "  ↳ 已處理 5800/5957 個實體，推理出 27572 個新關係\n",
      "  ↳ 已處理 5810/5957 個實體，推理出 27612 個新關係\n",
      "  ↳ 已處理 5820/5957 個實體，推理出 27661 個新關係\n",
      "  ↳ 已處理 5830/5957 個實體，推理出 27702 個新關係\n",
      "  ↳ 已處理 5840/5957 個實體，推理出 27749 個新關係\n",
      "  ↳ 已處理 5850/5957 個實體，推理出 27794 個新關係\n",
      "  ↳ 已處理 5860/5957 個實體，推理出 27843 個新關係\n",
      "  ↳ 已處理 5870/5957 個實體，推理出 27896 個新關係\n",
      "  ↳ 已處理 5880/5957 個實體，推理出 27942 個新關係\n",
      "  ↳ 已處理 5890/5957 個實體，推理出 27990 個新關係\n",
      "  ↳ 已處理 5900/5957 個實體，推理出 28037 個新關係\n",
      "  ↳ 已處理 5910/5957 個實體，推理出 28075 個新關係\n",
      "  ↳ 已處理 5920/5957 個實體，推理出 28124 個新關係\n",
      "  ↳ 已處理 5930/5957 個實體，推理出 28173 個新關係\n",
      "  ↳ 已處理 5940/5957 個實體，推理出 28219 個新關係\n",
      "  ↳ 已處理 5950/5957 個實體，推理出 28268 個新關係\n",
      "\n",
      "======================================================================\n",
      "✅ 全局關係推理完成\n",
      "  • 處理實體數：5957\n",
      "  • 推理新關係：28296\n",
      "  • 密度變化：0.841 → 5.298 (+4.457)\n",
      "======================================================================\n",
      "\n",
      "📊 階段2完成摘要：\n",
      "  • 處理實體數：5957\n",
      "  • 推理關係數：28296\n",
      "  • 密度提升：0.841 → 5.298\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 🚀 執行階段2：全局關係推理\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔴 階段2：全局關係推理（針對弱連接實體）\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 執行全局關係推理，針對度數1-3的實體\n",
    "INFERENCE_RESULTS = InferGlobalRelations(\n",
    "    driver=GRAPH_DRIVER,\n",
    "    client=OLLAMA_CLIENT,\n",
    "    model=GRAPH_CREATE_MODEL,\n",
    "    dataset_id=DATASET_ID,\n",
    "    max_inference_per_entity=5,\n",
    "    min_entity_degree=1,\n",
    "    max_entity_degree=3,\n",
    "    temperature=0.0,\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 階段2完成摘要：\")\n",
    "print(f\"  • 處理實體數：{INFERENCE_RESULTS['processed_entities']}\")\n",
    "print(f\"  • 推理關係數：{INFERENCE_RESULTS['inferred_relations']}\")\n",
    "print(f\"  • 密度提升：{INFERENCE_RESULTS['density_before']:.3f} → {INFERENCE_RESULTS['density_after']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "81c49dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 開始修正關係來源標記...\n",
      "======================================================================\n",
      "\n",
      "📊 檢查問題規模...\n",
      "  • 缺少來源標記的關係總數：28273\n",
      "    - inferred 關係：28273\n",
      "    - densified 關係：0\n",
      "    - enhanced 關係：0\n",
      "\n",
      "🔄 修正 inferred 關係的來源標記...\n",
      "  ✅ 修正 28273 個 inferred 關係\n",
      "\n",
      "🔄 檢查 densified 關係...\n",
      "  ✅ 修正 0 個 densified 關係\n",
      "\n",
      "🔄 檢查 enhanced 關係...\n",
      "  ✅ 修正 0 個 enhanced 關係\n",
      "\n",
      "📊 最終驗證...\n",
      "  • 關係總數：33606\n",
      "  • 有來源標記：33606 (100.0%)\n",
      "  • 多來源支持：1733 (5.2%)\n",
      "  • 仍缺少來源：0 (0.0%)\n",
      "\n",
      "======================================================================\n",
      "✅ 關係來源標記修正完成！\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# 🔧 修正關係來源標記\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔧 開始修正關係來源標記...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    # 1. 檢查問題規模\n",
    "    print(\"\\n📊 檢查問題規模...\")\n",
    "    \n",
    "    missing_source = session.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        WHERE r.chunks IS NULL OR size(r.chunks) = 0\n",
    "        RETURN \n",
    "            count(r) AS missing_count,\n",
    "            sum(CASE WHEN r.inferred = true THEN 1 ELSE 0 END) AS inferred_missing,\n",
    "            sum(CASE WHEN r.densified = true THEN 1 ELSE 0 END) AS densified_missing,\n",
    "            sum(CASE WHEN r.enhanced = true THEN 1 ELSE 0 END) AS enhanced_missing\n",
    "    \"\"\").single()\n",
    "    \n",
    "    print(f\"  • 缺少來源標記的關係總數：{missing_source['missing_count']}\")\n",
    "    print(f\"    - inferred 關係：{missing_source['inferred_missing']}\")\n",
    "    print(f\"    - densified 關係：{missing_source['densified_missing']}\")\n",
    "    print(f\"    - enhanced 關係：{missing_source['enhanced_missing']}\")\n",
    "    \n",
    "    if missing_source['missing_count'] == 0:\n",
    "        print(\"\\n✅ 所有關係都已正確標記來源！\")\n",
    "    else:\n",
    "        # 2. 修正 inferred 關係的來源\n",
    "        print(f\"\\n🔄 修正 inferred 關係的來源標記...\")\n",
    "        \n",
    "        result_inferred = session.run(\"\"\"\n",
    "            MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "            WHERE r.inferred = true AND (r.chunks IS NULL OR size(r.chunks) = 0)\n",
    "            \n",
    "            // 找到頭尾實體共同出現的 Chunks\n",
    "            OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(h)\n",
    "            OPTIONAL MATCH (c)-[:MENTIONS]->(t)\n",
    "            WITH r, collect(DISTINCT c.id) AS common_chunks\n",
    "            \n",
    "            // 如果有共同 Chunk，使用共同 Chunk；否則使用頭實體的 Chunk\n",
    "            SET r.chunks = CASE \n",
    "                WHEN size(common_chunks) > 0 THEN common_chunks\n",
    "                ELSE []\n",
    "            END\n",
    "            \n",
    "            RETURN count(r) AS fixed_count\n",
    "        \"\"\").single()\n",
    "        \n",
    "        print(f\"  ✅ 修正 {result_inferred['fixed_count']} 個 inferred 關係\")\n",
    "        \n",
    "        # 3. 修正 densified 關係的來源（應該在寫入時就有，這是備用）\n",
    "        print(f\"\\n🔄 檢查 densified 關係...\")\n",
    "        \n",
    "        result_densified = session.run(\"\"\"\n",
    "            MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "            WHERE r.densified = true AND (r.chunks IS NULL OR size(r.chunks) = 0)\n",
    "            \n",
    "            // 找到頭尾實體共同出現的 Chunks\n",
    "            OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(h)\n",
    "            OPTIONAL MATCH (c)-[:MENTIONS]->(t)\n",
    "            WITH r, collect(DISTINCT c.id) AS common_chunks\n",
    "            \n",
    "            SET r.chunks = common_chunks\n",
    "            \n",
    "            RETURN count(r) AS fixed_count\n",
    "        \"\"\").single()\n",
    "        \n",
    "        print(f\"  ✅ 修正 {result_densified['fixed_count']} 個 densified 關係\")\n",
    "        \n",
    "        # 4. 修正 enhanced 關係的來源\n",
    "        print(f\"\\n🔄 檢查 enhanced 關係...\")\n",
    "        \n",
    "        result_enhanced = session.run(\"\"\"\n",
    "            MATCH (h:Entity)-[r:RELATION]->(t:Entity)\n",
    "            WHERE r.enhanced = true AND (r.chunks IS NULL OR size(r.chunks) = 0)\n",
    "            \n",
    "            // 找到頭尾實體共同出現的 Chunks\n",
    "            OPTIONAL MATCH (c:Chunk)-[:MENTIONS]->(h)\n",
    "            OPTIONAL MATCH (c)-[:MENTIONS]->(t)\n",
    "            WITH r, collect(DISTINCT c.id) AS common_chunks\n",
    "            \n",
    "            SET r.chunks = common_chunks\n",
    "            \n",
    "            RETURN count(r) AS fixed_count\n",
    "        \"\"\").single()\n",
    "        \n",
    "        print(f\"  ✅ 修正 {result_enhanced['fixed_count']} 個 enhanced 關係\")\n",
    "        \n",
    "        # 5. 最終驗證\n",
    "        print(f\"\\n📊 最終驗證...\")\n",
    "        \n",
    "        final_check = session.run(\"\"\"\n",
    "            MATCH ()-[r:RELATION]->()\n",
    "            RETURN \n",
    "                count(r) AS total_relations,\n",
    "                sum(CASE WHEN r.chunks IS NULL OR size(r.chunks) = 0 THEN 1 ELSE 0 END) AS still_missing,\n",
    "                sum(CASE WHEN size(r.chunks) >= 1 THEN 1 ELSE 0 END) AS has_source,\n",
    "                sum(CASE WHEN size(r.chunks) >= 2 THEN 1 ELSE 0 END) AS multi_source\n",
    "        \"\"\").single()\n",
    "        \n",
    "        print(f\"  • 關係總數：{final_check['total_relations']}\")\n",
    "        print(f\"  • 有來源標記：{final_check['has_source']} ({final_check['has_source']/final_check['total_relations']*100:.1f}%)\")\n",
    "        print(f\"  • 多來源支持：{final_check['multi_source']} ({final_check['multi_source']/final_check['total_relations']*100:.1f}%)\")\n",
    "        print(f\"  • 仍缺少來源：{final_check['still_missing']} ({final_check['still_missing']/final_check['total_relations']*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ 關係來源標記修正完成！\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61ed13",
   "metadata": {},
   "source": [
    "# 🐐 Graph RAG Evaluation Workflow\n",
    "\n",
    "依照指定流程，以下 Notebook 將以四個獨立的程式 Cell 完成：\n",
    "1. **全域配置與初始化** – 設定所有必要的變數與路徑。\n",
    "2. **圖譜建立、驗證與檢索優化** – 建構 Neo4j 圖譜、檢查完整性並調整檢索 reranking。\n",
    "3. **數據載入、問答執行與處理** – 限制題目數量、執行 RAG 問答並清理輸出。\n",
    "4. **結果儲存與函式呼叫** – 匯出評估結果並呼叫 QA / QA_expansion 函式。\n",
    "\n",
    "執行前請確認 Neo4j 與 Ollama 服務已啟動，且模型已於本機安裝完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75f160e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded → LLM: deepseek-r1:8b-llama-distill-q4_K_M, Embedder: nomic-embed-text:nomic, KB: goat_data_text collection-1.2-eng.txt, Max questions: 50, Language: english\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from neo4j_graphrag.llm import OllamaLLM\n",
    "from neo4j_graphrag.retrievers import VectorRetriever\n",
    "from neo4j_graphrag.retrievers.hybrid import HybridRetriever, HybridSearchRanker\n",
    "from ollama import Client\n",
    "\n",
    "# 全域配置與初始化\n",
    "LLM_MODEL = \"deepseek-r1:8b-llama-distill-q4_K_M\"\n",
    "GRAPH_CREATE_MODEL = \"deepseek-r1:8b-llama-distill-q4_K_M\"\n",
    "MAX_QUESTIONS = 50\n",
    "EMBED_MODEL = \"nomic-embed-text:nomic\"\n",
    "KNOWLEDGE_BASE_PATH = Path(\"goat_data_text collection-1.2-eng.txt\")\n",
    "QUESTION_DATASET_PATH = Path(\"topic.csv\")\n",
    "CHUNK_SIZE = 2048\n",
    "CHUNK_OVERLAP = 512\n",
    "LLM_TEMPERATURE = 0.0\n",
    "TOP_K = 15\n",
    "VECTOR_INDEX_NAME = \"chunk_embeddings\"\n",
    "FULLTEXT_INDEX_NAME = \"chunk_text_fts\"\n",
    "DATASET_ID = KNOWLEDGE_BASE_PATH.stem.replace(\" \", \"_\")\n",
    "ANSWER_LANGUAGE = \"english\"\n",
    "DRIVER = GraphDatabase.driver(\n",
    "    os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\"),\n",
    "    auth=(\n",
    "        os.environ.get(\"NEO4J_USER\", \"neo4j\"),\n",
    "        os.environ.get(\"NEO4J_PASSWORD\", \"neo4jgoat\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "NEO4J_URI = os.environ.get(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.environ.get(\"NEO4J_USER\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.environ.get(\"NEO4J_PASSWORD\", \"neo4jgoat\")\n",
    "OLLAMA_HOST = os.environ.get(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "\n",
    "print(\n",
    "    \"✅ Configuration loaded → LLM: {model}, Embedder: {embed}, KB: {kb}, Max questions: {n}, Language: {lang}\".format(\n",
    "        model=LLM_MODEL,\n",
    "        embed=EMBED_MODEL,\n",
    "        kb=KNOWLEDGE_BASE_PATH.name,\n",
    "        n=MAX_QUESTIONS,\n",
    "        lang=ANSWER_LANGUAGE,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc48b1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已修補 OllamaLLM.invoke 方法，支援 Ollama 字典響應格式\n",
      "   修復問題：'dict' object has no attribute 'message'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔧 修復 neo4j_graphrag OllamaLLM 與 Ollama 客戶端的兼容性問題\n",
    "# 問題：OllamaLLM 期望新版 API (response.message.content)\n",
    "#       但 Ollama 客戶端返回舊版字典格式 (response['message']['content'])\n",
    "# ============================================================\n",
    "\n",
    "from neo4j_graphrag.llm.ollama_llm import OllamaLLM\n",
    "from neo4j_graphrag.llm import LLMResponse\n",
    "\n",
    "# 保存原始方法\n",
    "_original_invoke = OllamaLLM.invoke\n",
    "\n",
    "def _patched_invoke(self, input, message_history=None, system_instruction=None):\n",
    "    \"\"\"\n",
    "    修補版本的 OllamaLLM.invoke 方法\n",
    "    兼容 Ollama 客戶端的字典返回格式\n",
    "    \"\"\"\n",
    "    # 處理 message_history (如果是對象則轉換為列表)\n",
    "    if message_history is not None and hasattr(message_history, 'messages'):\n",
    "        message_history = message_history.messages\n",
    "    \n",
    "    response = self.client.chat(\n",
    "        model=self.model_name,\n",
    "        messages=self.get_messages(input, message_history, system_instruction),\n",
    "        **self.model_params,\n",
    "    )\n",
    "    \n",
    "    # 🔧 關鍵修復：兼容字典和對象兩種格式\n",
    "    if isinstance(response, dict):\n",
    "        # 舊版 Ollama 返回字典格式\n",
    "        content = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "    elif hasattr(response, 'message'):\n",
    "        # 新版 Ollama 返回對象格式\n",
    "        content = response.message.content or \"\"\n",
    "    else:\n",
    "        # 容錯處理\n",
    "        content = str(response)\n",
    "    \n",
    "    return LLMResponse(content=content)\n",
    "\n",
    "# 應用 monkey patch\n",
    "OllamaLLM.invoke = _patched_invoke\n",
    "\n",
    "print(\"✅ 已修補 OllamaLLM.invoke 方法，支援 Ollama 字典響應格式\")\n",
    "print(\"   修復問題：'dict' object has no attribute 'message'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b73ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DRIVER.session() as session:\n",
    "    session.run(\"DROP INDEX chunk_embeddings IF EXISTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6593c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ValidateGraphIntegrity() 函式已載入（學術級專業版）\n"
     ]
    }
   ],
   "source": [
    "# 圖譜質量檢驗與完整度驗證（學術級專業版）\n",
    "\n",
    "def ValidateGraphIntegrity(\n",
    "    driver,\n",
    "    original_chunks: List[Dict[str, str]],\n",
    "    dataset_id: str,\n",
    "    sample_size: int = 10,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    執行 Graph RAG 知識圖譜的完整度與質量檢驗（基於學術與實務標準）。\n",
    "    \n",
    "    檢驗架構：\n",
    "    【第一組】結構與完整度檢驗 (Completeness & Structural Quality)\n",
    "       - 節點覆蓋率 (Node Coverage)\n",
    "       - 關係密度 (Relationship Density)\n",
    "       - 屬性填充率 (Property Fill Rate)\n",
    "       - 孤立節點比例 (Isolated Nodes Ratio)\n",
    "    \n",
    "    【第二組】一致性與類型檢查 (Consistency & Schema Adherence)\n",
    "       - 類型遵守率 (Schema Adherence)\n",
    "       - 重複實體檢測 (Duplication Check)\n",
    "       - 屬性合法性檢查 (Attribute Validity)\n",
    "    \n",
    "    【第三組】核心數據質量報告 (Accuracy & Provenance)\n",
    "       - 人工抽樣驗證 (Manual Sampling) - 10 個三元組\n",
    "       - 出處標註率 (Provenance Rate)\n",
    "    \n",
    "    Args:\n",
    "        driver: Neo4j GraphDatabase driver\n",
    "        original_chunks: 原始知識庫的文本區塊列表\n",
    "        dataset_id: 資料集識別符\n",
    "        sample_size: 人工抽樣三元組數量（預設 10）\n",
    "        verbose: 是否輸出詳細報告\n",
    "    \n",
    "    Returns:\n",
    "        包含所有檢驗結果與專家結論的字典\n",
    "    \n",
    "    參考文獻：\n",
    "        - Completeness metrics: Paulheim (2017), \"Knowledge Graph Refinement\"\n",
    "        - Quality dimensions: Zaveri et al. (2016), \"Quality Assessment for Linked Data\"\n",
    "    \"\"\"\n",
    "    validation_results = {\n",
    "        \"completeness_structural\": {},\n",
    "        \"consistency_schema\": {},\n",
    "        \"accuracy_provenance\": {},\n",
    "        \"overall_pass\": False,\n",
    "        \"quality_grade\": \"\",\n",
    "        \"expert_conclusion\": \"\",\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 100)\n",
    "        print(\"🔬 知識圖譜質量與完整度專業檢驗報告 (Academic-Grade KG Quality Assessment)\")\n",
    "        print(\"=\" * 100)\n",
    "        print(\"📚 檢驗標準：Paulheim (2017) + Zaveri et al. (2016)\")\n",
    "        print(\"=\" * 100)\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # ==========================================\n",
    "        # 【第一組】結構與完整度檢驗\n",
    "        # ==========================================\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(\"【第一組】結構與完整度檢驗 (Completeness & Structural Quality)\")\n",
    "            print(\"=\" * 100)\n",
    "        \n",
    "        # 1.1 節點覆蓋率 (Node Coverage)\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 1.1 | 節點覆蓋率 (Node Coverage)\")\n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        expected_chunk_count = len(original_chunks)\n",
    "        db_chunk_count = session.run(\n",
    "            \"MATCH (c:Chunk {dataset: $dataset}) RETURN count(c) AS cnt\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        total_chunk_count = session.run(\"MATCH (c:Chunk) RETURN count(c) AS cnt\").single()[\"cnt\"]\n",
    "        other_chunks = total_chunk_count - db_chunk_count\n",
    "        \n",
    "        node_coverage = (db_chunk_count / expected_chunk_count * 100) if expected_chunk_count > 0 else 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • 原始文本 Chunk 總數 (Expected)：{expected_chunk_count}\")\n",
    "            print(f\"  • Neo4j 當前 Dataset Chunk 數 (Actual)：{db_chunk_count}\")\n",
    "            print(f\"  • 節點覆蓋率 (Coverage Rate)：{node_coverage:.2f}%\")\n",
    "            if other_chunks > 0:\n",
    "                print(f\"  ⚠️ 警告：資料庫中有其他 dataset 的 {other_chunks} 個舊 Chunk\")\n",
    "            if node_coverage >= 100:\n",
    "                print(f\"  ✅ 評估：節點覆蓋率達標 (≥100%)\")\n",
    "            elif node_coverage >= 95:\n",
    "                print(f\"  ⚠️ 評估：節點覆蓋率可接受 (95-100%)\")\n",
    "            else:\n",
    "                print(f\"  ❌ 評估：節點覆蓋率不足 (<95%)\")\n",
    "        \n",
    "        # 1.2 關係密度 (Relationship Density)\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 1.2 | 關係密度 (Relationship Density)\")\n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        entity_count = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)\n",
    "            RETURN count(DISTINCT e) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        relation_count = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(h:Entity)-[r:RELATION]->(t:Entity)\n",
    "            RETURN count(DISTINCT r) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        relationship_density = (relation_count / entity_count) if entity_count > 0 else 0\n",
    "        \n",
    "        # 統計實體的關係分佈\n",
    "        # 注意：這裡統計的是每個實體參與的關係數（作為 head 或 tail）\n",
    "        # 由於關係是有向的，每條關係會被計入 head 和 tail 各一次\n",
    "        # 所以 avg_relations ≈ 2 * relationship_density（理論值）\n",
    "        entity_relation_stats = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)\n",
    "            OPTIONAL MATCH (e)-[r:RELATION]-()\n",
    "            WITH e, count(DISTINCT r) AS rel_count\n",
    "            RETURN \n",
    "                count(CASE WHEN rel_count = 0 THEN 1 END) AS isolated_entities,\n",
    "                count(CASE WHEN rel_count = 1 THEN 1 END) AS single_rel_entities,\n",
    "                count(CASE WHEN rel_count >= 2 AND rel_count < 5 THEN 1 END) AS moderate_rel_entities,\n",
    "                count(CASE WHEN rel_count >= 5 THEN 1 END) AS high_rel_entities,\n",
    "                avg(rel_count) AS avg_relations,\n",
    "                max(rel_count) AS max_relations\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()\n",
    "        \n",
    "        isolated_count = entity_relation_stats[\"isolated_entities\"] or 0\n",
    "        single_rel_count = entity_relation_stats[\"single_rel_entities\"] or 0\n",
    "        moderate_rel_count = entity_relation_stats[\"moderate_rel_entities\"] or 0\n",
    "        high_rel_count = entity_relation_stats[\"high_rel_entities\"] or 0\n",
    "        avg_rels = entity_relation_stats[\"avg_relations\"] or 0\n",
    "        max_rels = entity_relation_stats[\"max_relations\"] or 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • 實體節點總數 (Entity Nodes)：{entity_count}\")\n",
    "            print(f\"  • 關係總數 (Relations)：{relation_count}\")\n",
    "            print(f\"  • 關係密度 (Density = Relations/Entities)：{relationship_density:.4f}\")\n",
    "            print()\n",
    "            print(f\"  📊 實體連接度分佈：\")\n",
    "            print(f\"    • 孤立實體（0 個關係）：{isolated_count} ({isolated_count/entity_count*100:.1f}%)\")\n",
    "            print(f\"    • 弱連接實體（1 個關係）：{single_rel_count} ({single_rel_count/entity_count*100:.1f}%)\")\n",
    "            print(f\"    • 中度連接實體（2-4 個關係）：{moderate_rel_count} ({moderate_rel_count/entity_count*100:.1f}%)\")\n",
    "            print(f\"    • 高度連接實體（≥5 個關係）：{high_rel_count} ({high_rel_count/entity_count*100:.1f}%)\")\n",
    "            print()\n",
    "            print(f\"  📈 實體連接度統計：\")\n",
    "            print(f\"    • 平均每實體關係數（雙向計數）：{avg_rels:.2f}\")\n",
    "            print(f\"      └─ 說明：統計時每條關係被計入 head 和 tail 各 1 次\")\n",
    "            print(f\"      └─ 理論關係：平均關係數 ≈ 2 × 關係密度 = {relationship_density*2:.2f}\")\n",
    "            print(f\"      └─ 實際比值：{avg_rels/relationship_density if relationship_density > 0 else 0:.2f}x\")\n",
    "            print(f\"    • 最大連接度：{max_rels}\")\n",
    "            print()\n",
    "            \n",
    "            # 專家級評估\n",
    "            print(f\"  🔬 專家評估：\")\n",
    "            if relationship_density >= 2.0:\n",
    "                print(f\"    ✅ 關係密度優秀（≥2.0）\")\n",
    "                print(f\"       └─ 圖譜具備豐富的語義連通性，適合複雜推理任務\")\n",
    "            elif relationship_density >= 1.5:\n",
    "                print(f\"    ✅ 關係密度良好（1.5-2.0）\")\n",
    "                print(f\"       └─ 圖譜連通性充足，支持多跳查詢\")\n",
    "            elif relationship_density >= 1.0:\n",
    "                print(f\"    ⚠️ 關係密度中等（1.0-1.5）\")\n",
    "                print(f\"       └─ 基本滿足需求，但仍有改進空間\")\n",
    "            elif relationship_density >= 0.5:\n",
    "                print(f\"    ⚠️ 關係密度偏低（0.5-1.0）\")\n",
    "                print(f\"       └─ 連通性不足，多跳推理能力受限\")\n",
    "                print(f\"       └─ 建議：增強關係抽取深度和廣度\")\n",
    "            else:\n",
    "                print(f\"    ❌ 關係密度嚴重不足（<0.5）\")\n",
    "                print(f\"       └─ 圖譜幾乎呈孤立狀態，無法有效支持推理\")\n",
    "                print(f\"       └─ 緊急需求：全面優化三元組抽取策略\")\n",
    "            \n",
    "            print()\n",
    "            if isolated_count / entity_count > 0.3:\n",
    "                print(f\"    ⚠️ 孤立實體比例過高（{isolated_count/entity_count*100:.1f}%）\")\n",
    "                print(f\"       建議：檢查實體抽取是否過於寬泛，或關係抽取過於保守\")\n",
    "            \n",
    "            if single_rel_count / entity_count > 0.4:\n",
    "                print(f\"    ⚠️ 弱連接實體佔比過大（{single_rel_count/entity_count*100:.1f}%）\")\n",
    "                print(f\"       建議：增加屬性關係、時間關係、因果鏈等多維度關係\")\n",
    "            \n",
    "            if high_rel_count / entity_count < 0.1:\n",
    "                print(f\"    💡 缺乏核心樞紐節點（高連接度實體 < 10%）\")\n",
    "                print(f\"       建議：識別並強化領域核心概念的關係網絡\")\n",
    "        \n",
    "        # 1.3 屬性填充率 (Property Fill Rate)\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 1.3 | 屬性填充率 (Property Fill Rate)\")\n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        # 檢查 Entity 的 name 屬性填充率\n",
    "        entity_with_name = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)\n",
    "            WHERE e.name IS NOT NULL AND e.name <> ''\n",
    "            RETURN count(DISTINCT e) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        name_fill_rate = (entity_with_name / entity_count * 100) if entity_count > 0 else 0\n",
    "        \n",
    "        # 檢查 Chunk 的 text 屬性填充率\n",
    "        chunk_with_text = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})\n",
    "            WHERE c.text IS NOT NULL AND c.text <> ''\n",
    "            RETURN count(c) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        text_fill_rate = (chunk_with_text / db_chunk_count * 100) if db_chunk_count > 0 else 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • Entity.name 填充率：{name_fill_rate:.2f}% ({entity_with_name}/{entity_count})\")\n",
    "            print(f\"  • Chunk.text 填充率：{text_fill_rate:.2f}% ({chunk_with_text}/{db_chunk_count})\")\n",
    "            avg_fill_rate = (name_fill_rate + text_fill_rate) / 2\n",
    "            print(f\"  • 平均屬性填充率：{avg_fill_rate:.2f}%\")\n",
    "            if avg_fill_rate >= 95:\n",
    "                print(f\"  ✅ 評估：屬性填充率優秀 (≥95%)\")\n",
    "            elif avg_fill_rate >= 80:\n",
    "                print(f\"  ⚠️ 評估：屬性填充率良好 (80-95%)\")\n",
    "            else:\n",
    "                print(f\"  ❌ 評估：屬性填充率不足 (<80%)\")\n",
    "        \n",
    "        # 1.4 孤立節點比例 (Isolated Nodes Ratio)\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 1.4 | 孤立節點比例 (Isolated Nodes Ratio)\")\n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        isolated_chunks = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})\n",
    "            WHERE NOT (c)-[:MENTIONS]->(:Entity)\n",
    "            RETURN count(c) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        isolated_entities = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)\n",
    "            WHERE NOT (e)-[:RELATION]-()\n",
    "            RETURN count(DISTINCT e) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        isolated_chunk_ratio = (isolated_chunks / db_chunk_count * 100) if db_chunk_count > 0 else 0\n",
    "        isolated_entity_ratio = (isolated_entities / entity_count * 100) if entity_count > 0 else 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • 孤立 Chunk 數 (無 MENTIONS 連接)：{isolated_chunks} ({isolated_chunk_ratio:.2f}%)\")\n",
    "            print(f\"  • 孤立 Entity 數 (無 RELATION 連接)：{isolated_entities} ({isolated_entity_ratio:.2f}%)\")\n",
    "            if isolated_chunk_ratio <= 5 and isolated_entity_ratio <= 15:\n",
    "                print(f\"  ✅ 評估：孤立節點比例低，結構品質良好\")\n",
    "            elif isolated_chunk_ratio <= 10 and isolated_entity_ratio <= 30:\n",
    "                print(f\"  ⚠️ 評估：孤立節點比例中等，建議優化三元組抽取\")\n",
    "            else:\n",
    "                print(f\"  ❌ 評估：孤立節點比例偏高，可能影響知識推理能力\")\n",
    "        \n",
    "        # 儲存第一組檢驗結果\n",
    "        validation_results[\"completeness_structural\"] = {\n",
    "            \"node_coverage\": node_coverage,\n",
    "            \"relationship_density\": relationship_density,\n",
    "            \"avg_relations_per_entity\": avg_rels,\n",
    "            \"max_relations_per_entity\": max_rels,\n",
    "            \"property_fill_rate\": (name_fill_rate + text_fill_rate) / 2,\n",
    "            \"isolated_chunk_ratio\": isolated_chunk_ratio,\n",
    "            \"isolated_entity_ratio\": isolated_entity_ratio,\n",
    "            \"entity_connection_distribution\": {\n",
    "                \"isolated\": isolated_count,\n",
    "                \"single_relation\": single_rel_count,\n",
    "                \"moderate_relations\": moderate_rel_count,\n",
    "                \"high_relations\": high_rel_count,\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"expected_chunks\": expected_chunk_count,\n",
    "                \"db_chunks\": db_chunk_count,\n",
    "                \"entity_count\": entity_count,\n",
    "                \"relation_count\": relation_count,\n",
    "                \"isolated_chunks\": isolated_chunks,\n",
    "                \"isolated_entities\": isolated_entities,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # ==========================================\n",
    "        # 【第二組】一致性與類型檢查\n",
    "        # ==========================================\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(\"【第二組】一致性與類型檢查 (Consistency & Schema Adherence)\")\n",
    "            print(\"=\" * 100)\n",
    "        \n",
    "        # 2.1 類型遵守率 (Schema Adherence)\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 2.1 | 類型遵守率 (Schema Adherence)\")\n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        node_labels_result = session.run(\n",
    "            \"\"\"\n",
    "            CALL db.labels() YIELD label\n",
    "            RETURN collect(label) AS labels\n",
    "            \"\"\"\n",
    "        ).single()\n",
    "        node_labels = node_labels_result[\"labels\"] if node_labels_result else []\n",
    "        \n",
    "        relationship_types_result = session.run(\n",
    "            \"\"\"\n",
    "            CALL db.relationshipTypes() YIELD relationshipType\n",
    "            RETURN collect(relationshipType) AS types\n",
    "            \"\"\"\n",
    "        ).single()\n",
    "        relationship_types = relationship_types_result[\"types\"] if relationship_types_result else []\n",
    "        \n",
    "        # 檢查關係的語義有效性（檢查 RELATION.type 屬性，而非關係類型名稱）\n",
    "        # 統計不同的語義關係類型（從 r.type 屬性）\n",
    "        semantic_relations_result = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()\n",
    "            WHERE r.type IS NOT NULL AND r.type <> ''\n",
    "            RETURN DISTINCT r.type AS relation_type\n",
    "            ORDER BY relation_type\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).data()\n",
    "        semantic_relations = [row[\"relation_type\"] for row in semantic_relations_result]\n",
    "        \n",
    "        # 檢測過於寬泛的語義關係（在 r.type 屬性中）\n",
    "        generic_semantic_relations = [\n",
    "            rt for rt in semantic_relations \n",
    "            if rt.upper() in ['RELATION', 'RELATES_TO', 'CONNECTED_TO', 'ASSOCIATED_WITH', '關聯', '相關', '連接']\n",
    "        ]\n",
    "        \n",
    "        # 統計使用通用關係的數量\n",
    "        generic_relation_count = 0\n",
    "        if generic_semantic_relations:\n",
    "            generic_relation_count = session.run(\n",
    "                \"\"\"\n",
    "                MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()\n",
    "                WHERE r.type IN $generic_types\n",
    "                RETURN count(r) AS cnt\n",
    "                \"\"\",\n",
    "                dataset=dataset_id,\n",
    "                generic_types=generic_semantic_relations,\n",
    "            ).single()[\"cnt\"]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • 節點標籤 (Node Labels)：{', '.join(node_labels)}\")\n",
    "            print(f\"    └─ 共 {len(node_labels)} 種節點類型\")\n",
    "            print(f\"  • Neo4j 關係類型 (Relationship Types)：{', '.join(relationship_types)}\")\n",
    "            print(f\"    └─ 共 {len(relationship_types)} 種 Neo4j 關係類型\")\n",
    "            print(f\"  • 語義關係類型 (RELATION.type 屬性值)：{', '.join(semantic_relations[:20])}\")\n",
    "            if len(semantic_relations) > 20:\n",
    "                print(f\"    ... 以及其他 {len(semantic_relations) - 20} 種語義關係\")\n",
    "            print(f\"    └─ 共 {len(semantic_relations)} 種語義關係類型\")\n",
    "            \n",
    "            if generic_semantic_relations:\n",
    "                print(f\"  ⚠️ 警告：檢測到過於寬泛的語義關係類型：{', '.join(generic_semantic_relations)}\")\n",
    "                print(f\"     共有 {generic_relation_count} 個關係使用了通用語義\")\n",
    "                print(f\"     建議：優化提示詞以產生更具體的語義關係類型\")\n",
    "            \n",
    "            if len(node_labels) >= 2 and len(semantic_relations) >= 2:\n",
    "                print(f\"  ✅ 評估：圖譜類型結構完整，語義關係豐富\")\n",
    "            else:\n",
    "                print(f\"  ❌ 評估：圖譜類型結構不完整或語義關係過於單一\")\n",
    "        \n",
    "        # 2.2 重複實體檢測 (Duplication Check)\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 2.2 | 重複實體檢測 (Duplication Check)\")\n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        # 檢查是否有名稱完全相同的實體（可能表示重複）\n",
    "        duplicate_entities = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)\n",
    "            WITH e.name AS name, collect(DISTINCT e) AS entities\n",
    "            WHERE size(entities) > 1\n",
    "            RETURN count(*) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        # 抽樣潛在重複實體\n",
    "        duplicate_samples = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)\n",
    "            WITH e.name AS name, collect(DISTINCT id(e)) AS entity_ids\n",
    "            WHERE size(entity_ids) > 1\n",
    "            RETURN name, size(entity_ids) AS count\n",
    "            LIMIT 5\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).data()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • 檢測到的重複實體名稱數：{duplicate_entities}\")\n",
    "            if duplicate_samples:\n",
    "                print(f\"  • 抽樣範例：\")\n",
    "                for sample in duplicate_samples:\n",
    "                    print(f\"    - 「{sample['name']}」: {sample['count']} 個節點\")\n",
    "                print(f\"  ⚠️ 評估：存在重複實體，可能需要實體消歧（Entity Disambiguation）\")\n",
    "            else:\n",
    "                print(f\"  ✅ 評估：未檢測到明顯重複實體\")\n",
    "        \n",
    "        # 2.3 屬性合法性檢查 (Attribute Validity)\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 2.3 | 屬性合法性檢查 (Attribute Validity)\")\n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        # 檢查空值或無效值\n",
    "        invalid_relations = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()\n",
    "            WHERE r.type IS NULL OR r.type = ''\n",
    "            RETURN count(r) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • 無效關係數（type 屬性為空）：{invalid_relations}\")\n",
    "            if invalid_relations == 0:\n",
    "                print(f\"  ✅ 評估：所有關係屬性合法\")\n",
    "            else:\n",
    "                print(f\"  ❌ 評估：存在 {invalid_relations} 個無效關係，需要修正\")\n",
    "        \n",
    "        # 儲存第二組檢驗結果\n",
    "        validation_results[\"consistency_schema\"] = {\n",
    "            \"node_label_count\": len(node_labels),\n",
    "            \"relationship_type_count\": len(relationship_types),\n",
    "            \"semantic_relation_count\": len(semantic_relations),\n",
    "            \"node_labels\": node_labels,\n",
    "            \"relationship_types\": relationship_types,\n",
    "            \"semantic_relations\": semantic_relations,\n",
    "            \"generic_semantic_relations\": generic_semantic_relations,\n",
    "            \"generic_relation_count\": generic_relation_count,\n",
    "            \"duplicate_entities\": duplicate_entities,\n",
    "            \"duplicate_samples\": duplicate_samples,\n",
    "            \"invalid_relations\": invalid_relations,\n",
    "        }\n",
    "        \n",
    "        # ==========================================\n",
    "        # 【第三組】核心數據質量報告\n",
    "        # ==========================================\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\" * 100)\n",
    "            print(\"【第三組】核心數據質量報告 (Accuracy & Provenance)\")\n",
    "            print(\"=\" * 100)\n",
    "        \n",
    "        # 3.1 人工抽樣驗證 (Manual Sampling) - 10 個三元組\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 3.1 | 人工抽樣驗證 (Manual Sampling for Accuracy)\")\n",
    "            print(\"-\" * 100)\n",
    "            print(f\"  隨機抽取 {sample_size} 個三元組，請進行人工語義正確性檢查：\")\n",
    "            print()\n",
    "        \n",
    "        sampled_triples = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(h:Entity)-[r:RELATION]->(t:Entity)\n",
    "            WITH h, r, t, rand() AS random\n",
    "            ORDER BY random\n",
    "            LIMIT $limit\n",
    "            RETURN h.name AS head, r.type AS relation, t.name AS tail\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "            limit=sample_size,\n",
    "        ).data()\n",
    "        \n",
    "        if verbose:\n",
    "            if sampled_triples:\n",
    "                for idx, triple in enumerate(sampled_triples, start=1):\n",
    "                    head = triple.get(\"head\", \"N/A\")\n",
    "                    relation = triple.get(\"relation\", \"N/A\")\n",
    "                    tail = triple.get(\"tail\", \"N/A\")\n",
    "                    print(f\"  [{idx:2d}] ({head}) --[{relation}]--> ({tail})\")\n",
    "                print()\n",
    "                print(\"  \" + \"=\" * 96)\n",
    "                print(\"  💡 人工檢查指引 (Manual Verification Guidelines)\")\n",
    "                print(\"  \" + \"=\" * 96)\n",
    "                print(\"  請針對以上三元組逐一評估以下三個維度：\")\n",
    "                print()\n",
    "                print(\"  ✓ 語義正確性 (Semantic Correctness)\")\n",
    "                print(\"    └─ 實體名稱是否正確且有意義？\")\n",
    "                print(\"    └─ 關係類型是否準確描述兩實體間的語義關聯？\")\n",
    "                print()\n",
    "                print(\"  ✓ 邏輯一致性 (Logical Consistency)\")\n",
    "                print(\"    └─ 三元組的邏輯是否符合真實世界或原始知識庫內容？\")\n",
    "                print(\"    └─ Head 和 Tail 的實體類型是否與 Relation 相容？\")\n",
    "                print()\n",
    "                print(\"  ✓ 資訊完整性 (Information Completeness)\")\n",
    "                print(\"    └─ 三元組是否包含足夠的上下文資訊？\")\n",
    "                print(\"    └─ 是否有明顯的資訊缺失或歧義？\")\n",
    "                print()\n",
    "                print(\"  📝 建議：記錄有問題的三元組編號，用於後續優化提示詞或知識抽取流程。\")\n",
    "                print(\"  \" + \"=\" * 96)\n",
    "            else:\n",
    "                print(\"  ❌ 警告：無法抽取三元組，圖譜中可能沒有有效的 RELATION 關係\")\n",
    "        \n",
    "        # 3.2 出處標註率 (Provenance Rate)\n",
    "        if verbose:\n",
    "            print(\"\\n📊 指標 3.2 | 出處標註率 (Provenance Rate)\")\n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        # 檢查 Chunk 的 source 屬性填充率\n",
    "        chunks_with_source = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})\n",
    "            WHERE c.source IS NOT NULL AND c.source <> ''\n",
    "            RETURN count(c) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        provenance_rate = (chunks_with_source / db_chunk_count * 100) if db_chunk_count > 0 else 0\n",
    "        \n",
    "        # 檢查是否有 RELATION 包含來源 chunks 資訊\n",
    "        relations_with_chunks = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()\n",
    "            WHERE r.chunks IS NOT NULL AND size(r.chunks) > 0\n",
    "            RETURN count(DISTINCT r) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        \n",
    "        relation_provenance_rate = (relations_with_chunks / relation_count * 100) if relation_count > 0 else 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  • Chunk 出處標註率 (source 屬性)：{provenance_rate:.2f}% ({chunks_with_source}/{db_chunk_count})\")\n",
    "            print(f\"  • Relation 來源追溯率 (chunks 屬性)：{relation_provenance_rate:.2f}% ({relations_with_chunks}/{relation_count})\")\n",
    "            avg_provenance = (provenance_rate + relation_provenance_rate) / 2\n",
    "            print(f\"  • 平均出處標註率：{avg_provenance:.2f}%\")\n",
    "            if avg_provenance >= 90:\n",
    "                print(f\"  ✅ 評估：出處標註率優秀，知識溯源性高\")\n",
    "            elif avg_provenance >= 70:\n",
    "                print(f\"  ⚠️ 評估：出處標註率良好，建議進一步提升\")\n",
    "            else:\n",
    "                print(f\"  ❌ 評估：出處標註率不足，可能影響知識可信度\")\n",
    "        \n",
    "        # 儲存第三組檢驗結果\n",
    "        validation_results[\"accuracy_provenance\"] = {\n",
    "            \"sampled_triples\": sampled_triples,\n",
    "            \"sample_size\": len(sampled_triples),\n",
    "            \"provenance_rate\": provenance_rate,\n",
    "            \"relation_provenance_rate\": relation_provenance_rate,\n",
    "            \"avg_provenance\": (provenance_rate + relation_provenance_rate) / 2,\n",
    "        }\n",
    "    \n",
    "    # ==========================================\n",
    "    # 【最終專家結論】綜合質量評估\n",
    "    # ==========================================\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"【最終專家結論】綜合質量評估 (Overall Quality Grade & Expert Conclusion)\")\n",
    "        print(\"=\" * 100)\n",
    "    \n",
    "    # 計算各組指標的分數\n",
    "    comp_struct = validation_results[\"completeness_structural\"]\n",
    "    consist_schema = validation_results[\"consistency_schema\"]\n",
    "    acc_prov = validation_results[\"accuracy_provenance\"]\n",
    "    \n",
    "    # 評分邏輯（基於學術標準）\n",
    "    score_completeness = 0\n",
    "    if comp_struct[\"node_coverage\"] >= 100:\n",
    "        score_completeness += 25\n",
    "    elif comp_struct[\"node_coverage\"] >= 95:\n",
    "        score_completeness += 20\n",
    "    \n",
    "    if comp_struct[\"relationship_density\"] >= 0.5:\n",
    "        score_completeness += 25\n",
    "    elif comp_struct[\"relationship_density\"] >= 0.2:\n",
    "        score_completeness += 15\n",
    "    \n",
    "    if comp_struct[\"property_fill_rate\"] >= 95:\n",
    "        score_completeness += 25\n",
    "    elif comp_struct[\"property_fill_rate\"] >= 80:\n",
    "        score_completeness += 20\n",
    "    \n",
    "    if comp_struct[\"isolated_chunk_ratio\"] <= 5 and comp_struct[\"isolated_entity_ratio\"] <= 15:\n",
    "        score_completeness += 25\n",
    "    elif comp_struct[\"isolated_chunk_ratio\"] <= 10 and comp_struct[\"isolated_entity_ratio\"] <= 30:\n",
    "        score_completeness += 15\n",
    "    \n",
    "    score_consistency = 0\n",
    "    if consist_schema[\"node_label_count\"] >= 2 and consist_schema[\"semantic_relation_count\"] >= 5:\n",
    "        score_consistency += 30\n",
    "    elif consist_schema[\"semantic_relation_count\"] >= 2:\n",
    "        score_consistency += 15\n",
    "    \n",
    "    # 檢查語義關係的質量（無通用關係 = 滿分，否則按比例扣分）\n",
    "    if consist_schema[\"generic_relation_count\"] == 0:\n",
    "        score_consistency += 30\n",
    "    else:\n",
    "        # 按照通用關係比例扣分\n",
    "        total_relations = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()\n",
    "            RETURN count(r) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=dataset_id,\n",
    "        ).single()[\"cnt\"]\n",
    "        if total_relations > 0:\n",
    "            generic_ratio = consist_schema[\"generic_relation_count\"] / total_relations\n",
    "            if generic_ratio < 0.1:  # 小於 10% 的通用關係\n",
    "                score_consistency += 25\n",
    "            elif generic_ratio < 0.3:  # 小於 30% 的通用關係\n",
    "                score_consistency += 15\n",
    "            elif generic_ratio < 0.5:  # 小於 50% 的通用關係\n",
    "                score_consistency += 5\n",
    "    \n",
    "    if consist_schema[\"duplicate_entities\"] == 0:\n",
    "        score_consistency += 20\n",
    "    if consist_schema[\"invalid_relations\"] == 0:\n",
    "        score_consistency += 20\n",
    "    \n",
    "    score_accuracy = 0\n",
    "    if acc_prov[\"sample_size\"] >= sample_size:\n",
    "        score_accuracy += 50\n",
    "    if acc_prov[\"avg_provenance\"] >= 90:\n",
    "        score_accuracy += 50\n",
    "    elif acc_prov[\"avg_provenance\"] >= 70:\n",
    "        score_accuracy += 35\n",
    "    \n",
    "    total_score = (score_completeness + score_consistency + score_accuracy) / 3\n",
    "    \n",
    "    # 質量等級判定\n",
    "    if total_score >= 85:\n",
    "        quality_grade = \"優秀 (Excellent)\"\n",
    "        grade_emoji = \"🏆\"\n",
    "        fitness_status = \"高質量\"\n",
    "    elif total_score >= 70:\n",
    "        quality_grade = \"良好 (Good)\"\n",
    "        grade_emoji = \"✅\"\n",
    "        fitness_status = \"中高質量\"\n",
    "    elif total_score >= 55:\n",
    "        quality_grade = \"中等 (Fair)\"\n",
    "        grade_emoji = \"⚠️\"\n",
    "        fitness_status = \"中等質量\"\n",
    "    else:\n",
    "        quality_grade = \"需改進 (Poor)\"\n",
    "        grade_emoji = \"❌\"\n",
    "        fitness_status = \"低質量\"\n",
    "    \n",
    "    # 找出最弱指標\n",
    "    weakest_metrics = []\n",
    "    if comp_struct[\"relationship_density\"] < 0.2:\n",
    "        weakest_metrics.append(\"關係密度偏低 (影響多跳推理)\")\n",
    "    if comp_struct[\"isolated_entity_ratio\"] > 30:\n",
    "        weakest_metrics.append(\"孤立節點比例過高 (影響知識連通性)\")\n",
    "    if comp_struct[\"property_fill_rate\"] < 80:\n",
    "        weakest_metrics.append(\"屬性填充率不足 (影響資訊完整性)\")\n",
    "    if consist_schema[\"generic_semantic_relations\"]:\n",
    "        weakest_metrics.append(f\"存在過於寬泛的語義關係 ({', '.join(consist_schema['generic_semantic_relations'][:5])})\")\n",
    "    if acc_prov[\"avg_provenance\"] < 70:\n",
    "        weakest_metrics.append(\"出處標註率偏低 (影響知識溯源性)\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n  {grade_emoji} 質量等級：{quality_grade}\")\n",
    "        print(f\"  📊 綜合評分：{total_score:.1f}/100\")\n",
    "        print()\n",
    "        print(f\"  分項評分：\")\n",
    "        print(f\"    • 完整度與結構 (Completeness & Structure)：{score_completeness:.1f}/100\")\n",
    "        print(f\"    • 一致性與類型 (Consistency & Schema)：{score_consistency:.1f}/100\")\n",
    "        print(f\"    • 準確性與溯源 (Accuracy & Provenance)：{score_accuracy:.1f}/100\")\n",
    "        print()\n",
    "        print(\"  \" + \"=\" * 96)\n",
    "        print(\"  📋 專家結論 (Expert Conclusion)\")\n",
    "        print(\"  \" + \"=\" * 96)\n",
    "        print()\n",
    "        print(f\"  根據 (i) 關係密度 ({comp_struct['relationship_density']:.4f})、\")\n",
    "        print(f\"       (ii) 孤立節點比率 (Chunks: {comp_struct['isolated_chunk_ratio']:.2f}%, Entities: {comp_struct['isolated_entity_ratio']:.2f}%)、\")\n",
    "        print(f\"       (iii) 屬性填充率 ({comp_struct['property_fill_rate']:.2f}%)、\")\n",
    "        print(f\"       (iv) 語義關係豐富度 ({consist_schema['semantic_relation_count']} 種)、\")\n",
    "        print(f\"       (v) 平均連接度 (每實體 {comp_struct['avg_relations_per_entity']:.2f} 個關係) 的數據，\")\n",
    "        print()\n",
    "        print(f\"  本圖譜已達到【{fitness_status}】標準，{'可' if total_score >= 70 else '暫不建議'}投入 Graph RAG 系統使用。\")\n",
    "        print()\n",
    "        if weakest_metrics:\n",
    "            print(f\"  ⚠️ 需注意的指標：\")\n",
    "            for metric in weakest_metrics:\n",
    "                print(f\"     • {metric}\")\n",
    "            print()\n",
    "            print(f\"  💡 改進建議：\")\n",
    "            \n",
    "            # 關係密度專項建議\n",
    "            if comp_struct[\"relationship_density\"] < 1.5:\n",
    "                print(f\"     📊 關係密度提升策略（當前：{comp_struct['relationship_density']:.2f}，目標：≥1.5）：\")\n",
    "                print(f\"        ├─ 🔧 增強抽取深度：\")\n",
    "                print(f\"        │  • 擴展關係類型：屬性關係（數值為、濃度為）、時間關係（發生於、持續）\")\n",
    "                print(f\"        │  • 挖掘隱式關係：因果鏈（A→B→C）、共現關係、層級關係\")\n",
    "                print(f\"        │  • 實施共指消解：將「它」「該物質」還原為具體實體名，增加實體複用\")\n",
    "                print(f\"        ├─ 🎯 優化提示詞：\")\n",
    "                print(f\"        │  • 明確要求「每個實體至少 2 個關係」\")\n",
    "                print(f\"        │  • 提供多維度關係範例（因果、屬性、功能、時間）\")\n",
    "                print(f\"        │  • 增加「從不同角度描述實體」的指令\")\n",
    "                print(f\"        └─ 🧪 後處理增強：\")\n",
    "                print(f\"           • 知識圖譜補全（Link Prediction）：TransE/RotatE 預測缺失關係\")\n",
    "                print(f\"           • 實體合併：識別同義實體（如「維生素A」vs「視黃醇」）\")\n",
    "                print(f\"           • 關係推理：基於規則的傳遞閉包（如 A包含B, B含有C → A間接含有C）\")\n",
    "                print()\n",
    "            \n",
    "            if comp_struct[\"isolated_entity_ratio\"] > 30:\n",
    "                print(f\"     • 孤立實體過多（{comp_struct['isolated_entity_ratio']:.1f}%）：\")\n",
    "                print(f\"       └─ 可能原因：實體粒度過細、關係抽取過於保守\")\n",
    "                print(f\"       └─ 建議：提高實體抽象層級，或增加實體間的弱關係（如共現、上下位）\")\n",
    "                print()\n",
    "            \n",
    "            if consist_schema[\"generic_semantic_relations\"]:\n",
    "                print(f\"     • 存在通用語義關係：請使用具體動詞（導致、含有、影響）替代模糊詞（關聯、相關）\")\n",
    "                print()\n",
    "            \n",
    "            if acc_prov[\"avg_provenance\"] < 70:\n",
    "                print(f\"     • 出處標註率偏低：確保所有關係包含來源追溯資訊（chunks 屬性）\")\n",
    "        else:\n",
    "            print(f\"  ✅ 所有核心指標均達到優良標準，圖譜質量卓越！\")\n",
    "        print()\n",
    "        print(\"  \" + \"=\" * 96)\n",
    "        print(f\"  📚 參考文獻與進階技術：\")\n",
    "        print(f\"     • Paulheim, H. (2017). Knowledge graph refinement: A survey of approaches.\")\n",
    "        print(f\"     • Zaveri, A., et al. (2016). Quality assessment for linked data.\")\n",
    "        print(f\"     • TransE/RotatE: 知識圖譜嵌入模型，用於鏈接預測與補全\")\n",
    "        print(f\"     • Coreference Resolution: 共指消解技術，提升實體複用率\")\n",
    "        print(\"  \" + \"=\" * 96)\n",
    "    \n",
    "    validation_results[\"overall_pass\"] = (total_score >= 70)\n",
    "    validation_results[\"quality_grade\"] = quality_grade\n",
    "    validation_results[\"total_score\"] = total_score\n",
    "    validation_results[\"score_breakdown\"] = {\n",
    "        \"completeness_structural\": score_completeness,\n",
    "        \"consistency_schema\": score_consistency,\n",
    "        \"accuracy_provenance\": score_accuracy,\n",
    "    }\n",
    "    validation_results[\"weakest_metrics\"] = weakest_metrics\n",
    "    validation_results[\"expert_conclusion\"] = f\"本圖譜達到【{fitness_status}】標準（評分：{total_score:.1f}/100）\"\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "\n",
    "print(\"✅ ValidateGraphIntegrity() 函式已載入（學術級專業版）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e91bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ clean_database() 函式已載入\n",
      "\n",
      "💡 使用方式：\n",
      "   # 清理特定 dataset 的資料（推薦）\n",
      "   clean_database(GRAPH_DRIVER, 'goat_data_text_collection-1.2-eng', clean_all=False)\n",
      "\n",
      "   # 清理所有資料（謹慎使用！）\n",
      "   clean_database(GRAPH_DRIVER, '', clean_all=True)\n"
     ]
    }
   ],
   "source": [
    "# 資料庫清理與初始化（可選）\n",
    "\n",
    "def clean_database(driver, dataset_id: str, clean_all: bool = False) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    清理 Neo4j 資料庫中的舊資料。\n",
    "    \n",
    "    Args:\n",
    "        driver: Neo4j GraphDatabase driver\n",
    "        dataset_id: 要清理的資料集 ID\n",
    "        clean_all: 若為 True，清理所有資料；否則僅清理指定 dataset_id 的資料\n",
    "    \n",
    "    Returns:\n",
    "        刪除的節點和關係統計\n",
    "    \"\"\"\n",
    "    with driver.session() as session:\n",
    "        if clean_all:\n",
    "            print(\"🗑️ 清理所有資料...\")\n",
    "            # 刪除所有節點和關係\n",
    "            deleted_relations = session.run(\"MATCH ()-[r]->() DELETE r RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "            deleted_nodes = session.run(\"MATCH (n) DELETE n RETURN count(n) AS cnt\").single()[\"cnt\"]\n",
    "            print(f\"  ✅ 已刪除 {deleted_nodes} 個節點, {deleted_relations} 個關係\")\n",
    "        else:\n",
    "            print(f\"🗑️ 清理 dataset_id = '{dataset_id}' 的資料...\")\n",
    "            \n",
    "            # 刪除與指定 dataset 相關的 Chunk 節點及其關係\n",
    "            deleted_mentions = session.run(\n",
    "                \"\"\"\n",
    "                MATCH (c:Chunk {dataset: $dataset})-[m:MENTIONS]->(:Entity)\n",
    "                DELETE m\n",
    "                RETURN count(m) AS cnt\n",
    "                \"\"\",\n",
    "                dataset=dataset_id,\n",
    "            ).single()[\"cnt\"]\n",
    "            \n",
    "            # 清理孤立的 Entity 和 RELATION\n",
    "            deleted_relations = session.run(\n",
    "                \"\"\"\n",
    "                MATCH (e:Entity)\n",
    "                WHERE NOT (e)<-[:MENTIONS]-(:Chunk)\n",
    "                MATCH (e)-[r:RELATION]-()\n",
    "                DELETE r\n",
    "                RETURN count(r) AS cnt\n",
    "                \"\"\"\n",
    "            ).single()[\"cnt\"]\n",
    "            \n",
    "            deleted_entities = session.run(\n",
    "                \"\"\"\n",
    "                MATCH (e:Entity)\n",
    "                WHERE NOT (e)<-[:MENTIONS]-(:Chunk)\n",
    "                  AND NOT (e)-[:RELATION]-()\n",
    "                DELETE e\n",
    "                RETURN count(e) AS cnt\n",
    "                \"\"\"\n",
    "            ).single()[\"cnt\"]\n",
    "            \n",
    "            deleted_chunks = session.run(\n",
    "                \"\"\"\n",
    "                MATCH (c:Chunk {dataset: $dataset})\n",
    "                DELETE c\n",
    "                RETURN count(c) AS cnt\n",
    "                \"\"\",\n",
    "                dataset=dataset_id,\n",
    "            ).single()[\"cnt\"]\n",
    "            \n",
    "            print(f\"  ✅ 已刪除 {deleted_chunks} 個 Chunks\")\n",
    "            print(f\"  ✅ 已刪除 {deleted_mentions} 個 MENTIONS 關係\")\n",
    "            print(f\"  ✅ 已刪除 {deleted_entities} 個孤立 Entities\")\n",
    "            print(f\"  ✅ 已刪除 {deleted_relations} 個孤立 RELATIONS\")\n",
    "    \n",
    "    return {\n",
    "        \"deleted_chunks\": deleted_chunks if not clean_all else deleted_nodes,\n",
    "        \"deleted_mentions\": deleted_mentions if not clean_all else deleted_relations,\n",
    "        \"deleted_entities\": deleted_entities if not clean_all else 0,\n",
    "        \"deleted_relations\": deleted_relations if not clean_all else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✅ clean_database() 函式已載入\")\n",
    "print()\n",
    "print(\"💡 使用方式：\")\n",
    "print(\"   # 清理特定 dataset 的資料（推薦）\")\n",
    "print(f\"   clean_database(GRAPH_DRIVER, '{DATASET_ID}', clean_all=False)\")\n",
    "print()\n",
    "print(\"   # 清理所有資料（謹慎使用！）\")\n",
    "print(\"   clean_database(GRAPH_DRIVER, '', clean_all=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6aad3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 準備清理當前 dataset 的資料...\n",
      "   Dataset ID: goat_data_text_collection-1.2-eng\n",
      "\n",
      "🗑️ 清理所有資料...\n",
      "  ✅ 已刪除 0 個節點, 0 個關係\n",
      "\n",
      "✅ 清理完成！現在可以執行圖譜建立 Cell 重新開始。\n"
     ]
    }
   ],
   "source": [
    "# 🚀 快速清理當前 Dataset 資料（執行此 Cell 以清理舊資料）\n",
    "# 僅在需要重新開始時執行此 Cell\n",
    "print(\"⚠️ 準備清理當前 dataset 的資料...\")\n",
    "print(f\"   Dataset ID: {DATASET_ID}\")\n",
    "print()\n",
    "\n",
    "user_confirm = input(\"確定要清理嗎？(輸入 yes 繼續，其他任意鍵取消): \")\n",
    "\n",
    "if user_confirm.lower() == \"yes\":\n",
    "    clean_database(DRIVER, DATASET_ID, clean_all=True)\n",
    "    print()\n",
    "    print(\"✅ 清理完成！現在可以執行圖譜建立 Cell 重新開始。\")\n",
    "else:\n",
    "    print(\"❌ 已取消清理操作。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a054b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "🔍 資料庫狀態診斷報告\n",
      "====================================================================================================\n",
      "\n",
      "📊 當前狀態：\n",
      "  • 當前 Dataset (goat_data_text_collection-1.2-eng)：\n",
      "    └─ Chunks: 0\n",
      "    └─ Entities: 0\n",
      "\n",
      "  • 資料庫總計：\n",
      "    └─ 所有 Chunks: 0\n",
      "    └─ 所有 Entities: 0\n",
      "\n",
      "✅ 資料庫狀態良好！沒有發現需要清理的問題。\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 🔍 資料庫狀態診斷工具（執行此 Cell 以檢查是否需要清理舊資料）\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"🔍 資料庫狀態診斷報告\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "with DRIVER.session() as session:\n",
    "    # 統計當前 dataset 的資料\n",
    "    current_chunks = session.run(\n",
    "        \"MATCH (c:Chunk {dataset: $dataset}) RETURN count(c) AS cnt\",\n",
    "        dataset=DATASET_ID\n",
    "    ).single()[\"cnt\"]\n",
    "    \n",
    "    current_entities = session.run(\n",
    "        \"\"\"\n",
    "        MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)\n",
    "        RETURN count(DISTINCT e) AS cnt\n",
    "        \"\"\",\n",
    "        dataset=DATASET_ID\n",
    "    ).single()[\"cnt\"]\n",
    "    \n",
    "    # 統計所有資料\n",
    "    total_chunks = session.run(\"MATCH (c:Chunk) RETURN count(c) AS cnt\").single()[\"cnt\"]\n",
    "    total_entities = session.run(\"MATCH (e:Entity) RETURN count(e) AS cnt\").single()[\"cnt\"]\n",
    "    \n",
    "    # 統計其他 dataset 的資料\n",
    "    other_chunks = total_chunks - current_chunks\n",
    "    \n",
    "    # 統計孤立節點\n",
    "    orphan_entities = session.run(\n",
    "        \"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)<-[:MENTIONS]-(:Chunk)\n",
    "        RETURN count(e) AS cnt\n",
    "        \"\"\"\n",
    "    ).single()[\"cnt\"]\n",
    "    \n",
    "    # 檢查關係類型（注意：這裡檢查的是 r.type 屬性，而非關係類型名稱）\n",
    "    relation_types = session.run(\n",
    "        \"\"\"\n",
    "        CALL db.relationshipTypes() YIELD relationshipType\n",
    "        RETURN collect(relationshipType) AS types\n",
    "        \"\"\"\n",
    "    ).single()[\"types\"]\n",
    "    \n",
    "    # 檢查語義關係（RELATION.type 屬性值）\n",
    "    semantic_relations = session.run(\n",
    "        \"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        WHERE r.type IS NOT NULL AND r.type <> ''\n",
    "        RETURN DISTINCT r.type AS relation_type\n",
    "        ORDER BY relation_type\n",
    "        \"\"\"\n",
    "    ).data()\n",
    "    \n",
    "    semantic_relation_list = [row[\"relation_type\"] for row in semantic_relations]\n",
    "    \n",
    "    # 檢測過於寬泛的語義關係\n",
    "    generic_relations = [\n",
    "        rt for rt in semantic_relation_list \n",
    "        if rt.upper() in ['RELATION', 'RELATES_TO', 'CONNECTED_TO', 'ASSOCIATED_WITH', '關聯', '相關', '連接']\n",
    "    ]\n",
    "    \n",
    "    # 統計使用通用關係的數量\n",
    "    generic_relation_count = 0\n",
    "    if generic_relations:\n",
    "        generic_relation_count = session.run(\n",
    "            \"\"\"\n",
    "            MATCH ()-[r:RELATION]->()\n",
    "            WHERE r.type IN $generic_types\n",
    "               OR r.type IS NULL OR r.type = ''\n",
    "            RETURN count(r) AS cnt\n",
    "            \"\"\",\n",
    "            generic_types=generic_relations,\n",
    "        ).single()[\"cnt\"]\n",
    "\n",
    "print()\n",
    "print(f\"📊 當前狀態：\")\n",
    "print(f\"  • 當前 Dataset ({DATASET_ID})：\")\n",
    "print(f\"    └─ Chunks: {current_chunks}\")\n",
    "print(f\"    └─ Entities: {current_entities}\")\n",
    "print()\n",
    "print(f\"  • 資料庫總計：\")\n",
    "print(f\"    └─ 所有 Chunks: {total_chunks}\")\n",
    "print(f\"    └─ 所有 Entities: {total_entities}\")\n",
    "print()\n",
    "\n",
    "# 判斷是否需要清理\n",
    "issues_found = []\n",
    "\n",
    "if other_chunks > 0:\n",
    "    issues_found.append(f\"發現 {other_chunks} 個其他 dataset 的舊 Chunks\")\n",
    "    print(f\"⚠️  警告：資料庫中有 {other_chunks} 個其他 dataset 的舊 Chunks\")\n",
    "    print(f\"   建議：執行下方的「快速清理」Cell 清除舊資料\")\n",
    "    print()\n",
    "\n",
    "if orphan_entities > 0:\n",
    "    issues_found.append(f\"發現 {orphan_entities} 個孤立 Entity 節點\")\n",
    "    print(f\"⚠️  警告：資料庫中有 {orphan_entities} 個孤立 Entity 節點（未連接到任何 Chunk）\")\n",
    "    print(f\"   建議：執行清理以移除這些無效節點\")\n",
    "    print()\n",
    "\n",
    "if generic_relations:\n",
    "    issues_found.append(f\"發現 {len(generic_relations)} 種過於寬泛的語義關係：{', '.join(generic_relations)}\")\n",
    "    print(f\"⚠️  警告：檢測到過於寬泛的語義關係（在 RELATION.type 屬性中）：{', '.join(generic_relations)}\")\n",
    "    if generic_relation_count > 0:\n",
    "        print(f\"   共有 {generic_relation_count} 個關係使用了通用語義\")\n",
    "    print(f\"   說明：\")\n",
    "    print(f\"     • Neo4j 關係類型 'RELATION' 是正確的（固定使用）\")\n",
    "    print(f\"     • 問題在於 r.type 屬性值過於寬泛（如 '關聯'、'相關'）\")\n",
    "    print(f\"   建議：\")\n",
    "    print(f\"     1. 清理當前圖譜資料\")\n",
    "    print(f\"     2. 使用改進的提示詞重新建立圖譜\")\n",
    "    print(f\"     3. 新提示詞會產生具體語義（如：導致、缺乏、含有、影響）\")\n",
    "    print()\n",
    "\n",
    "if not issues_found:\n",
    "    print(\"✅ 資料庫狀態良好！沒有發現需要清理的問題。\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"=\" * 100)\n",
    "    print(\"📋 問題總結：\")\n",
    "    for idx, issue in enumerate(issues_found, 1):\n",
    "        print(f\"  {idx}. {issue}\")\n",
    "    print()\n",
    "    print(\"💡 建議操作：\")\n",
    "    print(f\"  1️⃣  執行下方的「快速清理」Cell\")\n",
    "    print(f\"  2️⃣  重新執行「圖譜建立」Cell（會使用改進的提示詞）\")\n",
    "    print(f\"  3️⃣  檢查驗證報告，確認問題已解決\")\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea250d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔌 Connecting to Neo4j ...\n",
      "🤖 Connecting to Ollama ...\n",
      "📚 Loading and chunking knowledge base ...\n",
      "  ↳ Prepared 452 chunks (size=2048, overlap=512)\n",
      "🧮 Ensuring vector index ...\n",
      "📝 Ensuring fulltext index ...\n",
      "  ↳ Fulltext index ready: True\n",
      "⬆️ Upserting chunks into Neo4j ...\n",
      "  ↳ Upserted 452, skipped 0 unchanged chunks\n",
      "🔗 Extracting triples and updating knowledge graph ...\n",
      "  ↳ Triples ingested for 441 chunks (skipped 11)\n",
      "  ⚠️ No triples extracted for 11 chunks → ['goat_data_text_collection-1.2-eng_chunk_00072', 'goat_data_text_collection-1.2-eng_chunk_00164', 'goat_data_text_collection-1.2-eng_chunk_00190', 'goat_data_text_collection-1.2-eng_chunk_00236', 'goat_data_text_collection-1.2-eng_chunk_00262']...\n",
      "📊 Graph summary → Chunks: 452, Entities: 10878, Relations: 9885, Orphan chunks: 11\n",
      "  ⚠️ Some chunks lack entity links; consider refining extraction prompts or chunk size.\n",
      "\n",
      "================================================================================\n",
      "🔬 執行圖譜完整度與質量檢驗\n",
      "================================================================================\n",
      "====================================================================================================\n",
      "🔬 知識圖譜質量與完整度專業檢驗報告 (Academic-Grade KG Quality Assessment)\n",
      "====================================================================================================\n",
      "📚 檢驗標準：Paulheim (2017) + Zaveri et al. (2016)\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "【第一組】結構與完整度檢驗 (Completeness & Structural Quality)\n",
      "====================================================================================================\n",
      "\n",
      "📊 指標 1.1 | 節點覆蓋率 (Node Coverage)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  • 原始文本 Chunk 總數 (Expected)：452\n",
      "  • Neo4j 當前 Dataset Chunk 數 (Actual)：452\n",
      "  • 節點覆蓋率 (Coverage Rate)：100.00%\n",
      "  ⚠️ 警告：資料庫中有其他 dataset 的 452 個舊 Chunk\n",
      "  ✅ 評估：節點覆蓋率達標 (≥100%)\n",
      "\n",
      "📊 指標 1.2 | 關係密度 (Relationship Density)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  • 實體節點總數 (Entity Nodes)：5818\n",
      "  • 關係總數 (Relations)：7431\n",
      "  • 關係密度 (Density = Relations/Entities)：1.2772\n",
      "\n",
      "  📊 實體連接度分佈：\n",
      "    • 孤立實體（0 個關係）：0 (0.0%)\n",
      "    • 弱連接實體（1 個關係）：4219 (72.5%)\n",
      "    • 中度連接實體（2-4 個關係）：1152 (19.8%)\n",
      "    • 高度連接實體（≥5 個關係）：447 (7.7%)\n",
      "\n",
      "  📈 實體連接度統計：\n",
      "    • 平均每實體關係數（雙向計數）：2.31\n",
      "      └─ 說明：統計時每條關係被計入 head 和 tail 各 1 次\n",
      "      └─ 理論關係：平均關係數 ≈ 2 × 關係密度 = 2.55\n",
      "      └─ 實際比值：1.81x\n",
      "    • 最大連接度：368\n",
      "\n",
      "  🔬 專家評估：\n",
      "    ⚠️ 關係密度中等（1.0-1.5）\n",
      "       └─ 基本滿足需求，但仍有改進空間\n",
      "\n",
      "    ⚠️ 弱連接實體佔比過大（72.5%）\n",
      "       建議：增加屬性關係、時間關係、因果鏈等多維度關係\n",
      "    💡 缺乏核心樞紐節點（高連接度實體 < 10%）\n",
      "       建議：識別並強化領域核心概念的關係網絡\n",
      "\n",
      "📊 指標 1.3 | 屬性填充率 (Property Fill Rate)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  • Entity.name 填充率：100.00% (5818/5818)\n",
      "  • Chunk.text 填充率：100.00% (452/452)\n",
      "  • 平均屬性填充率：100.00%\n",
      "  ✅ 評估：屬性填充率優秀 (≥95%)\n",
      "\n",
      "📊 指標 1.4 | 孤立節點比例 (Isolated Nodes Ratio)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  • 孤立 Chunk 數 (無 MENTIONS 連接)：11 (2.43%)\n",
      "  • 孤立 Entity 數 (無 RELATION 連接)：0 (0.00%)\n",
      "  ✅ 評估：孤立節點比例低，結構品質良好\n",
      "\n",
      "====================================================================================================\n",
      "【第二組】一致性與類型檢查 (Consistency & Schema Adherence)\n",
      "====================================================================================================\n",
      "\n",
      "📊 指標 2.1 | 類型遵守率 (Schema Adherence)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated function. ('id' has been replaced by 'elementId or consider using an application-generated id')} {position: line: 3, column: 51, offset: 123} for query: '\\n            MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->(e:Entity)\\n            WITH e.name AS name, collect(DISTINCT id(e)) AS entity_ids\\n            WHERE size(entity_ids) > 1\\n            RETURN name, size(entity_ids) AS count\\n            LIMIT 5\\n            '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  • 節點標籤 (Node Labels)：Chunk, Entity\n",
      "    └─ 共 2 種節點類型\n",
      "  • Neo4j 關係類型 (Relationship Types)：RELATION, MENTIONS\n",
      "    └─ 共 2 種 Neo4j 關係類型\n",
      "  • 語義關係類型 (RELATION.type 屬性值)：12%_deep_well_supported, 8%_large_with_wide_chest_base, :adjust, :attr, :install, Achieves, Administered via, Appeal to, Are caused by, Are more common in, Attempt to Stand, Attribute of, Be, Behavior, Bred by, Can be used, Can develop, Can result in, Cause, Causes\n",
      "    ... 以及其他 3072 種語義關係\n",
      "    └─ 共 3092 種語義關係類型\n",
      "  ⚠️ 警告：檢測到過於寬泛的語義關係類型：associated_with, connected_to, relates_to\n",
      "     共有 110 個關係使用了通用語義\n",
      "     建議：優化提示詞以產生更具體的語義關係類型\n",
      "  ✅ 評估：圖譜類型結構完整，語義關係豐富\n",
      "\n",
      "📊 指標 2.2 | 重複實體檢測 (Duplication Check)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  • 檢測到的重複實體名稱數：0\n",
      "  ✅ 評估：未檢測到明顯重複實體\n",
      "\n",
      "📊 指標 2.3 | 屬性合法性檢查 (Attribute Validity)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  • 無效關係數（type 屬性為空）：0\n",
      "  ✅ 評估：所有關係屬性合法\n",
      "\n",
      "====================================================================================================\n",
      "【第三組】核心數據質量報告 (Accuracy & Provenance)\n",
      "====================================================================================================\n",
      "\n",
      "📊 指標 3.1 | 人工抽樣驗證 (Manual Sampling for Accuracy)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  隨機抽取 5 個三元組，請進行人工語義正確性檢查：\n",
      "\n",
      "  [ 1] (Sheep) --[suffer_from]--> (Bloat)\n",
      "  [ 2] (sheep) --[classified_as]--> (Artiodactyla)\n",
      "  [ 3] (lamb) --[originates_from]--> (Australia_and_New_Zealand)\n",
      "  [ 4] (Sheep) --[require]--> (energy)\n",
      "  [ 5] (sheep) --[establish]--> (pecking order)\n",
      "\n",
      "  ================================================================================================\n",
      "  💡 人工檢查指引 (Manual Verification Guidelines)\n",
      "  ================================================================================================\n",
      "  請針對以上三元組逐一評估以下三個維度：\n",
      "\n",
      "  ✓ 語義正確性 (Semantic Correctness)\n",
      "    └─ 實體名稱是否正確且有意義？\n",
      "    └─ 關係類型是否準確描述兩實體間的語義關聯？\n",
      "\n",
      "  ✓ 邏輯一致性 (Logical Consistency)\n",
      "    └─ 三元組的邏輯是否符合真實世界或原始知識庫內容？\n",
      "    └─ Head 和 Tail 的實體類型是否與 Relation 相容？\n",
      "\n",
      "  ✓ 資訊完整性 (Information Completeness)\n",
      "    └─ 三元組是否包含足夠的上下文資訊？\n",
      "    └─ 是否有明顯的資訊缺失或歧義？\n",
      "\n",
      "  📝 建議：記錄有問題的三元組編號，用於後續優化提示詞或知識抽取流程。\n",
      "  ================================================================================================\n",
      "\n",
      "📊 指標 3.2 | 出處標註率 (Provenance Rate)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  • Chunk 出處標註率 (source 屬性)：100.00% (452/452)\n",
      "  • Relation 來源追溯率 (chunks 屬性)：100.00% (7431/7431)\n",
      "  • 平均出處標註率：100.00%\n",
      "  ✅ 評估：出處標註率優秀，知識溯源性高\n",
      "\n",
      "====================================================================================================\n",
      "【最終專家結論】綜合質量評估 (Overall Quality Grade & Expert Conclusion)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "ename": "SessionError",
     "evalue": "Session closed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSessionError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1002\u001b[39m\n\u001b[32m   1000\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🔬 執行圖譜完整度與質量檢驗\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1001\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m VALIDATION_RESULTS = \u001b[43mValidateGraphIntegrity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGRAPH_DRIVER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mKNOWLEDGE_CHUNKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATASET_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# Reranking 調整建議：維持 LINEAR 但提高 alpha=0.60，讓語意嵌入權重更大、同時保留關鍵字搜尋的補強作用。\u001b[39;00m\n\u001b[32m   1011\u001b[39m RETRIEVER_ALPHA = \u001b[32m0.60\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 610\u001b[39m, in \u001b[36mValidateGraphIntegrity\u001b[39m\u001b[34m(driver, original_chunks, dataset_id, sample_size, verbose)\u001b[39m\n\u001b[32m    607\u001b[39m     score_consistency += \u001b[32m30\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    609\u001b[39m     \u001b[38;5;66;03m# 按照通用關係比例扣分\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     total_relations = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[38;5;250;43m        \u001b[39;49m\u001b[33;43;03m\"\"\"\u001b[39;49;00m\n\u001b[32m    612\u001b[39m \u001b[33;43;03m        MATCH (c:Chunk {dataset: $dataset})-[:MENTIONS]->()-[r:RELATION]->()\u001b[39;49;00m\n\u001b[32m    613\u001b[39m \u001b[33;43;03m        RETURN count(r) AS cnt\u001b[39;49;00m\n\u001b[32m    614\u001b[39m \u001b[33;43;03m        \"\"\"\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.single()[\u001b[33m\"\u001b[39m\u001b[33mcnt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m total_relations > \u001b[32m0\u001b[39m:\n\u001b[32m    618\u001b[39m         generic_ratio = consist_schema[\u001b[33m\"\u001b[39m\u001b[33mgeneric_relation_count\u001b[39m\u001b[33m\"\u001b[39m] / total_relations\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\neo4j\\_sync\\work\\session.py:298\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, parameters, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;129m@NonConcurrentMethodChecker\u001b[39m._non_concurrent_method\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    269\u001b[39m     **kwargs: t.Any,\n\u001b[32m    270\u001b[39m ) -> Result:\n\u001b[32m    271\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m    Run a Cypher query within an auto-commit transaction.\u001b[39;00m\n\u001b[32m    273\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    296\u001b[39m \u001b[33;03m    :raises SessionError: if the session has been closed.\u001b[39;00m\n\u001b[32m    297\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m query:\n\u001b[32m    300\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot run an empty query\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\neo4j\\_sync\\work\\workspace.py:319\u001b[39m, in \u001b[36mWorkspace._check_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._closed:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SessionError(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSession closed\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mSessionError\u001b[39m: Session closed"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from typing import Any\n",
    "\n",
    "# 圖譜建立、驗證與檢索優化\n",
    "\n",
    "class OllamaVectorEmbedder:\n",
    "    def __init__(self, client: Client, model: str):\n",
    "        self._client = client\n",
    "        self._model = model\n",
    "        self._dimension: Optional[int] = None\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        resp = self._client.embeddings(model=self._model, prompt=text or \" \")\n",
    "        return resp[\"embedding\"]\n",
    "\n",
    "    def embed_documents(self, texts: Iterable[str]) -> List[List[float]]:\n",
    "        return [self.embed_query(t) for t in texts]\n",
    "\n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        if self._dimension is None:\n",
    "            self._dimension = len(self.embed_query(\"dimension probe\"))\n",
    "        return self._dimension\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"chunk_size must be positive\")\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    chunks: List[str] = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end >= len(text):\n",
    "            break\n",
    "        start += step\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def load_chunks(path: Path) -> List[Dict[str, str]]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Knowledge base not found: {path}\")\n",
    "    raw_text = path.read_text(encoding=\"utf-8\")\n",
    "    segments = chunk_text(raw_text, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    docs: List[Dict[str, str]] = []\n",
    "    for idx, segment in enumerate(segments):\n",
    "        text = segment.strip()\n",
    "        doc_id = f\"{DATASET_ID}_chunk_{idx:05d}\"\n",
    "        docs.append(\n",
    "            {\n",
    "                \"id\": doc_id,\n",
    "                \"text\": text,\n",
    "                \"source\": path.name,\n",
    "                \"hash\": hashlib.sha256(text.encode(\"utf-8\")).hexdigest(),\n",
    "            }\n",
    "        )\n",
    "    return docs\n",
    "\n",
    "\n",
    "def ensure_vector_index(driver, name: str, label: str, prop: str, dimensions: int, similarity: str = \"cosine\") -> None:\n",
    "    with driver.session() as session:\n",
    "        existing = session.run(\"SHOW INDEXES\").data()\n",
    "        if any(idx.get(\"name\") == name for idx in existing):\n",
    "            return\n",
    "        cypher = f\"\"\"\n",
    "        CREATE VECTOR INDEX {name}\n",
    "        FOR (n:{label}) ON (n.{prop})\n",
    "        OPTIONS {{ indexConfig: {{ `vector.dimensions`: {dimensions}, `vector.similarity_function`: '{similarity}' }} }}\n",
    "        \"\"\"\n",
    "        session.run(cypher)\n",
    "        session.run(\"CALL db.awaitIndexes()\")\n",
    "\n",
    "\n",
    "def ensure_fulltext_index(driver, name: str, label: str, prop: str = \"text\") -> bool:\n",
    "    def _exists(sess) -> bool:\n",
    "        rows = sess.run(\"SHOW INDEXES\").data()\n",
    "        for row in rows:\n",
    "            if str(row.get(\"name\")) == name and (\n",
    "                row.get(\"type\") == \"FULLTEXT\" or row.get(\"indexType\") == \"FULLTEXT\"\n",
    "            ):\n",
    "                return True\n",
    "        try:\n",
    "            rows = sess.run(\n",
    "                \"CALL db.index.fulltext.list() YIELD name WHERE name = $name RETURN name\",\n",
    "                name=name,\n",
    "            ).data()\n",
    "            if rows:\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    with driver.session() as session:\n",
    "        if _exists(session):\n",
    "            return True\n",
    "        try:\n",
    "            session.run(\n",
    "                f\"\"\"\n",
    "                CREATE FULLTEXT INDEX {name} IF NOT EXISTS\n",
    "                FOR (n:{label}) ON EACH [n.{prop}]\n",
    "                \"\"\"\n",
    "            )\n",
    "        except Exception:\n",
    "            try:\n",
    "                session.run(\n",
    "                    \"CALL db.index.fulltext.createNodeIndex($name, $labels, $props)\",\n",
    "                    name=name,\n",
    "                    labels=[label],\n",
    "                    props=[prop],\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            session.run(\"CALL db.awaitIndexes()\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    with driver.session() as session:\n",
    "        return _exists(session)\n",
    "\n",
    "\n",
    "def upsert_chunks(driver, embedder: OllamaVectorEmbedder, docs: List[Dict[str, str]]) -> Tuple[int, int]:\n",
    "    inserted = 0\n",
    "    skipped = 0\n",
    "    with driver.session() as session:\n",
    "        for doc in docs:\n",
    "            existing = session.run(\n",
    "                \"MATCH (c:Chunk {id:$id}) RETURN c.text_hash AS hash\",\n",
    "                id=doc[\"id\"],\n",
    "            ).single()\n",
    "            if existing and existing.get(\"hash\") == doc[\"hash\"]:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            embedding = embedder.embed_query(doc[\"text\"])\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (c:Chunk {id:$id})\n",
    "                SET c.text = $text,\n",
    "                    c.source = $source,\n",
    "                    c.dataset = $dataset,\n",
    "                    c.embedding = $embedding,\n",
    "                    c.text_hash = $hash\n",
    "                \"\"\",\n",
    "                id=doc[\"id\"],\n",
    "                text=doc[\"text\"],\n",
    "                source=doc[\"source\"],\n",
    "                dataset=DATASET_ID,\n",
    "                embedding=embedding,\n",
    "                hash=doc[\"hash\"],\n",
    "            )\n",
    "            inserted += 1\n",
    "    return inserted, skipped\n",
    "\n",
    "\n",
    "TRIPLE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are a professional knowledge graph construction expert. Your task is to **deeply extract both explicit and implicit semantic relationships** from text, extracting **as many rich knowledge triples as possible** to build a high-density, highly connected knowledge network in {language}.\n",
    "\n",
    "🎯 **Core Objectives (Urgent Optimization - Current Weak Connection Entities 77.3%)**:\n",
    "1. ⚠️ **Eliminate the 77.3% \"1-relation nodes\"**, transforming them into \"3+ relation nodes\"\n",
    "2. 🔍 **Deep mining of implicit relationships**: Extract not only explicit relationships but also infer implicit semantic connections\n",
    "3. 📊 **Use standardized, specific relation types** (prohibit vague relations like \"related\", \"associated\")\n",
    "4. 🎯 **Maximize attribute relationship extraction** (numerical values, states, time, types, etc.)\n",
    "5. 🌐 **Build core hub nodes**: Identify domain core concepts and establish rich relationship networks for them\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "## 📋 Part 0: Explicit vs Implicit Relationships (New! Extremely Important)\n",
    "\n",
    "### 🔍 **Strategy: 3-Layer Relationship Mining Depth**\n",
    "\n",
    "#### **Layer 1: Explicit Relationships (Directly stated in text)**\n",
    "```\n",
    "Text: \"Goats deficient in vitamin A develop night blindness.\"\n",
    "Explicit relationship:\n",
    "- (vitamin_A_deficiency, causes, night_blindness)\n",
    "```\n",
    "\n",
    "#### **Layer 2: Near-distance Implicit Relationships (Intra-sentence inference)**\n",
    "```\n",
    "Text: \"Goats deficient in vitamin A develop night blindness.\"\n",
    "Implicit relationships (inferred from same sentence):\n",
    "- (goat, deficient_in, vitamin_A)          ← subject-action relationship\n",
    "- (goat, may_develop, night_blindness)     ← subject-outcome relationship\n",
    "- (night_blindness, caused_by, vitamin_A_deficiency)   ← reverse causality\n",
    "- (night_blindness, affects, visual_function)          ← domain knowledge inference\n",
    "```\n",
    "\n",
    "#### **Layer 3: Long-distance Implicit Relationships (Cross-sentence inference)**\n",
    "```\n",
    "Text:\n",
    "\"Goats are deficient in vitamin A. (Sentence 1)\n",
    "Night blindness is a common symptom. (Sentence 2)\n",
    "Supplementation with carotene is needed. (Sentence 3)\"\n",
    "\n",
    "Long-distance implicit relationships (cross-sentence inference):\n",
    "- (carotene, used_to_treat, vitamin_A_deficiency)   ← connects sentences 1 and 3\n",
    "- (carotene, prevents, night_blindness)             ← connects sentences 2 and 3\n",
    "- (carotene, converts_to, vitamin_A)                ← domain knowledge inference\n",
    "- (goat, requires_intake_of, carotene)              ← complete causal chain\n",
    "```\n",
    "\n",
    "### 💡 **Implicit Relationship Identification Strategies (Must Execute)**\n",
    "\n",
    "#### **Strategy 1: Causal Chain Reasoning**\n",
    "```\n",
    "Explicit: A causes B\n",
    "Implicit:\n",
    "- (A, triggers, B)      ← synonymous expression\n",
    "- (B, originates_from, A)      ← reverse causality\n",
    "- (B, etiology_is, A)    ← etiological relationship\n",
    "- (subject, experiences, B)   ← subject-outcome relationship\n",
    "```\n",
    "\n",
    "#### **Strategy 2: Classification Reasoning**\n",
    "```\n",
    "Explicit: A belongs_to B\n",
    "Implicit:\n",
    "- (B, contains, A)          ← reverse membership\n",
    "- (A, is_type_of, B)    ← synonymous expression\n",
    "- (A, has_characteristics_of, B)    ← feature inheritance\n",
    "```\n",
    "\n",
    "#### **Strategy 3: Composition Reasoning**\n",
    "```\n",
    "Explicit: A contains B\n",
    "Implicit:\n",
    "- (B, exists_in, A)        ← reverse composition\n",
    "- (B, component_of, A) ← component relationship\n",
    "- (A, composed_of, B_and_others)     ← structural relationship\n",
    "```\n",
    "\n",
    "#### **Strategy 4: Functional Reasoning**\n",
    "```\n",
    "Explicit: A used_for B\n",
    "Implicit:\n",
    "- (B, depends_on, A)          ← dependency relationship\n",
    "- (A, function_is, achieve_B)    ← functional description\n",
    "- (A, applicable_to, B_scenario)    ← application scenario\n",
    "```\n",
    "\n",
    "#### **Strategy 5: Attribute Reasoning**\n",
    "```\n",
    "Explicit: A's X is Y (numerical value/state)\n",
    "Implicit:\n",
    "- (A, X_attribute_is, Y)       ← attribute relationship\n",
    "- (Y, describes, A's_X)       ← descriptive relationship\n",
    "- (A, has, Y_as_X)     ← characteristic relationship\n",
    "```\n",
    "\n",
    "### 📖 **Complete Example: 3-Layer Relationship Mining**\n",
    "\n",
    "**Text**:\n",
    "\"Goats deficient in vitamin A, weighing approximately 45 kg. It causes growth retardation and night blindness. Supplementing with carotene can improve this condition.\"\n",
    "\n",
    "**❌ Extract only explicit relationships (shallow extraction)**:\n",
    "```json\n",
    "[\n",
    "  {{\"head\":\"goat\", \"relation\":\"deficient_in\", \"tail\":\"vitamin_A\"}},\n",
    "  {{\"head\":\"goat\", \"relation\":\"weighs\", \"tail\":\"45kg\"}},\n",
    "  {{\"head\":\"it\", \"relation\":\"causes\", \"tail\":\"growth_retardation\"}},\n",
    "  {{\"head\":\"carotene\", \"relation\":\"improves\", \"tail\":\"this_condition\"}}\n",
    "]\n",
    "```\n",
    "**Problem**: 4 relationships, density = 0.57, pronoun entities, weak connections!\n",
    "\n",
    "**✅ Deep mining of explicit + implicit relationships (complete extraction)**:\n",
    "```json\n",
    "[\n",
    "  // === Layer 1: Explicit Relationships ===\n",
    "  {{\"head\":\"goat\", \"relation\":\"deficient_in\", \"tail\":\"vitamin_A\"}},\n",
    "  {{\"head\":\"goat\", \"relation\":\"weighs\", \"tail\":\"45kg\"}},\n",
    "  {{\"head\":\"vitamin_A_deficiency\", \"relation\":\"causes\", \"tail\":\"growth_retardation\"}},     // Coreference: \"it\"→\"vitamin_A_deficiency\"\n",
    "  {{\"head\":\"vitamin_A_deficiency\", \"relation\":\"causes\", \"tail\":\"night_blindness\"}},\n",
    "  {{\"head\":\"carotene\", \"relation\":\"improves\", \"tail\":\"vitamin_A_deficiency\"}},       // Coreference: \"this_condition\"→\"vitamin_A_deficiency\"\n",
    "  \n",
    "  // === Layer 2: Near-distance Implicit Relationships (intra-sentence inference) ===\n",
    "  {{\"head\":\"goat\", \"relation\":\"exhibits\", \"tail\":\"growth_retardation\"}},            // subject-symptom\n",
    "  {{\"head\":\"goat\", \"relation\":\"develops\", \"tail\":\"night_blindness\"}},                // subject-symptom\n",
    "  {{\"head\":\"growth_retardation\", \"relation\":\"originates_from\", \"tail\":\"vitamin_A_deficiency\"}},       // reverse causality\n",
    "  {{\"head\":\"night_blindness\", \"relation\":\"etiology_is\", \"tail\":\"vitamin_A_deficiency\"}},       // etiology\n",
    "  {{\"head\":\"growth_retardation\", \"relation\":\"belongs_to\", \"tail\":\"nutritional_deficiency_symptoms\"}},      // classification\n",
    "  {{\"head\":\"night_blindness\", \"relation\":\"belongs_to\", \"tail\":\"vitamin_A_deficiency_symptoms\"}},     // classification\n",
    "  {{\"head\":\"45kg\", \"relation\":\"describes\", \"tail\":\"goat_weight\"}},            // attribute description\n",
    "  \n",
    "  // === Layer 3: Long-distance Implicit Relationships (cross-sentence inference) ===\n",
    "  {{\"head\":\"carotene\", \"relation\":\"used_to_treat\", \"tail\":\"vitamin_A_deficiency\"}},   // treatment\n",
    "  {{\"head\":\"carotene\", \"relation\":\"prevents\", \"tail\":\"growth_retardation\"}},          // prevention\n",
    "  {{\"head\":\"carotene\", \"relation\":\"prevents\", \"tail\":\"night_blindness\"}},            // prevention\n",
    "  {{\"head\":\"goat\", \"relation\":\"requires_intake_of\", \"tail\":\"carotene\"}},          // requirement\n",
    "  {{\"head\":\"carotene\", \"relation\":\"supplements\", \"tail\":\"vitamin_A\"}},           // supplementation\n",
    "  {{\"head\":\"carotene\", \"relation\":\"converts_to\", \"tail\":\"vitamin_A\"}},         // conversion (domain knowledge)\n",
    "  {{\"head\":\"nutritional_deficiency_symptoms\", \"relation\":\"includes\", \"tail\":\"growth_retardation\"}},      // reverse membership\n",
    "  {{\"head\":\"vitamin_A_deficiency_symptoms\", \"relation\":\"includes\", \"tail\":\"night_blindness\"}},     // reverse membership\n",
    "  {{\"head\":\"goat\", \"relation\":\"health_status\", \"tail\":\"malnourished\"}},          // status inference\n",
    "  {{\"head\":\"vitamin_A\", \"relation\":\"affects\", \"tail\":\"growth_development\"}},           // functional relationship\n",
    "  {{\"head\":\"vitamin_A\", \"relation\":\"affects\", \"tail\":\"visual_function\"}}            // functional relationship\n",
    "]\n",
    "```\n",
    "**Results**:\n",
    "- **23 relationships** (5 explicit + 18 implicit)\n",
    "- Entity \"goat\": 7 relationships ✅\n",
    "- Entity \"vitamin_A_deficiency\": 6 relationships ✅\n",
    "- Entity \"carotene\": 6 relationships ✅\n",
    "- **Density: 23 / 12 = 1.92 ✅** (improved from 0.57 to 1.92!)\n",
    "- **Weak connection nodes: 0% ✅**\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "## 📋 Part 1: Standardized Relation Type List (Strictly Follow)\n",
    "\n",
    "### ✅ **Allowed Relation Types** (50+ types)\n",
    "\n",
    "#### **A. Causality & Influence (15 types)**\n",
    "causes, triggers, results_in, produces, induces, prompts, originates_from, due_to, because_of, affects, acts_on, determines, controls, regulates, changes\n",
    "\n",
    "#### **B. Membership & Classification (12 types)**\n",
    "belongs_to, classified_as, is_type_of, contained_in, categorized_as, type_is, variety_is, kind_is, attribute_is, affiliated_with, included_in, divided_into\n",
    "\n",
    "#### **C. Composition & Structure (10 types)**\n",
    "contains, comprises, composed_of, constitutes, component_is, part_is, element_is, constituent_is, structural_element, component_of\n",
    "\n",
    "#### **D. Numerical & Measurement (10 types)**\n",
    "value_is, content_is, concentration_is, weight_is, body_weight_is, length_is, ratio_is, percentage_is, dosage_is, amount_is\n",
    "\n",
    "#### **E. State & Characteristics (10 types)**\n",
    "state_is, in_state_of, exhibits, presents, characteristic_is, appearance_is, property_is, morphology_is, color_is, texture_is\n",
    "\n",
    "#### **F. Time & Process (10 types)**\n",
    "occurs_at, lasts, cycle_is, starts_at, ends_at, duration_is, frequency_is, interval_is, timepoint_is, moment_is\n",
    "\n",
    "#### **G. Function & Application (10 types)**\n",
    "used_for, applicable_to, applied_to, treats, prevents, improves, alleviates, promotes, inhibits, enhances\n",
    "\n",
    "#### **H. Requirements & Dependencies (8 types)**\n",
    "requires, depends_on, demands, must_have, deficient_in, rich_in, supplements, adds\n",
    "\n",
    "#### **I. Changes & Transformations (8 types)**\n",
    "increases, decreases, enhances_level_of, reduces, converts_to, evolves_into, becomes, decomposes_to\n",
    "\n",
    "#### **J. Comparison & Association (8 types)**\n",
    "higher_than, lower_than, superior_to, inferior_to, equivalent_to, equal_to, corresponds_to, similar_to\n",
    "\n",
    "#### **K. Spatial & Location (6 types)**\n",
    "located_at, distributed_in, exists_in, appears_in, concentrated_in, dispersed_in\n",
    "\n",
    "#### **L. Interaction & Action (6 types)**\n",
    "synergizes_with, antagonizes, competes_with, cooperates_with, interacts_with, acts_together_with\n",
    "\n",
    "### ❌ **Strictly Prohibited Vague Relation Types**\n",
    "🚫 related, associated, connected, concerning, involves, about, corresponding, linking, connection, influence (too vague)\n",
    "🚫 is, has, are, and, with, or, etc. (too basic)\n",
    "🚫 produces, occurs, appears (needs specification as \"causes\", \"triggers\", etc.)\n",
    "\n",
    "⚠️ **Relation Type Normalization Rules**:\n",
    "• Each relationship must use the most precise verb from the above list\n",
    "• If no exact match, choose the closest specific verb\n",
    "• Synonym unification: e.g., \"lacks\"→\"deficient_in\", \"contains\"→\"comprises\"\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "## 📋 Part 2: Deep Attribute Relationship Extraction (Key! Directly Improves Density)\n",
    "\n",
    "### 🎯 **Strategy: Transform descriptive information of each entity into relationships**\n",
    "\n",
    "#### **1. Numerical Attribute Relationships (Must Extract!)**\n",
    "```\n",
    "Text: \"Goat's weight is approximately 45 kg, height 60 cm.\"\n",
    "Extract:\n",
    "{{\"head\":\"goat\", \"relation\":\"weight_is\", \"tail\":\"45kg\"}}\n",
    "{{\"head\":\"goat\", \"relation\":\"height_is\", \"tail\":\"60cm\"}}\n",
    "```\n",
    "\n",
    "#### **2. Concentration/Content Attribute Relationships (Must Extract!)**\n",
    "```\n",
    "Text: \"Feed protein content is 18%, calcium content 2.5%.\"\n",
    "Extract:\n",
    "{{\"head\":\"feed\", \"relation\":\"protein_content_is\", \"tail\":\"18%\"}}\n",
    "{{\"head\":\"feed\", \"relation\":\"calcium_content_is\", \"tail\":\"2.5%\"}}\n",
    "{{\"head\":\"protein\", \"relation\":\"ratio_is\", \"tail\":\"18%\"}}\n",
    "{{\"head\":\"calcium\", \"relation\":\"ratio_is\", \"tail\":\"2.5%\"}}\n",
    "```\n",
    "\n",
    "#### **3. State Attribute Relationships (Must Extract!)**\n",
    "```\n",
    "Text: \"Sick goats present lethargic state, body temperature 40°C.\"\n",
    "Extract:\n",
    "{{\"head\":\"sick_goat\", \"relation\":\"state_is\", \"tail\":\"lethargic\"}}\n",
    "{{\"head\":\"sick_goat\", \"relation\":\"body_temperature_is\", \"tail\":\"40°C\"}}\n",
    "{{\"head\":\"lethargic\", \"relation\":\"belongs_to\", \"tail\":\"disease_symptoms\"}}\n",
    "```\n",
    "\n",
    "#### **4. Time Attribute Relationships (Must Extract!)**\n",
    "```\n",
    "Text: \"Treatment cycle is 7 days, medication administered twice daily.\"\n",
    "Extract:\n",
    "{{\"head\":\"treatment\", \"relation\":\"cycle_is\", \"tail\":\"7_days\"}}\n",
    "{{\"head\":\"medication\", \"relation\":\"frequency_is\", \"tail\":\"twice_daily\"}}\n",
    "{{\"head\":\"treatment\", \"relation\":\"lasts\", \"tail\":\"7_days\"}}\n",
    "```\n",
    "\n",
    "#### **5. Type/Classification Attribute Relationships (Must Extract!)**\n",
    "```\n",
    "Text: \"Goats belong to ruminants, breed is Boer goat.\"\n",
    "Extract:\n",
    "{{\"head\":\"goat\", \"relation\":\"type_is\", \"tail\":\"ruminant\"}}\n",
    "{{\"head\":\"goat\", \"relation\":\"breed_is\", \"tail\":\"Boer_goat\"}}\n",
    "{{\"head\":\"Boer_goat\", \"relation\":\"belongs_to\", \"tail\":\"goat_breed\"}}\n",
    "```\n",
    "\n",
    "#### **6. Location Attribute Relationships**\n",
    "```\n",
    "Text: \"Lesion located in lower small intestine, area approximately 5 square cm.\"\n",
    "Extract:\n",
    "{{\"head\":\"lesion\", \"relation\":\"located_at\", \"tail\":\"lower_small_intestine\"}}\n",
    "{{\"head\":\"lesion\", \"relation\":\"area_is\", \"tail\":\"5_square_cm\"}}\n",
    "```\n",
    "\n",
    "### 💡 **Attribute Extraction Strategy Summary**:\n",
    "**Wherever \"number + unit\", \"state description\", \"time description\", or \"classification name\" appears, it should be extracted as a relationship!**\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "## 📋 Part 3: Coreference Resolution (Extremely Important! Directly Solves 71.8% Problem)\n",
    "\n",
    "### ⚠️ **Must Execute Resolution Rules**\n",
    "\n",
    "#### **Rule 1: Pronoun Resolution Table**\n",
    "| Pronoun | Resolution Strategy | Example |\n",
    "|---------|-------------------|---------|\n",
    "| it, its, that | Most recently mentioned entity | \"Vitamin A deficiency. It causes...\"→\"Vitamin A deficiency causes...\"|\n",
    "| this, that, such | Previous referent | \"This disease requires...\"→\"Vitamin A deficiency requires...\"|\n",
    "| this type, such kind | Remove demonstrative | \"This condition\"→\"Vitamin A deficiency\"|\n",
    "| the substance, the animal | Extract core noun | \"The animal\"→\"goat\"|\n",
    "| aforementioned, above | Most recent prior entity | \"Above symptoms\"→\"growth retardation\"|\n",
    "\n",
    "#### **Rule 2: Omitted Subject Completion**\n",
    "```\n",
    "Text: \"Goats deficient in vitamin A, causes night blindness.\"\n",
    "                        ↑ omitted subject\n",
    "Correct: \"Vitamin A deficiency\" causes \"night blindness\"\n",
    "Wrong: \"causes\" as entity (❌)\n",
    "```\n",
    "\n",
    "#### **Rule 3: Compound Reference Decomposition**\n",
    "```\n",
    "\"This condition\" → Decompose to specific event mentioned earlier\n",
    "\"This phenomenon\" → Restore to specific phenomenon name\n",
    "\"This type of disease\" → Restore to specific disease name\n",
    "```\n",
    "\n",
    "### 📖 **Complete Coreference Resolution Example**\n",
    "\n",
    "**Text**:\n",
    "\"Goats are deficient in vitamin A. It causes growth retardation and reduced immunity. This condition requires timely supplementation. The animal also develops night blindness, which severely affects production performance.\"\n",
    "\n",
    "**❌ Incorrect Extraction (No Coreference Resolution)**:\n",
    "```json\n",
    "[\n",
    "  {{\"head\":\"goat\", \"relation\":\"deficient_in\", \"tail\":\"vitamin_A\"}},\n",
    "  {{\"head\":\"it\", \"relation\":\"causes\", \"tail\":\"growth_retardation\"}},              ← Wrong! Pronoun as entity\n",
    "  {{\"head\":\"this_condition\", \"relation\":\"requires\", \"tail\":\"timely_supplementation\"}},        ← Wrong! Referent as entity\n",
    "  {{\"head\":\"the_animal\", \"relation\":\"develops\", \"tail\":\"night_blindness\"}}             ← Wrong! Referent as entity\n",
    "]\n",
    "```\n",
    "**Problem**: 4 false entities, density = 1.0, 71.8% weak connection nodes!\n",
    "\n",
    "**✅ Correct Extraction (Complete Coreference Resolution + Attribute Relationships)**:\n",
    "```json\n",
    "[\n",
    "  {{\"head\":\"goat\", \"relation\":\"deficient_in\", \"tail\":\"vitamin_A\"}},\n",
    "  {{\"head\":\"vitamin_A_deficiency\", \"relation\":\"causes\", \"tail\":\"growth_retardation\"}},        ← \"it\" → \"vitamin_A_deficiency\"\n",
    "  {{\"head\":\"vitamin_A_deficiency\", \"relation\":\"causes\", \"tail\":\"reduced_immunity\"}},\n",
    "  {{\"head\":\"vitamin_A_deficiency\", \"relation\":\"requires\", \"tail\":\"timely_supplementation\"}},         ← \"this_condition\" → \"vitamin_A_deficiency\"\n",
    "  {{\"head\":\"goat\", \"relation\":\"exhibits\", \"tail\":\"growth_retardation\"}},            ← from causal inference\n",
    "  {{\"head\":\"goat\", \"relation\":\"exhibits\", \"tail\":\"reduced_immunity\"}},\n",
    "  {{\"head\":\"goat\", \"relation\":\"develops\", \"tail\":\"night_blindness\"}},                ← \"the_animal\" → \"goat\"\n",
    "  {{\"head\":\"vitamin_A_deficiency\", \"relation\":\"triggers\", \"tail\":\"night_blindness\"}},          ← causal chain\n",
    "  {{\"head\":\"night_blindness\", \"relation\":\"affects\", \"tail\":\"production_performance\"}},             ← continue extraction\n",
    "  {{\"head\":\"growth_retardation\", \"relation\":\"belongs_to\", \"tail\":\"vitamin_A_deficiency_symptoms\"}},     ← classification\n",
    "  {{\"head\":\"night_blindness\", \"relation\":\"belongs_to\", \"tail\":\"vitamin_A_deficiency_symptoms\"}},\n",
    "  {{\"head\":\"goat\", \"relation\":\"production_performance\", \"tail\":\"impaired\"}}               ← state relationship\n",
    "]\n",
    "```\n",
    "**Results**:\n",
    "- Entity \"goat\": 6 relationships ✅\n",
    "- Entity \"vitamin_A_deficiency\": 5 relationships ✅\n",
    "- Density: 12 relationships / 8 entities = 1.5 ✅\n",
    "- Weak connection nodes: 0% ✅\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "## 📋 Part 4: Synonym Unification Rules\n",
    "\n",
    "### 🎯 **Entity Name Standardization**\n",
    "\n",
    "#### **Rule 1: Unify to Most Complete Name**\n",
    "- \"vitamin_A\" = \"VA\" = \"retinol\" → **Unify to \"vitamin_A\"**\n",
    "- \"goat\" = \"sheep\" = \"caprine\" → **Unify to \"goat\"**\n",
    "- \"protein\" = \"proteins\" = \"proteinaceous\" → **Unify to \"protein\"**\n",
    "\n",
    "#### **Rule 2: Preserve Domain-Specific Terms**\n",
    "- \"silage\" = \"ensiled_feed\" → **Unify to \"silage\"** (standard term)\n",
    "- \"ruminant\" ≠ \"rumination\" → **Keep complete name**\n",
    "\n",
    "#### **Rule 3: Normalize Numerical Units**\n",
    "- \"45kg\" = \"45 kilograms\" → **Unify to \"45kg\"**\n",
    "- \"18%\" = \"eighteen_percent\" → **Unify to \"18%\"**\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "## 📊 **Quality Checklist (Must Check Before Each Extraction)**\n",
    "\n",
    "After extraction, check each item:\n",
    "\n",
    "□ **Coreference Resolution Check**:\n",
    "  - Are all pronouns like \"it\", \"this\", \"that\", \"its\" fully resolved to concrete entities?\n",
    "  - Are omitted subjects completed?\n",
    "\n",
    "□ **Attribute Relationship Check**:\n",
    "  - Are all \"number + unit\" extracted as attribute relationships?\n",
    "  - Are all state descriptions (color, morphology, state) extracted?\n",
    "  - Are all time information (duration, frequency, cycle) extracted?\n",
    "  - Are all classification information (type, breed, kind) extracted?\n",
    "\n",
    "□ **Relation Type Check**:\n",
    "  - Are verbs from the standardized relation type list used?\n",
    "  - Are vague relations like \"related\", \"associated\", \"connected\" present? (❌ Prohibited)\n",
    "\n",
    "□ **Entity Connectivity Check**:\n",
    "  - Does each core entity (main object) have at least 2+ relationships?\n",
    "  - Are there isolated single-relationship entities?\n",
    "\n",
    "□ **Synonym Check**:\n",
    "  - Is the same entity using a unified name?\n",
    "  - For example, are \"vitamin_A\" and \"VA\" unified?\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "## ✅ **Output Format Requirements**\n",
    "\n",
    "Output only JSON array, each triple containing head, relation, tail three fields.\n",
    "\n",
    "**Example**:\n",
    "```json\n",
    "[\n",
    "  {{\"head\":\"goat\", \"relation\":\"weighs\", \"tail\":\"45kg\"}},\n",
    "  {{\"head\":\"goat\", \"relation\":\"belongs_to\", \"tail\":\"ruminant\"}},\n",
    "  {{\"head\":\"ruminant\", \"relation\":\"characteristic_is\", \"tail\":\"multi_stomach_digestive_system\"}}\n",
    "]\n",
    "```\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "## 🎯 **Goals & Benchmarks**\n",
    "\n",
    "• **Density Goal**: Relationship density > 1.8 (relationships/entities)\n",
    "• **Connectivity Goal**: Weak connection entities (1 relationship) < 20% (down from current 71.8%)\n",
    "• **Extraction Goal**: Extract 12,000+ triples from 170,000 character text\n",
    "\n",
    "Please begin extraction from the following text:\n",
    "\n",
    "{chunk}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _normalize_text(value: Any) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(value).strip())\n",
    "\n",
    "\n",
    "def deduplicate_triples(triples: Iterable[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    unique: List[Dict[str, str]] = []\n",
    "    seen = set()\n",
    "    for triple in triples:\n",
    "        key = (triple.get(\"head\"), triple.get(\"relation\"), triple.get(\"tail\"))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        unique.append(triple)\n",
    "    return unique\n",
    "\n",
    "\n",
    "def parse_triples(raw: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    解析三元組並進行質量過濾\n",
    "    \n",
    "    質量控制規則：\n",
    "    1. 過濾自環關係（head == tail）\n",
    "    2. 過濾超長實體名稱（>50字元，可能是句子片段）\n",
    "    3. 過濾過短實體名稱（<2字元，可能是無意義符號）\n",
    "    4. 過濾空白或純數字關係類型\n",
    "    \"\"\"\n",
    "    candidates: List[Dict[str, str]] = []\n",
    "    payload = None\n",
    "    try:\n",
    "        payload = json.loads(raw)\n",
    "    except Exception:\n",
    "        match = re.search(r\"\\[[\\s\\S]*\\]\", raw)\n",
    "        if match:\n",
    "            try:\n",
    "                payload = json.loads(match.group(0))\n",
    "            except Exception:\n",
    "                payload = None\n",
    "    \n",
    "    if isinstance(payload, list):\n",
    "        for item in payload:\n",
    "            if isinstance(item, dict):\n",
    "                head = item.get(\"head\")\n",
    "                relation = item.get(\"relation\")\n",
    "                tail = item.get(\"tail\")\n",
    "            elif isinstance(item, (list, tuple)) and len(item) == 3:\n",
    "                head, relation, tail = item\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # 基本類型檢查\n",
    "            if not all(isinstance(x, str) and x.strip() for x in (head, relation, tail)):\n",
    "                continue\n",
    "            \n",
    "            # 正規化\n",
    "            head = _normalize_text(head)\n",
    "            relation = _normalize_text(relation)\n",
    "            tail = _normalize_text(tail)\n",
    "            \n",
    "            # ═══════════════════════════════════════════════════════\n",
    "            # 質量過濾規則\n",
    "            # ═══════════════════════════════════════════════════════\n",
    "            \n",
    "            # 規則 1：過濾自環關係（實體指向自己）\n",
    "            if head.lower() == tail.lower():\n",
    "                continue\n",
    "            \n",
    "            # 規則 2：過濾超長實體名稱（可能是句子片段）\n",
    "            if len(head) > 50 or len(tail) > 50:\n",
    "                continue\n",
    "            \n",
    "            # 規則 3：過濾過短實體名稱（可能是無意義符號）\n",
    "            if len(head) < 2 or len(tail) < 2:\n",
    "                continue\n",
    "            \n",
    "            # 規則 4：過濾空白或純數字關係類型\n",
    "            if not relation or relation.isdigit():\n",
    "                continue\n",
    "            \n",
    "            # 規則 5：過濾關係類型過長（可能是句子）\n",
    "            if len(relation) > 30:\n",
    "                continue\n",
    "            \n",
    "            # 規則 6：過濾常見的無意義實體（可選）\n",
    "            meaningless_entities = {'it', 'this', 'that', 'these', 'those', 'they', 'them',\n",
    "                                   '它', '這', '那', '該', '此', '其'}\n",
    "            if head.lower() in meaningless_entities or tail.lower() in meaningless_entities:\n",
    "                continue\n",
    "            \n",
    "            # 通過所有過濾規則，加入候選列表\n",
    "            candidates.append({\n",
    "                \"head\": head,\n",
    "                \"relation\": relation,\n",
    "                \"tail\": tail,\n",
    "            })\n",
    "    \n",
    "    return deduplicate_triples(candidates)\n",
    "\n",
    "\n",
    "def split_text_for_triples(text: str, max_length: int = 1024) -> List[str]:\n",
    "    paragraphs = [p.strip() for p in re.split(r\"\\n{2,}\", text) if p.strip()]\n",
    "    if not paragraphs:\n",
    "        paragraphs = [text]\n",
    "    segments: List[str] = []\n",
    "    for para in paragraphs:\n",
    "        if len(para) <= max_length:\n",
    "            segments.append(para)\n",
    "            continue\n",
    "        start = 0\n",
    "        while start < len(para):\n",
    "            end = min(len(para), start + max_length)\n",
    "            segments.append(para[start:end])\n",
    "            start = end\n",
    "    return segments\n",
    "\n",
    "\n",
    "def extract_triples(\n",
    "    client: Client,\n",
    "    text: str,\n",
    "    model: str,\n",
    "    language: str,\n",
    "    retries: int = 2,\n",
    "    allow_recursive: bool = True,\n",
    ") -> List[Dict[str, str]]:\n",
    "    prompt = TRIPLE_PROMPT_TEMPLATE.format(chunk=text, language=language)\n",
    "    for attempt in range(retries + 1):\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.15 + attempt * 0.05, \"top_p\": 0.9},\n",
    "        )\n",
    "        content = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "        triples = parse_triples(content)\n",
    "        if triples:\n",
    "            return deduplicate_triples(triples)\n",
    "    if allow_recursive and len(text) > 600:\n",
    "        aggregated: List[Dict[str, str]] = []\n",
    "        for segment in split_text_for_triples(text):\n",
    "            partial = extract_triples(\n",
    "                client,\n",
    "                segment,\n",
    "                model=model,\n",
    "                language=language,\n",
    "                retries=1,\n",
    "                allow_recursive=False,\n",
    "            )\n",
    "            aggregated.extend(partial)\n",
    "        return deduplicate_triples(aggregated)\n",
    "    return []\n",
    "\n",
    "\n",
    "def collect_triples_for_documents(\n",
    "    client: Client,\n",
    "    docs: List[Dict[str, str]],\n",
    "    model: str,\n",
    "    language: str,\n",
    ") -> Tuple[Dict[str, List[Dict[str, str]]], List[str]]:\n",
    "    triple_map: Dict[str, List[Dict[str, str]]] = {}\n",
    "    empty_chunks: List[str] = []\n",
    "    for doc in docs:\n",
    "        triples = extract_triples(client, doc[\"text\"], model=model, language=language)\n",
    "        if not triples:\n",
    "            empty_chunks.append(doc[\"id\"])\n",
    "        triple_map[doc[\"id\"]] = triples\n",
    "    return triple_map, empty_chunks\n",
    "\n",
    "\n",
    "def ingest_triples(\n",
    "    driver,\n",
    "    docs: List[Dict[str, str]],\n",
    "    client: Client,\n",
    "    model: str,\n",
    "    language: str,\n",
    ") -> Tuple[int, int, List[str]]:\n",
    "    \"\"\"\n",
    "    增量式知識圖譜構建 (Incremental Construction)\n",
    "    \n",
    "    核心原則：\n",
    "    1. 使用 MERGE 而非 CREATE，確保實體和關係不重複\n",
    "    2. 不刪除既有的 MENTIONS 和 RELATION\n",
    "    3. 僅增量添加新的知識三元組\n",
    "    4. 保留所有歷史來源追溯 (r.chunks)\n",
    "    \n",
    "    階段劃分：\n",
    "    - 階段一：實體節點增量寫入 (Entity Nodes)\n",
    "    - 階段二：關係/三元組增量寫入 (Relationships/Triples)\n",
    "    - 階段三：Chunk 與出處增量連接 (Provenance Linking)\n",
    "    \"\"\"\n",
    "    triple_map, empty_chunks = collect_triples_for_documents(client, docs, model, language)\n",
    "    updated = 0\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        for doc in docs:\n",
    "            chunk_id = doc[\"id\"]\n",
    "            triples = triple_map.get(chunk_id, [])\n",
    "            \n",
    "            # ⚠️ 重要變更：移除所有 DELETE 操作\n",
    "            # 舊邏輯（已廢除）：\n",
    "            # - DELETE MENTIONS 關係\n",
    "            # - DELETE RELATION 關係\n",
    "            # 新邏輯：保留所有既有資料，僅增量添加\n",
    "            \n",
    "            if not triples:\n",
    "                # 即使沒有新三元組，也不刪除既有資料\n",
    "                continue\n",
    "            \n",
    "            # ═══════════════════════════════════════════════════════════\n",
    "            # 階段一 + 階段二 + 階段三：合併執行（性能優化）\n",
    "            # ═══════════════════════════════════════════════════════════\n",
    "            session.run(\n",
    "            \"\"\"\n",
    "            // ===== 階段一：實體節點增量寫入 =====\n",
    "            UNWIND $triples AS triple\n",
    "            \n",
    "            // 創建或匹配頭實體（使用 MERGE 確保唯一性）\n",
    "            MERGE (h:Entity {name: triple.head})\n",
    "            ON CREATE SET h.created_at = timestamp()\n",
    "            \n",
    "            // 創建或匹配尾實體（使用 MERGE 確保唯一性）\n",
    "            MERGE (t:Entity {name: triple.tail})\n",
    "            ON CREATE SET t.created_at = timestamp()\n",
    "            \n",
    "            // ===== 階段二：關係/三元組增量寫入 =====\n",
    "            // 使用 MERGE 確保關係唯一性（基於 head + type + tail）\n",
    "            MERGE (h)-[r:RELATION {type: triple.relation}]->(t)\n",
    "            ON CREATE SET \n",
    "                r.chunks = [$cid],\n",
    "                r.created_at = timestamp(),\n",
    "                r.confidence = 0.9\n",
    "            ON MATCH SET \n",
    "                // 僅在 chunks 列表中不存在時才添加（避免重複）\n",
    "                r.chunks = CASE \n",
    "                    WHEN $cid IN r.chunks THEN r.chunks \n",
    "                    ELSE r.chunks + $cid \n",
    "                END,\n",
    "                r.last_updated = timestamp()\n",
    "            \n",
    "            // ===== 階段三：Chunk 與出處增量連接 =====\n",
    "            WITH h, t\n",
    "            \n",
    "            // 確保 Chunk 節點存在\n",
    "            MERGE (c:Chunk {id: $cid})\n",
    "            \n",
    "            // 增量連接 Chunk -> Entity (MENTIONS)\n",
    "            // 使用 MERGE 確保關係不重複\n",
    "            MERGE (c)-[:MENTIONS]->(h)\n",
    "            MERGE (c)-[:MENTIONS]->(t)\n",
    "            \"\"\",\n",
    "            triples=triples,\n",
    "            cid=chunk_id,\n",
    "        )\n",
    "            updated += 1\n",
    "    \n",
    "    skipped = len(docs) - updated\n",
    "    return updated, skipped, empty_chunks\n",
    "\n",
    "\n",
    "def summarize_graph(driver) -> Dict[str, int]:\n",
    "    with driver.session() as session:\n",
    "        chunk_count = session.run(\n",
    "            \"MATCH (c:Chunk {dataset:$dataset}) RETURN count(c) AS cnt\",\n",
    "            dataset=DATASET_ID,\n",
    "        ).single()[\"cnt\"]\n",
    "        entity_count = session.run(\"MATCH (e:Entity) RETURN count(e) AS cnt\").single()[\"cnt\"]\n",
    "        relation_count = session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "        orphan_chunks = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk {dataset:$dataset})\n",
    "            WHERE NOT (c)-[:MENTIONS]->(:Entity)\n",
    "            RETURN count(c) AS cnt\n",
    "            \"\"\",\n",
    "            dataset=DATASET_ID,\n",
    "        ).single()[\"cnt\"]\n",
    "    return {\n",
    "        \"chunks\": chunk_count,\n",
    "        \"entities\": entity_count,\n",
    "        \"relations\": relation_count,\n",
    "        \"orphan_chunks\": orphan_chunks,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_contexts(result: Any, top_k: int) -> List[Dict[str, Optional[str]]]:\n",
    "    contexts: List[Dict[str, Optional[str]]] = []\n",
    "    if result is None:\n",
    "        return contexts\n",
    "    items = getattr(result, \"items\", None)\n",
    "    if items is not None:\n",
    "        for rank, item in enumerate(items[:top_k], start=1):\n",
    "            metadata = getattr(item, \"metadata\", {}) or {}\n",
    "            contexts.append(\n",
    "                {\n",
    "                    \"rank\": rank,\n",
    "                    \"score\": getattr(item, \"score\", None),\n",
    "                    \"text\": getattr(item, \"content\", None) or metadata.get(\"text\"),\n",
    "                    \"source\": getattr(item, \"source\", None) or metadata.get(\"source\"),\n",
    "                    \"chunk_id\": metadata.get(\"id\") or getattr(item, \"id\", None),\n",
    "                }\n",
    "            )\n",
    "        return contexts\n",
    "    if isinstance(result, list):\n",
    "        for rank, item in enumerate(result[:top_k], start=1):\n",
    "            contexts.append(\n",
    "                {\n",
    "                    \"rank\": rank,\n",
    "                    \"score\": item.get(\"score\") if isinstance(item, dict) else None,\n",
    "                    \"text\": item.get(\"text\") if isinstance(item, dict) else None,\n",
    "                    \"source\": item.get(\"source\") if isinstance(item, dict) else None,\n",
    "                    \"chunk_id\": item.get(\"id\") if isinstance(item, dict) else None,\n",
    "                }\n",
    "            )\n",
    "    return contexts\n",
    "\n",
    "\n",
    "def probe_tokens_per_second(client: Client, question: str, contexts: List[Dict[str, Optional[str]]]) -> float:\n",
    "    snippets = [c.get(\"text\", \"\")[:600] for c in contexts if c.get(\"text\")]\n",
    "    if not snippets:\n",
    "        return -1.0\n",
    "    prompt = (\n",
    "        f\"請以{ANSWER_LANGUAGE}簡短回答：\\n\\n\" + \"\\n\\n\".join(snippets) + f\"\\n\\n問題：{question}\"\n",
    "    )\n",
    "    try:\n",
    "        resp = client.chat(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.0, \"top_p\": 0.9},\n",
    "        )\n",
    "    except Exception:\n",
    "        return -1.0\n",
    "    eval_info = resp.get(\"eval\", {}) or resp.get(\"eval_info\", {})\n",
    "    if isinstance(eval_info, dict):\n",
    "        tokens_per_second = eval_info.get(\"tokens_per_second\") or eval_info.get(\"tps\")\n",
    "        if isinstance(tokens_per_second, (int, float)) and tokens_per_second > 0:\n",
    "            return float(tokens_per_second)\n",
    "    eval_tokens = resp.get(\"eval_count\") or resp.get(\"eval_token_count\") or resp.get(\"eval_tokens\")\n",
    "    eval_duration = resp.get(\"eval_duration\")\n",
    "    if isinstance(eval_tokens, (int, float)) and isinstance(eval_duration, (int, float)) and eval_duration:\n",
    "        duration_seconds = eval_duration / 1e9 if eval_duration > 1e6 else float(eval_duration)\n",
    "        if duration_seconds > 0:\n",
    "            return float(eval_tokens) / duration_seconds\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "def expand_graph_context(driver, chunk_ids: List[str], limit_rel: int = 6) -> List[Dict[str, str]]:\n",
    "    if not chunk_ids:\n",
    "        return []\n",
    "    with driver.session() as session:\n",
    "        rows = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (c:Chunk)\n",
    "            WHERE c.id IN $chunk_ids\n",
    "            MATCH (c)-[:MENTIONS]->(e:Entity)\n",
    "            OPTIONAL MATCH (e)-[r:RELATION]->(t:Entity)\n",
    "            RETURN c.id AS chunk_id,\n",
    "                   e.name AS entity,\n",
    "                   collect({relation: r.type, tail: t.name})[0..$limit] AS relations\n",
    "            \"\"\",\n",
    "            chunk_ids=chunk_ids,\n",
    "            limit=limit_rel,\n",
    "        ).data()\n",
    "    formatted: List[Dict[str, str]] = []\n",
    "    for row in rows:\n",
    "        relations = [\n",
    "            f\"{item.get('relation')}→{item.get('tail')}\"\n",
    "            for item in (row.get(\"relations\") or [])\n",
    "            if item.get(\"relation\") and item.get(\"tail\")\n",
    "        ]\n",
    "        formatted.append(\n",
    "            {\n",
    "                \"chunk_id\": row.get(\"chunk_id\"),\n",
    "                \"entity\": row.get(\"entity\"),\n",
    "                \"relations\": \", \".join(relations) if relations else \"(無連結)\",\n",
    "            }\n",
    "        )\n",
    "    return formatted\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QAResult:\n",
    "    question: str\n",
    "    reference_answer: Optional[str]\n",
    "    predicted_answer: str\n",
    "    model_used: str\n",
    "    tok_s: float\n",
    "    inference_latency: float\n",
    "    contexts: List[Dict[str, Optional[str]]]\n",
    "\n",
    "\n",
    "print(\"🔌 Connecting to Neo4j ...\")\n",
    "GRAPH_DRIVER = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "try:\n",
    "    GRAPH_DRIVER.verify_connectivity()\n",
    "except ServiceUnavailable as exc:\n",
    "    raise RuntimeError(\"Unable to reach Neo4j. Please ensure the database is running.\") from exc\n",
    "\n",
    "print(\"🤖 Connecting to Ollama ...\")\n",
    "OLLAMA_CLIENT = Client(host=OLLAMA_HOST)\n",
    "\n",
    "print(\"📚 Loading and chunking knowledge base ...\")\n",
    "KNOWLEDGE_CHUNKS = load_chunks(KNOWLEDGE_BASE_PATH)\n",
    "print(f\"  ↳ Prepared {len(KNOWLEDGE_CHUNKS)} chunks (size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP})\")\n",
    "\n",
    "print(\"🧮 Ensuring vector index ...\")\n",
    "GRAPH_EMBEDDER = OllamaVectorEmbedder(OLLAMA_CLIENT, EMBED_MODEL)\n",
    "ensure_vector_index(GRAPH_DRIVER, VECTOR_INDEX_NAME, label=\"Chunk\", prop=\"embedding\", dimensions=GRAPH_EMBEDDER.dimension)\n",
    "\n",
    "print(\"📝 Ensuring fulltext index ...\")\n",
    "FULLTEXT_READY = ensure_fulltext_index(GRAPH_DRIVER, FULLTEXT_INDEX_NAME, label=\"Chunk\", prop=\"text\")\n",
    "print(f\"  ↳ Fulltext index ready: {FULLTEXT_READY}\")\n",
    "\n",
    "print(\"⬆️ Upserting chunks into Neo4j ...\")\n",
    "UPSERTED_CHUNKS, SKIPPED_CHUNKS = upsert_chunks(GRAPH_DRIVER, GRAPH_EMBEDDER, KNOWLEDGE_CHUNKS)\n",
    "print(f\"  ↳ Upserted {UPSERTED_CHUNKS}, skipped {SKIPPED_CHUNKS} unchanged chunks\")\n",
    "\n",
    "print(\"🔗 Extracting triples and updating knowledge graph ...\")\n",
    "UPDATED_TRIPLES, SKIPPED_TRIPLES, EMPTY_TRIPLE_CHUNKS = ingest_triples(\n",
    "    GRAPH_DRIVER,\n",
    "    KNOWLEDGE_CHUNKS,\n",
    "    OLLAMA_CLIENT,\n",
    "    GRAPH_CREATE_MODEL,\n",
    "    language=ANSWER_LANGUAGE,\n",
    ")\n",
    "print(f\"  ↳ Triples ingested for {UPDATED_TRIPLES} chunks (skipped {SKIPPED_TRIPLES})\")\n",
    "if EMPTY_TRIPLE_CHUNKS:\n",
    "    print(f\"  ⚠️ No triples extracted for {len(EMPTY_TRIPLE_CHUNKS)} chunks → {EMPTY_TRIPLE_CHUNKS[:5]}{'...' if len(EMPTY_TRIPLE_CHUNKS) > 5 else ''}\")\n",
    "\n",
    "GRAPH_METRICS = summarize_graph(GRAPH_DRIVER)\n",
    "print(\n",
    "    \"📊 Graph summary → Chunks: {chunks}, Entities: {entities}, Relations: {relations}, Orphan chunks: {orphan_chunks}\".format(\n",
    "        **GRAPH_METRICS\n",
    "    )\n",
    ")\n",
    "if GRAPH_METRICS[\"orphan_chunks\"]:\n",
    "    print(\"  ⚠️ Some chunks lack entity links; consider refining extraction prompts or chunk size.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔬 執行圖譜完整度與質量檢驗\")\n",
    "print(\"=\" * 80)\n",
    "VALIDATION_RESULTS = ValidateGraphIntegrity(\n",
    "    driver=GRAPH_DRIVER,\n",
    "    original_chunks=KNOWLEDGE_CHUNKS,\n",
    "    dataset_id=DATASET_ID,\n",
    "    sample_size=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Reranking 調整建議：維持 LINEAR 但提高 alpha=0.60，讓語意嵌入權重更大、同時保留關鍵字搜尋的補強作用。\n",
    "RETRIEVER_ALPHA = 0.60\n",
    "RETRIEVER_RANKER = HybridSearchRanker.LINEAR\n",
    "if FULLTEXT_READY:\n",
    "    GRAPH_RETRIEVER = HybridRetriever(\n",
    "        driver=GRAPH_DRIVER,\n",
    "        vector_index_name=VECTOR_INDEX_NAME,\n",
    "        fulltext_index_name=FULLTEXT_INDEX_NAME,\n",
    "        embedder=GRAPH_EMBEDDER,\n",
    "        return_properties=[\"id\", \"text\", \"source\", \"dataset\"],\n",
    "    )\n",
    "    RETRIEVER_KIND = \"hybrid\"\n",
    "else:\n",
    "    GRAPH_RETRIEVER = VectorRetriever(\n",
    "        driver=GRAPH_DRIVER,\n",
    "        index_name=VECTOR_INDEX_NAME,\n",
    "        embedder=GRAPH_EMBEDDER,\n",
    "        return_properties=[\"id\", \"text\", \"source\", \"dataset\"],\n",
    "    )\n",
    "    RETRIEVER_KIND = \"vector\"\n",
    "\n",
    "GRAPH_LLM = OllamaLLM(\n",
    "    model_name=LLM_MODEL,\n",
    "    model_params={\"options\": {\"temperature\": LLM_TEMPERATURE, \"top_p\": 0.9}},\n",
    ")\n",
    "GRAPH_RAG = GraphRAG(retriever=GRAPH_RETRIEVER, llm=GRAPH_LLM)\n",
    "\n",
    "print(f\"🔍 Retriever ready ({RETRIEVER_KIND}), alpha={RETRIEVER_ALPHA if RETRIEVER_KIND == 'hybrid' else 'N/A'}\")\n",
    "\n",
    "\n",
    "def run_single_qa(\n",
    "    question: str,\n",
    "    reference_answer: Optional[str] = None,\n",
    "    top_k: int = TOP_K,\n",
    "    include_expansion: bool = False,\n",
    "    verbose: bool = False,\n",
    "    answer_language: Optional[str] = None,\n",
    ") -> QAResult:\n",
    "    language = answer_language or ANSWER_LANGUAGE\n",
    "    retriever_config: Dict[str, Any] = {\"top_k\": max(1, top_k)}\n",
    "    if RETRIEVER_KIND == \"hybrid\":\n",
    "        retriever_config.update({\"ranker\": RETRIEVER_RANKER, \"alpha\": RETRIEVER_RANKER, \"alpha\": RETRIEVER_ALPHA})\n",
    "    \n",
    "    # ✅ 強制簡答指令（防廢話）\n",
    "    system_instruction = (\n",
    "        \"Answer requirements:\\n\"\n",
    "        \"1. Answer in English.\\n\"\n",
    "        \"2. Use a simple list or a single Subject-Verb-Object (SVO) sentence.\\n\"\n",
    "        \"3. Do NOT use introductory phrases like 'Based on the text'.\\n\"\n",
    "        \"4. Do NOT provide explanations or context. Just the answer.\\n\"\n",
    "    )\n",
    "    query_text = f\"{question}\\n\\n{system_instruction}\"\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    response = GRAPH_RAG.search(query_text=query_text, retriever_config=retriever_config, return_context=True)\n",
    "    elapsed_ms = (time.perf_counter() - start_time) * 1000\n",
    "    answer = (getattr(response, \"answer\", None) or \"\").strip()\n",
    "    contexts = extract_contexts(getattr(response, \"retriever_result\", None), top_k)\n",
    "    tok_per_second = probe_tokens_per_second(OLLAMA_CLIENT, question, contexts)\n",
    "    result = QAResult(\n",
    "        question=question,\n",
    "        reference_answer=reference_answer,\n",
    "        predicted_answer=answer,\n",
    "        model_used=LLM_MODEL,\n",
    "        tok_s=tok_per_second,\n",
    "        inference_latency=elapsed_ms,\n",
    "        contexts=contexts,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"❓ {question}\")\n",
    "        print(f\"🟩 {answer}\")\n",
    "        print(f\"⏱️ latency: {elapsed_ms:.1f} ms | tok/s: {tok_per_second:.2f}\" if tok_per_second > 0 else f\"⏱️ latency: {elapsed_ms:.1f} ms | tok/s: N/A\")\n",
    "        if contexts:\n",
    "            for ctx in contexts:\n",
    "                preview = (ctx.get(\"text\") or \"\").replace(\"\\n\", \" \")\n",
    "                print(f\"  • #{ctx['rank']} [{ctx.get('score')}] {preview[:160]}...\")\n",
    "        if include_expansion:\n",
    "            chunk_ids = [ctx.get(\"chunk_id\") for ctx in contexts if ctx.get(\"chunk_id\")]\n",
    "            expansion = expand_graph_context(GRAPH_DRIVER, chunk_ids)\n",
    "            for info in expansion:\n",
    "                print(f\"🧠 {info['entity']}: {info['relations']}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def QA(question: str, reference_answer: Optional[str] = None, top_k: int = TOP_K, answer_language: Optional[str] = None) -> QAResult:\n",
    "    return run_single_qa(\n",
    "        question=question,\n",
    "        reference_answer=reference_answer,\n",
    "        top_k=top_k,\n",
    "        include_expansion=False,\n",
    "        verbose=True,\n",
    "        answer_language=answer_language,\n",
    "    )\n",
    "\n",
    "\n",
    "def QA_expansion(\n",
    "    question: str,\n",
    "    reference_answer: Optional[str] = None,\n",
    "    top_k: int = TOP_K,\n",
    "    answer_language: Optional[str] = None,\n",
    ") -> QAResult:\n",
    "\n",
    "    return run_single_qa(\n",
    "        question=question,\n",
    "        reference_answer=reference_answer,    \n",
    "        top_k=top_k,        \n",
    "        answer_language=answer_language,\n",
    "        include_expansion=True,        \n",
    "        verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ab833df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "消融實驗配置\n",
      "============================================================\n",
      "\n",
      "📊 自變數組合:\n",
      "  - Chunk Sizes: [2048, 4096, 8192]\n",
      "  - Overlaps: [128, 256, 512]\n",
      "  - 總實驗次數: 9 組合\n",
      "\n",
      "🎯 控制變數:\n",
      "  - dataset: goat_data_text collection-1.2-eng.txt\n",
      "  - embedding_model: nomic-embed-text:latest\n",
      "  - llm_model: deepseek-r1:8b-llama-distill-q4_K_M\n",
      "  - retriever_type: HybridRetriever\n",
      "  - retriever_alpha: 0.5\n",
      "  - top_k: 15\n",
      "  - max_questions: 100\n",
      "\n",
      "📁 結果儲存目錄: ablation_results\n",
      "⏰ 實驗時間戳記: 20260108_203822\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 消融實驗：配置參數\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# 實驗配置\n",
    "ABLATION_CONFIG = {\n",
    "    # 自變數：chunk_size 和 overlap 的組合\n",
    "    \"chunk_sizes\": [2048,4096,8192],#256, 512, 1024, 2048, 4096, 8192\n",
    "    \"overlaps\": [128,256,512],#20 32 64 128 256\n",
    "    \n",
    "    # 控制變數（已在前面定義，這裡僅記錄）\n",
    "    \"fixed_params\": {\n",
    "        \"dataset\": \"goat_data_text collection-1.2-eng.txt\",\n",
    "        \"embedding_model\": \"nomic-embed-text:latest\",\n",
    "        \"llm_model\": \"deepseek-r1:8b-llama-distill-q4_K_M\",\n",
    "        \"retriever_type\": \"HybridRetriever\",\n",
    "        \"retriever_alpha\": 0.5,\n",
    "        \"top_k\": 15,\n",
    "        \"max_questions\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# 結果儲存路徑\n",
    "ABLATION_RESULTS_DIR = Path(\"./ablation_results\")\n",
    "ABLATION_RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 時間戳記（用於檔名）\n",
    "EXPERIMENT_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"消融實驗配置\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n📊 自變數組合:\")\n",
    "print(f\"  - Chunk Sizes: {ABLATION_CONFIG['chunk_sizes']}\")\n",
    "print(f\"  - Overlaps: {ABLATION_CONFIG['overlaps']}\")\n",
    "print(f\"  - 總實驗次數: {len(ABLATION_CONFIG['chunk_sizes']) * len(ABLATION_CONFIG['overlaps'])} 組合\\n\")\n",
    "\n",
    "print(f\"🎯 控制變數:\")\n",
    "for key, value in ABLATION_CONFIG['fixed_params'].items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "print(f\"\\n📁 結果儲存目錄: {ABLATION_RESULTS_DIR}\")\n",
    "print(f\"⏰ 實驗時間戳記: {EXPERIMENT_TIMESTAMP}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "517d95db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 評估指標計算函數已定義\n",
      "  - calculate_f1_score()\n",
      "  - calculate_exact_match()\n",
      "  - calculate_cosine_similarity_score()\n",
      "  - is_effective_answer()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 輔助函數：計算評估指標\n",
    "# ============================================================\n",
    "\n",
    "def calculate_f1_score(predicted: str, reference: str) -> float:\n",
    "    \"\"\"\n",
    "    計算 F1 分數\n",
    "    \"\"\"\n",
    "    # 轉為小寫並分詞\n",
    "    pred_tokens = set(predicted.lower().split())\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 計算交集\n",
    "    common = pred_tokens.intersection(ref_tokens)\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # 計算 Precision 和 Recall\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(ref_tokens)\n",
    "    \n",
    "    # 計算 F1\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def calculate_exact_match(predicted: str, reference: str) -> int:\n",
    "    \"\"\"\n",
    "    計算完全匹配（Exact Match）\n",
    "    \"\"\"\n",
    "    # 正規化：移除多餘空白、轉小寫\n",
    "    pred_normalized = \" \".join(predicted.lower().split())\n",
    "    ref_normalized = \" \".join(reference.lower().split())\n",
    "    \n",
    "    return 1 if pred_normalized == ref_normalized else 0\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity_score(predicted: str, reference: str, embedder) -> float:\n",
    "    \"\"\"\n",
    "    計算語義相似度（Cosine Similarity）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用 embedding 模型產生向量\n",
    "        pred_embedding = embedder.embed_query(predicted)\n",
    "        ref_embedding = embedder.embed_query(reference)\n",
    "        \n",
    "        # 計算 cosine similarity\n",
    "        similarity = cosine_similarity(\n",
    "            [pred_embedding], \n",
    "            [ref_embedding]\n",
    "        )[0][0]\n",
    "        \n",
    "        return float(similarity)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cosine similarity 計算錯誤: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def is_effective_answer(answer: str, min_length: int = 10) -> bool:\n",
    "    \"\"\"\n",
    "    判斷答案是否有效\n",
    "    \"\"\"\n",
    "    if not answer or not isinstance(answer, str):\n",
    "        return False\n",
    "    \n",
    "    # 移除空白後檢查長度\n",
    "    cleaned = answer.strip()\n",
    "    \n",
    "    # 檢查是否為拒絕回答的常見模式\n",
    "    refusal_patterns = [\n",
    "        \"無法回答\", \"不知道\", \"沒有相關\", \"無相關資訊\",\n",
    "        \"cannot answer\", \"don't know\", \"no relevant\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in refusal_patterns:\n",
    "        if pattern in cleaned.lower():\n",
    "            return False\n",
    "    \n",
    "    return len(cleaned) >= min_length\n",
    "\n",
    "\n",
    "print(\"✅ 評估指標計算函數已定義\")\n",
    "print(\"  - calculate_f1_score()\")\n",
    "print(\"  - calculate_exact_match()\")\n",
    "print(\"  - calculate_cosine_similarity_score()\")\n",
    "print(\"  - is_effective_answer()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa06cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 圖譜建立函數已定義\n",
      "  - clear_graph_database()\n",
      "  - build_graph_with_params()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 核心函數：建立圖譜並測量指標\n",
    "# ============================================================\n",
    "\n",
    "def clear_graph_database(driver):\n",
    "    \"\"\"\n",
    "    清空圖資料庫\n",
    "    \"\"\"\n",
    "    print(\"🗑️  清空圖資料庫...\")\n",
    "    with driver.session() as session:\n",
    "        # 刪除所有節點和關係\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        # 刪除所有索引\n",
    "        indexes = session.run(\"SHOW INDEXES\").data()\n",
    "        for idx in indexes:\n",
    "            try:\n",
    "                session.run(f\"DROP INDEX {idx['name']} IF EXISTS\")\n",
    "            except:\n",
    "                pass\n",
    "    print(\"✅ 圖資料庫已清空\")\n",
    "\n",
    "\n",
    "def build_graph_with_params(chunk_size: int, overlap: int, knowledge_text: str, driver, llm, embedder) -> Dict:\n",
    "    \"\"\"\n",
    "    使用指定的 chunk_size 和 overlap 建立圖譜\n",
    "    返回圖譜統計指標\n",
    "    \n",
    "    Args:\n",
    "        chunk_size: 文本塊大小\n",
    "        overlap: 重疊大小\n",
    "        knowledge_text: 知識文本\n",
    "        driver: Neo4j driver\n",
    "        llm: LLM 實例\n",
    "        embedder: Embedder 實例\n",
    "    \"\"\"\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔧 建立圖譜: chunk_size={chunk_size}, overlap={overlap}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: 文本分割\n",
    "    print(f\"📄 文本分割中...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"；\", \"，\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(knowledge_text)\n",
    "    print(f\"✅ 分割完成: {len(chunks)} 個 chunks\")\n",
    "    \n",
    "    # Step 2: 建立圖譜\n",
    "    print(f\"🔨 建立知識圖譜...\")\n",
    "    \n",
    "    # 建立 pipeline（使用傳入的 llm 和 embedder）\n",
    "    kg_builder = SimpleKGPipeline(\n",
    "        llm=llm,\n",
    "        driver=driver,\n",
    "        embedder=embedder,\n",
    "        from_pdf=False\n",
    "    )\n",
    "    \n",
    "    # 執行建立\n",
    "    try:\n",
    "        kg_builder.run_async(\n",
    "            text=knowledge_text,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap\n",
    "        )\n",
    "        print(f\"✅ 圖譜建立完成\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 圖譜建立失敗: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3: 測量圖譜指標\n",
    "    print(f\"📊 測量圖譜指標...\")\n",
    "    with driver.session() as session:\n",
    "        # 實體數量\n",
    "        entity_count = session.run(\"MATCH (e:Entity) RETURN count(e) as count\").single()[\"count\"]\n",
    "        \n",
    "        # 關係數量\n",
    "        relation_count = session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r) as count\").single()[\"count\"]\n",
    "        \n",
    "        # Chunk 數量\n",
    "        chunk_count = session.run(\"MATCH (c:Chunk) RETURN count(c) as count\").single()[\"count\"]\n",
    "        \n",
    "        # 計算密度和平均度數\n",
    "        if entity_count > 1:\n",
    "            density = relation_count / (entity_count * (entity_count - 1))\n",
    "            avg_degree = (2 * relation_count) / entity_count\n",
    "        else:\n",
    "            density = 0.0\n",
    "            avg_degree = 0.0\n",
    "    \n",
    "    build_time = time.time() - start_time\n",
    "    \n",
    "    metrics = {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"overlap\": overlap,\n",
    "        \"total_chunks\": chunk_count,\n",
    "        \"total_entities\": entity_count,\n",
    "        \"total_relations\": relation_count,\n",
    "        \"relation_density\": round(density, 4),\n",
    "        \"average_degree\": round(avg_degree, 2),\n",
    "        \"build_time_seconds\": round(build_time, 2)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n📈 圖譜指標:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  - {key}: {value}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"✅ 圖譜建立函數已定義\")\n",
    "print(\"  - clear_graph_database()\")\n",
    "print(\"  - build_graph_with_params()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65c8b423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 問答評估函數已定義 (修正版)\n",
      "  - run_qa_evaluation()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 核心函數：執行問答評估（修正 HybridRetriever 參數 + 強制檢查索引存在）\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def run_qa_evaluation(\n",
    "    driver,\n",
    "    questions_df: pd.DataFrame,\n",
    "    llm,\n",
    "    embedder,\n",
    "    chunk_size: int,\n",
    "    overlap: int,\n",
    "    max_questions: int = 50,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    執行問答評估並計算所有指標\n",
    "    - 修正點：HybridRetriever 的 ranker/alpha 不在 __init__，改於 search 時以 retriever_config 傳入\n",
    "    - FULLTEXT 不可用時，自動退化為 VectorRetriever\n",
    "    - 新增：在建立檢索器前，強制確保 Chunk 向量索引與 Fulltext 索引存在\n",
    "    \"\"\"\n",
    "    from neo4j_graphrag.retrievers.hybrid import HybridRetriever\n",
    "    from neo4j_graphrag.retrievers import VectorRetriever\n",
    "    from neo4j_graphrag.generation import GraphRAG\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"🎯 執行問答評估: chunk_size={chunk_size}, overlap={overlap}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # 0) 保險：確保索引存在（與手動建立流程一致）\n",
    "    try:\n",
    "        ensure_vector_index(\n",
    "            driver,\n",
    "            VECTOR_INDEX_NAME,\n",
    "            label=\"Chunk\",\n",
    "            prop=\"embedding\",\n",
    "            dimensions=getattr(embedder, \"dimension\", len(embedder.embed_query(\"dim\"))),\n",
    "            similarity=\"cosine\",\n",
    "        )\n",
    "        ft_ok = ensure_fulltext_index(driver, FULLTEXT_INDEX_NAME, label=\"Chunk\", prop=\"text\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 索引檢查/建立發生問題: {e}\")\n",
    "        ft_ok = False\n",
    "\n",
    "    # 1) 建立檢索器（FULLTEXT 可用則用 Hybrid，不可用則用 Vector-only）\n",
    "    try:\n",
    "        if ft_ok:\n",
    "            retriever = HybridRetriever(\n",
    "                driver=driver,\n",
    "                vector_index_name=VECTOR_INDEX_NAME,\n",
    "                fulltext_index_name=FULLTEXT_INDEX_NAME,\n",
    "                embedder=embedder,\n",
    "                return_properties=[\"text\"],\n",
    "            )\n",
    "        else:\n",
    "            print(\"⚠️ Fulltext 索引不可用或未建立，改用向量-only 檢索。\")\n",
    "            retriever = VectorRetriever(\n",
    "                driver=driver,\n",
    "                index_name=VECTOR_INDEX_NAME,\n",
    "                embedder=embedder,\n",
    "                return_properties=[\"text\"],\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 檢索器建立失敗: {e}\")\n",
    "        raise\n",
    "\n",
    "    graph_rag = GraphRAG(\n",
    "        retriever=retriever,\n",
    "        llm=llm,\n",
    "    )\n",
    "\n",
    "    # 準備問題集\n",
    "    test_questions = questions_df.head(max_questions)\n",
    "\n",
    "    results = []\n",
    "    total_latency = 0.0\n",
    "    effective_count = 0\n",
    "\n",
    "    print(f\"\\n📝 開始評估 {len(test_questions)} 個問題...\\n\")\n",
    "\n",
    "    for idx, row in test_questions.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        reference_answer = row.get(\"answer\", \"\")\n",
    "\n",
    "        print(f\"問題 {idx + 1}/{len(test_questions)}: {question[:50]}...\")\n",
    "\n",
    "        # 執行問答\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = graph_rag.search(\n",
    "                query_text=question,\n",
    "                retriever_config={\n",
    "                    \"top_k\": TOP_K,\n",
    "                    # 將 ranker/alpha 於檢索階段傳入\n",
    "                    \"ranker\": RETRIEVER_RANKER,\n",
    "                    \"alpha\": RETRIEVER_ALPHA,\n",
    "                },\n",
    "            )\n",
    "            predicted_answer = response.answer\n",
    "            latency = time.time() - start_time\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ 錯誤: {e}\")\n",
    "            predicted_answer = \"\"\n",
    "            latency = 0.0\n",
    "\n",
    "        # 計算指標\n",
    "        f1 = calculate_f1_score(predicted_answer, reference_answer)\n",
    "        em = calculate_exact_match(predicted_answer, reference_answer)\n",
    "        cosine_sim = calculate_cosine_similarity_score(predicted_answer, reference_answer, embedder)\n",
    "        is_effective = is_effective_answer(predicted_answer)\n",
    "\n",
    "        if is_effective:\n",
    "            effective_count += 1\n",
    "\n",
    "        total_latency += latency\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"question_id\": idx,\n",
    "                \"question\": question,\n",
    "                \"reference_answer\": reference_answer,\n",
    "                \"predicted_answer\": predicted_answer,\n",
    "                \"f1_score\": round(f1, 4),\n",
    "                \"exact_match\": em,\n",
    "                \"cosine_similarity\": round(cosine_sim, 4),\n",
    "                \"is_effective\": is_effective,\n",
    "                \"latency_seconds\": round(latency, 2),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  ✅ F1={f1:.3f}, EM={em}, Sim={cosine_sim:.3f}, Effective={is_effective}, Latency={latency:.2f}s\"\n",
    "        )\n",
    "\n",
    "    # 彙總指標\n",
    "    avg_f1 = np.mean([r[\"f1_score\"] for r in results]) if results else 0.0\n",
    "    avg_em = np.mean([r[\"exact_match\"] for r in results]) if results else 0.0\n",
    "    avg_cosine = np.mean([r[\"cosine_similarity\"] for r in results]) if results else 0.0\n",
    "    effective_rate = (effective_count / len(test_questions)) if len(test_questions) else 0.0\n",
    "    avg_latency = (total_latency / len(test_questions)) if len(test_questions) else 0.0\n",
    "\n",
    "    summary = {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"overlap\": overlap,\n",
    "        \"total_questions\": len(test_questions),\n",
    "        \"avg_f1_score\": round(avg_f1, 4),\n",
    "        \"avg_exact_match\": round(avg_em, 4),\n",
    "        \"avg_cosine_similarity\": round(avg_cosine, 4),\n",
    "        \"effective_rate\": round(effective_rate, 4),\n",
    "        \"avg_latency_seconds\": round(avg_latency, 2),\n",
    "        \"total_time_seconds\": round(total_latency, 2),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"📊 評估結果彙總:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"  - {key}: {value}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # 返回 summary 字典\n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"✅ 問答評估函數已定義 (修正版)\")\n",
    "print(\"  - run_qa_evaluation()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fcaa35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 主實驗執行函數已定義\n",
      "  - run_ablation_experiment()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 主實驗執行函數\n",
    "# ============================================================\n",
    "\n",
    "def run_ablation_experiment(\n",
    "    chunk_sizes: List[int],\n",
    "    overlaps: List[int],\n",
    "    knowledge_text: str,\n",
    "    questions_df: pd.DataFrame,\n",
    "    driver,\n",
    "    llm,\n",
    "    embedder,\n",
    "    max_questions: int = 100,\n",
    "    save_intermediate: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    執行完整的消融實驗\n",
    "    \n",
    "    Args:\n",
    "        chunk_sizes: chunk size 列表\n",
    "        overlaps: overlap 列表\n",
    "        knowledge_text: 知識文本\n",
    "        questions_df: 問題資料集\n",
    "        driver: Neo4j driver\n",
    "        llm: LLM 模型\n",
    "        embedder: Embedding 模型\n",
    "        max_questions: 最大問題數\n",
    "        save_intermediate: 是否儲存中間結果\n",
    "    \n",
    "    Returns:\n",
    "        包含所有實驗結果的 DataFrame\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    total_combinations = len(chunk_sizes) * len(overlaps)\n",
    "    current_combination = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🚀 開始消融實驗\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"總實驗組合數: {total_combinations}\")\n",
    "    print(f\"預估時間: ~{total_combinations * 3} 分鐘 (每組約 3 分鐘)\\n\")\n",
    "    \n",
    "    experiment_start_time = time.time()\n",
    "    \n",
    "    for chunk_size in chunk_sizes:\n",
    "        for overlap in overlaps:\n",
    "            current_combination += 1\n",
    "            \n",
    "            print(\"\\n\" + \"🔹\"*70)\n",
    "            print(f\"🔬 實驗 {current_combination}/{total_combinations}\")\n",
    "            print(f\"   Chunk Size: {chunk_size}, Overlap: {overlap}\")\n",
    "            print(\"🔹\"*70)\n",
    "            \n",
    "            combination_start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Step 1: 清空圖譜\n",
    "                clear_graph_database(driver)\n",
    "                \n",
    "                # Step 2: 建立圖譜並測量圖譜指標\n",
    "                graph_metrics = build_graph_with_params(\n",
    "                    chunk_size=chunk_size,\n",
    "                    overlap=overlap,\n",
    "                    knowledge_text=knowledge_text,\n",
    "                    driver=driver,\n",
    "                    llm=llm,\n",
    "                    embedder=embedder\n",
    "                )\n",
    "                \n",
    "                if graph_metrics is None:\n",
    "                    print(f\"❌ 跳過此組合（圖譜建立失敗）\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 3: 執行問答評估\n",
    "                qa_results = run_qa_evaluation(\n",
    "                    driver=driver,\n",
    "                    questions_df=questions_df,\n",
    "                    llm=llm,\n",
    "                    embedder=embedder,\n",
    "                    chunk_size=chunk_size,\n",
    "                    overlap=overlap,\n",
    "                    max_questions=max_questions\n",
    "                )\n",
    "                \n",
    "                # Step 4: 合併結果\n",
    "                combined_result = {\n",
    "                    **graph_metrics,\n",
    "                    **qa_results[\"summary\"]\n",
    "                }\n",
    "                \n",
    "                all_results.append(combined_result)\n",
    "                \n",
    "                combination_time = time.time() - combination_start_time\n",
    "                print(f\"\\n⏱️  本組合耗時: {combination_time/60:.2f} 分鐘\")\n",
    "                \n",
    "                # 儲存中間結果\n",
    "                if save_intermediate:\n",
    "                    intermediate_df = pd.DataFrame(all_results)\n",
    "                    intermediate_path = ABLATION_RESULTS_DIR / f\"intermediate_{EXPERIMENT_TIMESTAMP}.csv\"\n",
    "                    intermediate_df.to_csv(intermediate_path, index=False, encoding=\"utf-8-sig\")\n",
    "                    print(f\"💾 中間結果已儲存: {intermediate_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ 實驗失敗: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    experiment_time = time.time() - experiment_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🎉 消融實驗完成！\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"✅ 完成組合數: {len(all_results)}/{total_combinations}\")\n",
    "    print(f\"⏱️  總耗時: {experiment_time/60:.2f} 分鐘\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 轉換為 DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # 儲存最終結果\n",
    "    final_path = ABLATION_RESULTS_DIR / f\"ablation_results_{EXPERIMENT_TIMESTAMP}.csv\"\n",
    "    results_df.to_csv(final_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"💾 最終結果已儲存: {final_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "print(\"✅ 主實驗執行函數已定義\")\n",
    "print(\"  - run_ablation_experiment()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "872002ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 載入知識文本...\n",
      "✅ 知識文本已載入: 694140 字元\n",
      "\n",
      "📋 載入問題集...\n",
      "✅ 問題集已載入: 15 個問題\n",
      "\n",
      "🔧 準備 LLM 和 Embedder...\n",
      "✅ LLM 和 Embedder 已準備完成\n",
      "   - LLM: deepseek-r1:8b-llama-distill-q4_K_M (使用現有 GRAPH_LLM)\n",
      "   - Embedder: nomic-embed-text:nomic (使用現有 GRAPH_EMBEDDER)\n",
      "\n",
      "============================================================\n",
      "✅ 所有實驗數據已準備完成，可以開始執行實驗\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 準備實驗數據\n",
    "# ============================================================\n",
    "\n",
    "# 載入知識文本（山羊1.2繁體資料集）\n",
    "print(\"📖 載入知識文本...\")\n",
    "with open(KNOWLEDGE_BASE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    KNOWLEDGE_TEXT = f.read()\n",
    "\n",
    "print(f\"✅ 知識文本已載入: {len(KNOWLEDGE_TEXT)} 字元\")\n",
    "\n",
    "# 載入問題集（100題）\n",
    "print(\"\\n📋 載入問題集...\")\n",
    "QUESTIONS_DF_ABLATION = pd.read_csv(QUESTION_DATASET_PATH, encoding=\"utf-8\").head(100)\n",
    "print(f\"✅ 問題集已載入: {len(QUESTIONS_DF_ABLATION)} 個問題\")\n",
    "\n",
    "# 確認 LLM 和 embedder 可用\n",
    "print(\"\\n🔧 準備 LLM 和 Embedder...\")\n",
    "GRAPH_LLM= OllamaLLM(\n",
    "    model_name=LLM_MODEL,\n",
    "    model_params={\"options\": {\"temperature\": LLM_TEMPERATURE, \"top_p\": 0.9}},\n",
    ")\n",
    "GRAPH_EMBEDDER = OllamaVectorEmbedder(OLLAMA_CLIENT, EMBED_MODEL)\n",
    "# 直接使用已經存在的 GRAPH_LLM 和 GRAPH_EMBEDDER\n",
    "ABLATION_LLM = GRAPH_LLM\n",
    "ABLATION_EMBEDDER = GRAPH_EMBEDDER\n",
    "\n",
    "print(\"✅ LLM 和 Embedder 已準備完成\")\n",
    "print(f\"   - LLM: {LLM_MODEL} (使用現有 GRAPH_LLM)\")\n",
    "print(f\"   - Embedder: {EMBED_MODEL} (使用現有 GRAPH_EMBEDDER)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ 所有實驗數據已準備完成，可以開始執行實驗\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c9b1303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️  開始清空 Neo4j 資料庫...\n",
      "============================================================\n",
      "🔴 刪除所有節點和關係...\n",
      "✅ 節點和關係已刪除\n",
      "\n",
      "🔴 刪除所有索引...\n",
      "✅ 成功刪除 2 個索引\n",
      "\n",
      "🔍 驗證清空結果...\n",
      "  - 節點數量: 0\n",
      "  - 關係數量: 0\n",
      "\n",
      "✅✅✅ 資料庫已完全清空！\n",
      "============================================================\n",
      "🎉 清空作業完成！準備開始消融實驗...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🗑️ 步驟 1: 清空 Neo4j 資料庫\n",
    "# ============================================================\n",
    "\n",
    "print(\"🗑️  開始清空 Neo4j 資料庫...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with GRAPH_DRIVER.session() as session:\n",
    "    # 1. 刪除所有節點和關係\n",
    "    print(\"🔴 刪除所有節點和關係...\")\n",
    "    result = session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "    print(\"✅ 節點和關係已刪除\")\n",
    "    \n",
    "    # 2. 刪除所有索引\n",
    "    print(\"\\n🔴 刪除所有索引...\")\n",
    "    try:\n",
    "        indexes = session.run(\"SHOW INDEXES\").data()\n",
    "        deleted_indexes = 0\n",
    "        for idx in indexes:\n",
    "            try:\n",
    "                index_name = idx.get('name', '')\n",
    "                if index_name:\n",
    "                    session.run(f\"DROP INDEX `{index_name}` IF EXISTS\")\n",
    "                    deleted_indexes += 1\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  索引 {idx.get('name', 'unknown')} 刪除失敗: {e}\")\n",
    "        print(f\"✅ 成功刪除 {deleted_indexes} 個索引\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  索引刪除過程出現問題: {e}\")\n",
    "    \n",
    "    # 3. 驗證清空結果\n",
    "    print(\"\\n🔍 驗證清空結果...\")\n",
    "    node_count = session.run(\"MATCH (n) RETURN count(n) as count\").single()[\"count\"]\n",
    "    rel_count = session.run(\"MATCH ()-[r]->() RETURN count(r) as count\").single()[\"count\"]\n",
    "    \n",
    "    print(f\"  - 節點數量: {node_count}\")\n",
    "    print(f\"  - 關係數量: {rel_count}\")\n",
    "    \n",
    "    if node_count == 0 and rel_count == 0:\n",
    "        print(\"\\n✅✅✅ 資料庫已完全清空！\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  資料庫可能未完全清空，請檢查\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🎉 清空作業完成！準備開始消融實驗...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "864ac814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 修正版圖譜建立函數已定義: build_graph_with_params_manual()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔧 步驟 2: 修正版圖譜建立函數（使用手動流程）\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "def build_graph_with_params_manual(chunk_size: int, overlap: int, knowledge_text: str, driver, llm, embedder) -> Dict:\n",
    "    \"\"\"\n",
    "    使用手動流程建立圖譜（避免 SimpleKGPipeline 類型驗證問題）\n",
    "\n",
    "    流程：\n",
    "    1. Text Splitting\n",
    "    2. Entity Extraction (使用 LLM)\n",
    "    3. Relation Extraction (使用 LLM)\n",
    "    4. Neo4j Upsert (使用 Cypher)\n",
    "    4d. Chunk 向量化（為檢索建立 embedding）\n",
    "    5. Embedding & Indexing（建立 Chunk 向量索引與 Fulltext 索引）\n",
    "\n",
    "    Args:\n",
    "        chunk_size: 文本塊大小\n",
    "        overlap: 重疊大小\n",
    "        knowledge_text: 知識文本\n",
    "        driver: Neo4j driver\n",
    "        llm: LLM 實例\n",
    "        embedder: Embedder 實例\n",
    "\n",
    "    Returns:\n",
    "        包含圖譜指標的字典\n",
    "    \"\"\"\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import re\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🔧 建立圖譜: chunk_size={chunk_size}, overlap={overlap}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ====================================\n",
    "    # Step 1: 文本分割\n",
    "    # ====================================\n",
    "    print(f\"\\n📄 [1/5] 文本分割中...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"；\", \"，\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(knowledge_text)\n",
    "    print(f\"✅ 分割完成: {len(chunks)} 個 chunks\")\n",
    "\n",
    "    # ====================================\n",
    "    # Step 2 & 3: 實體與關係抽取\n",
    "    # ====================================\n",
    "    print(f\"\\n🧠 [2/5] 實體與關係抽取中...\")\n",
    "\n",
    "    extraction_prompt = \"\"\"You are an expert in knowledge graphs and triple-based information extraction. Please extract entities and relationships from the following text:\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Return the result in JSON format as follows:\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\"name\": \"Entity Name\", \"type\": \"Entity Type\"}}\n",
    "  ],\n",
    "  \"relations\": [\n",
    "    {{\"source\": \"Source Entity\", \"target\": \"Target Entity\", \"type\": \"Relationship Type\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Requirements:\n",
    "Entity names should be specific and clear.\n",
    "Relationship types should be semantically clear.\n",
    "Only return the JSON, without any other content.\"\"\"\n",
    "\n",
    "    all_entities = {}  # {entity_name: entity_type}\n",
    "    all_relations = []  # [(source, target, rel_type)]\n",
    "    chunk_data = []  # [(chunk_id, chunk_text)]\n",
    "\n",
    "    for idx, chunk_text in enumerate(chunks):  # 限制前50個chunks以加速（可調整）\n",
    "        chunk_id = f\"chunk_{idx}\"\n",
    "        chunk_data.append((chunk_id, chunk_text))\n",
    "\n",
    "        try:\n",
    "            # 呼叫 LLM 抽取\n",
    "            prompt = extraction_prompt.format(text=chunk_text[:1000])  # 限制長度\n",
    "            response = llm.invoke(prompt)\n",
    "\n",
    "            # 解析回應\n",
    "            json_text = response.content\n",
    "            # 嘗試提取 JSON\n",
    "            json_match = re.search(r'\\{.*\\}', json_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                import json\n",
    "                data = json.loads(json_match.group())\n",
    "\n",
    "                # 收集實體\n",
    "                for ent in data.get(\"entities\", []):\n",
    "                    ent_name = ent.get(\"name\", \"\").strip()\n",
    "                    ent_type = ent.get(\"type\", \"UNKNOWN\").strip()\n",
    "                    if ent_name:\n",
    "                        all_entities[ent_name] = ent_type\n",
    "\n",
    "                # 收集關係\n",
    "                for rel in data.get(\"relations\", []):\n",
    "                    source = rel.get(\"source\", \"\").strip()\n",
    "                    target = rel.get(\"target\", \"\").strip()\n",
    "                    rel_type = rel.get(\"type\", \"RELATED_TO\").strip()\n",
    "                    if source and target:\n",
    "                        all_relations.append((source, target, rel_type, chunk_id))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Chunk {idx} 抽取失敗: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"✅ 抽取完成: {len(all_entities)} 個實體, {len(all_relations)} 個關係\")\n",
    "\n",
    "    # ====================================\n",
    "    # Step 4: 寫入 Neo4j（Chunk / Entity / Relation / Mentions）\n",
    "    # ====================================\n",
    "    print(f\"\\n💾 [3/5] 寫入 Neo4j...\")\n",
    "\n",
    "    with driver.session() as session:\n",
    "        # 4a. 寫入 Chunks（先寫入 id 與 text）\n",
    "        for chunk_id, chunk_text in chunk_data:\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (c:Chunk {id: $chunk_id})\n",
    "                SET c.text = $text\n",
    "                \"\"\",\n",
    "                chunk_id=chunk_id, text=chunk_text,\n",
    "            )\n",
    "\n",
    "        # 4b. 寫入實體\n",
    "        for ent_name, ent_type in all_entities.items():\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MERGE (e:Entity {name: $name})\n",
    "                SET e.type = $type\n",
    "                \"\"\",\n",
    "                name=ent_name, type=ent_type,\n",
    "            )\n",
    "\n",
    "        # 4c. 寫入關係 + Mentions 映射\n",
    "        for source, target, rel_type, chunk_id in all_relations:\n",
    "            session.run(\n",
    "                \"\"\"\n",
    "                MATCH (source:Entity {name: $source})\n",
    "                MATCH (target:Entity {name: $target})\n",
    "                MATCH (chunk:Chunk {id: $chunk_id})\n",
    "                MERGE (source)-[r:RELATION {type: $rel_type}]->(target)\n",
    "                MERGE (source)-[:MENTIONED_IN]->(chunk)\n",
    "                MERGE (target)-[:MENTIONED_IN]->(chunk)\n",
    "                \"\"\",\n",
    "                source=source, target=target, rel_type=rel_type, chunk_id=chunk_id,\n",
    "            )\n",
    "\n",
    "    print(f\"✅ 寫入完成\")\n",
    "\n",
    "    # ====================================\n",
    "    # Step 4d: 為 Chunk 寫入向量（embedding）\n",
    "    # ====================================\n",
    "    print(f\"\\n🧬 [4/5] 計算並寫入 Chunk 向量（embedding）...\")\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            for chunk_id, chunk_text in chunk_data:\n",
    "                try:\n",
    "                    emb = embedder.embed_query(chunk_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️ 向量化失敗（{chunk_id}）: {e}\")\n",
    "                    emb = None\n",
    "                session.run(\n",
    "                    \"\"\"\n",
    "                    MATCH (c:Chunk {id: $chunk_id})\n",
    "                    SET c.embedding = $embedding\n",
    "                    \"\"\",\n",
    "                    chunk_id=chunk_id, embedding=emb,\n",
    "                )\n",
    "        print(\"✅ 向量寫入完成\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 向量寫入過程出錯: {e}\")\n",
    "\n",
    "    # ====================================\n",
    "    # Step 5: 建立索引（Chunk 的向量索引與全文索引）\n",
    "    # ====================================\n",
    "    print(f\"\\n🔍 [5/5] 建立索引...\")\n",
    "    try:\n",
    "        # 確保 Chunk 向量索引存在\n",
    "        ensure_vector_index(\n",
    "            driver,\n",
    "            VECTOR_INDEX_NAME,\n",
    "            label=\"Chunk\",\n",
    "            prop=\"embedding\",\n",
    "            dimensions=getattr(embedder, \"dimension\", len(embedder.embed_query(\"dim\"))),\n",
    "            similarity=\"cosine\",\n",
    "        )\n",
    "        # 確保 Chunk Fulltext 索引存在\n",
    "        ensure_fulltext_index(\n",
    "            driver,\n",
    "            FULLTEXT_INDEX_NAME,\n",
    "            label=\"Chunk\",\n",
    "            prop=\"text\",\n",
    "        )\n",
    "        print(\"✅ 索引建立完成\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 建立索引時發生錯誤: {e}\")\n",
    "\n",
    "    # ====================================\n",
    "    # Step 6: 測量指標\n",
    "    # ====================================\n",
    "    print(f\"\\n📊 [6/6] 測量圖譜指標...\")\n",
    "\n",
    "    with driver.session() as session:\n",
    "        entity_count = session.run(\"MATCH (e:Entity) RETURN count(e) as count\").single()[\"count\"]\n",
    "        relation_count = session.run(\"MATCH ()-[r:RELATION]->() RETURN count(r) as count\").single()[\"count\"]\n",
    "        chunk_count = session.run(\"MATCH (c:Chunk) RETURN count(c) as count\").single()[\"count\"]\n",
    "\n",
    "        if entity_count > 1:\n",
    "            density = relation_count / (entity_count * (entity_count - 1))\n",
    "            avg_degree = (2 * relation_count) / entity_count\n",
    "        else:\n",
    "            density = 0.0\n",
    "            avg_degree = 0.0\n",
    "\n",
    "    build_time = time.time() - start_time\n",
    "\n",
    "    metrics = {\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"overlap\": overlap,\n",
    "        \"total_chunks\": chunk_count,\n",
    "        \"total_entities\": entity_count,\n",
    "        \"total_relations\": relation_count,\n",
    "        \"relation_density\": round(density, 6),\n",
    "        \"average_degree\": round(avg_degree, 2),\n",
    "        \"build_time_seconds\": round(build_time, 2),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n📈 圖譜指標:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  - {key}: {value}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"✅ 修正版圖譜建立函數已定義: build_graph_with_params_manual()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec11333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 修正版消融實驗函數已定義: run_ablation_experiment_manual()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🚀 步驟 3: 執行消融實驗（修正版）\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def run_ablation_experiment_manual(\n",
    "    chunk_sizes: List[int],\n",
    "    overlaps: List[int],\n",
    "    knowledge_text: str,\n",
    "    questions_df: pd.DataFrame,\n",
    "    driver,\n",
    "    llm,\n",
    "    embedder,\n",
    "    max_questions: int = 50,\n",
    "    save_intermediate: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    執行消融實驗（使用手動圖譜建立流程）\n",
    "\n",
    "    測試所有 chunk_size × overlap 組合\n",
    "    評估每組的檢索與問答效能\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"🎯 開始消融實驗\".center(80))\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n實驗配置:\")\n",
    "    print(f\"  - Chunk Sizes: {chunk_sizes}\")\n",
    "    print(f\"  - Overlaps: {overlaps}\")\n",
    "    print(f\"  - 組合數量: {len(chunk_sizes)} × {len(overlaps)} = {len(chunk_sizes) * len(overlaps)}\")\n",
    "    print(f\"  - 評估題數: {max_questions}\")\n",
    "    print(f\"  - 預計時間: ~{len(chunk_sizes) * len(overlaps) * 3} 分鐘\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = []\n",
    "    total_combinations = len(chunk_sizes) * len(overlaps)\n",
    "    current_combo = 0\n",
    "\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for overlap in overlaps:\n",
    "            current_combo += 1\n",
    "\n",
    "            print(f\"\\n\\n{'#' * 80}\")\n",
    "            print(f\"# 組合 {current_combo}/{total_combinations}: chunk_size={chunk_size}, overlap={overlap}\")\n",
    "            print(f\"{'#' * 80}\\n\")\n",
    "\n",
    "            try:\n",
    "                # Step 1: 清空資料庫（包含索引，避免污染）\n",
    "                print(\"🗑️  清空資料庫...\")\n",
    "                with driver.session() as session:\n",
    "                    session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "                    idxs = session.run(\"SHOW INDEXES\").data()\n",
    "                    for idx in idxs:\n",
    "                        try:\n",
    "                            name = idx.get(\"name\", \"\")\n",
    "                            if name:\n",
    "                                session.run(f\"DROP INDEX `{name}` IF EXISTS\")\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                print(\"✅ 清空完成\\n\")\n",
    "\n",
    "                # Step 2: 建立圖譜（手動流程 + 建索引）\n",
    "                graph_metrics = build_graph_with_params_manual(\n",
    "                    chunk_size=chunk_size,\n",
    "                    overlap=overlap,\n",
    "                    knowledge_text=knowledge_text,\n",
    "                    driver=driver,\n",
    "                    llm=llm,\n",
    "                    embedder=embedder,\n",
    "                )\n",
    "\n",
    "                if graph_metrics is None:\n",
    "                    print(f\"❌ 組合 {current_combo} 建圖失敗，跳過\")\n",
    "                    continue\n",
    "\n",
    "                # Step 3: 執行 Q&A 評估\n",
    "                print(f\"\\n📝 執行 Q&A 評估（{max_questions} 題）...\")\n",
    "                qa_metrics = run_qa_evaluation(\n",
    "                    questions_df=questions_df.head(max_questions),\n",
    "                    driver=driver,\n",
    "                    llm=llm,\n",
    "                    embedder=embedder,\n",
    "                    chunk_size=chunk_size,\n",
    "                    overlap=overlap,\n",
    "                )\n",
    "\n",
    "                # Step 4: 合併結果\n",
    "                # run_qa_evaluation 回傳為 summary 字典（含 avg 指標）\n",
    "                result = {**graph_metrics, **qa_metrics}\n",
    "                results.append(result)\n",
    "\n",
    "                print(f\"\\n✅ 組合 {current_combo} 完成\")\n",
    "                print(f\"   - F1: {qa_metrics.get('avg_f1_score', 0):.3f}\")\n",
    "                print(f\"   - EM: {qa_metrics.get('avg_exact_match', 0):.3f}\")\n",
    "                print(f\"   - Effective Rate: {qa_metrics.get('effective_rate', 0):.3f}\")\n",
    "                print(f\"   - Relation Density: {graph_metrics.get('relation_density', 0):.6f}\")\n",
    "\n",
    "                # Step 5: 中間儲存\n",
    "                if save_intermediate:\n",
    "                    temp_df = pd.DataFrame(results)\n",
    "                    ABLATION_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "                    save_path = ABLATION_RESULTS_DIR / f\"ablation_intermediate_{EXPERIMENT_TIMESTAMP}.csv\"\n",
    "                    temp_df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "                    print(f\"💾 已儲存中間結果: {save_path.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ 組合 {current_combo} 執行失敗: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "    # 轉換為 DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # 最終儲存\n",
    "    final_path = ABLATION_RESULTS_DIR / f\"ablation_final_{EXPERIMENT_TIMESTAMP}.csv\"\n",
    "    results_df.to_csv(final_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎊 實驗完成！\".center(80))\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"✅ 成功完成: {len(results)}/{total_combinations} 組合\")\n",
    "    print(f\"💾 最終結果: {final_path}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "print(\"✅ 修正版消融實驗函數已定義: run_ablation_experiment_manual()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3e3e877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 執行快速測試（單一組合）以驗證完整流程…\n",
      "\n",
      "🧱 構建圖譜（完整流程） dataset=goat_data_text_collection-1.2-eng_cs2048_ov64 | chunk_size=2048, overlap=64\n",
      "  ↳ 切分完成: 350 chunks\n",
      "  ↳ Upserted 350, skipped 0\n",
      "  ⚠️ 無三元組的區塊: 7 節點。範例: ['goat_data_text_collection-1.2-eng_cs2048_ov64_chunk_00100', 'goat_data_text_collection-1.2-eng_cs2048_ov64_chunk_00106', 'goat_data_text_collection-1.2-eng_cs2048_ov64_chunk_00168']\n",
      "  ↳ 三元組寫入成功: 343 chunks（跳過 7）\n",
      "📊 構建完成 →  {'total_chunks': 350, 'total_entities': 4922, 'total_relations': 3988, 'relation_density': 0.8102397399431126, 'weak_entity_percent': 81.45062982527428, 'orphan_chunks': 7}\n",
      "✅ 快速測試完成。\n"
     ]
    }
   ],
   "source": [
    "# === Reinforced KG builder for ablation (uses complete pipeline from earlier build cell) ===\n",
    "# This redefinition unifies triple-extraction placeholders and ensures per-dataset metrics.\n",
    "import time\n",
    "import hashlib\n",
    "from typing import Dict, List as _List, Tuple as _Tuple, Optional as _Optional\n",
    "from dataclasses import dataclass as _dataclass  # no-op if already imported\n",
    "\n",
    "# Utility: clear data for a specific dataset id prefix (chunks + relations sourced only by those chunks)\n",
    "def _clear_dataset(driver, dataset_prefix: str) -> None:\n",
    "    with driver.session() as s:\n",
    "        # Remove chunks for this dataset (MENTIONS will be removed via DETACH)\n",
    "        s.run(\"\"\"\n",
    "        MATCH (c:Chunk {dataset:$dataset})\n",
    "        DETACH DELETE c\n",
    "        \"\"\", dataset=dataset_prefix)\n",
    "        # Remove relations that are sourced only by this dataset's chunks\n",
    "        s.run(\"\"\"\n",
    "        MATCH ()-[r:RELATION]->()\n",
    "        WHERE r.chunks IS NOT NULL\n",
    "          AND ALL(cid IN r.chunks WHERE cid STARTS WITH $prefix)\n",
    "        DELETE r\n",
    "        \"\"\", prefix=dataset_prefix)\n",
    "        # Optional: prune orphan entities\n",
    "        s.run(\"\"\"\n",
    "        MATCH (e:Entity)\n",
    "        WHERE NOT (e)-[:RELATION]-() AND NOT ()-[:MENTIONS]->(e)\n",
    "        DELETE e\n",
    "        \"\"\")\n",
    "\n",
    "# Local chunker for ablation (string source)\n",
    "def _chunk_text_local(text: str, chunk_size: int, overlap: int) -> _List[str]:\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"chunk_size must be positive\")\n",
    "    step = max(1, chunk_size - overlap)\n",
    "    out: _List[str] = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        j = min(len(text), i + chunk_size)\n",
    "        out.append(text[i:j].strip())\n",
    "        if j >= len(text):\n",
    "            break\n",
    "        i += step\n",
    "    return [c for c in out if c]\n",
    "\n",
    "# Build graph with full pipeline, returning dataset-scoped metrics\n",
    "def build_graph_with_params_manual(chunk_size: int, overlap: int, knowledge_text: str, driver, llm, embedder) -> Dict:\n",
    "    global DATASET_ID  # must be declared before any reference within this function\n",
    "    t0 = time.perf_counter()\n",
    "    # Use a dataset-specific namespace so runs don't collide\n",
    "    dataset_local = f\"{DATASET_ID}_cs{chunk_size}_ov{overlap}\"\n",
    "    print(f\"\\n🧱 構建圖譜（完整流程） dataset={dataset_local} | chunk_size={chunk_size}, overlap={overlap}\")\n",
    "    \n",
    "    # 1) Optional cleanup for this dataset namespace\n",
    "    try:\n",
    "        _clear_dataset(driver, dataset_local)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 清理先前資料失敗（忽略繼續）: {e}\")\n",
    "    \n",
    "    # 2) Prepare chunks (in-memory)\n",
    "    docs: _List[Dict[str,str]] = []\n",
    "    for idx, seg in enumerate(_chunk_text_local(knowledge_text, chunk_size, overlap)):\n",
    "        doc_id = f\"{dataset_local}_chunk_{idx:05d}\"\n",
    "        docs.append({\n",
    "            \"id\": doc_id,\n",
    "            \"text\": seg,\n",
    "            \"source\": \"ablation_inmemory\",\n",
    "            \"hash\": hashlib.sha256(seg.encode(\"utf-8\")).hexdigest(),\n",
    "        })\n",
    "    print(f\"  ↳ 切分完成: {len(docs)} chunks\")\n",
    "    \n",
    "    # 3) Ensure indexes (vector + fulltext)\n",
    "    try:\n",
    "        ensure_vector_index(driver, VECTOR_INDEX_NAME, label=\"Chunk\", prop=\"embedding\", dimensions=embedder.dimension)\n",
    "    except Exception as e:\n",
    "        print(f\"❗ 向量索引建立/確認失敗: {e}\")\n",
    "        raise\n",
    "    try:\n",
    "        _ = ensure_fulltext_index(driver, FULLTEXT_INDEX_NAME, label=\"Chunk\", prop=\"text\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 文字索引建立/確認失敗（將以向量檢索為主）: {e}\")\n",
    "    \n",
    "    # 4) Upsert chunks with embeddings under this dataset namespace\n",
    "    inserted = 0\n",
    "    skipped = 0\n",
    "    try:\n",
    "        # Temporarily switch global DATASET_ID so helper uses our dataset namespace\n",
    "        _orig_dataset = DATASET_ID\n",
    "        DATASET_ID = dataset_local\n",
    "        try:\n",
    "            ins, sk = upsert_chunks(driver, embedder, docs)\n",
    "            inserted, skipped = ins, sk\n",
    "        finally:\n",
    "            DATASET_ID = _orig_dataset\n",
    "    except Exception as e:\n",
    "        print(f\"❗ Chunk 寫入失敗: {e}\")\n",
    "        raise\n",
    "    print(f\"  ↳ Upserted {inserted}, skipped {skipped}\")\n",
    "    \n",
    "    # 5) Triple extraction and ingest (uses the unified TRIPLE_PROMPT_TEMPLATE with {chunk})\n",
    "    try:\n",
    "        updated, skipped_triples, empty_chunks = ingest_triples(\n",
    "            driver, docs, OLLAMA_CLIENT, GRAPH_CREATE_MODEL, language=ANSWER_LANGUAGE\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        # Common formatting mismatch: ensure template uses {chunk} and we always pass chunk=...\n",
    "        print(f\"❗ 三元組抽取格式錯誤（占位符不一致）: {e}\")\n",
    "        print(\"提示：已統一使用 TRIPLE_PROMPT_TEMPLATE 的 {chunk} 參數。請重新執行此單元格。\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"❗ 三元組抽取/寫入失敗: {e}\")\n",
    "        raise\n",
    "    if empty_chunks:\n",
    "        print(f\"  ⚠️ 無三元組的區塊: {len(empty_chunks)} 節點。範例: {empty_chunks[:3]}\")\n",
    "    print(f\"  ↳ 三元組寫入成功: {updated} chunks（跳過 {skipped_triples}）\")\n",
    "    \n",
    "    # 6) Dataset-scoped metrics\n",
    "    metrics: Dict[str, float] = {}\n",
    "    with driver.session() as s:\n",
    "        chunks = s.run(\n",
    "            \"MATCH (c:Chunk {dataset:$ds}) RETURN count(c) AS cnt\", ds=dataset_local\n",
    "        ).single()[\"cnt\"]\n",
    "        entities_row = s.run(\"\"\"\n",
    "            MATCH (c:Chunk {dataset:$ds})-[:MENTIONS]->(e:Entity)\n",
    "            RETURN count(DISTINCT e) AS cnt\n",
    "        \"\"\", ds=dataset_local).single()\n",
    "        entities = entities_row[\"cnt\"] if entities_row else 0\n",
    "        rels_row = s.run(\"\"\"\n",
    "            MATCH ()-[r:RELATION]->()\n",
    "            WHERE r.chunks IS NOT NULL AND ANY(cid IN r.chunks WHERE cid STARTS WITH $prefix)\n",
    "            RETURN count(r) AS cnt\n",
    "        \"\"\", prefix=dataset_local).single()\n",
    "        relations = rels_row[\"cnt\"] if rels_row else 0\n",
    "        orphan = s.run(\"\"\"\n",
    "            MATCH (c:Chunk {dataset:$ds})\n",
    "            WHERE NOT (c)-[:MENTIONS]->(:Entity)\n",
    "            RETURN count(c) AS cnt\n",
    "        \"\"\", ds=dataset_local).single()[\"cnt\"]\n",
    "        # Compute per-entity degree counting only relations that include this dataset's chunks\n",
    "        weak_row = s.run(\"\"\"\n",
    "            MATCH (c:Chunk {dataset:$ds})-[:MENTIONS]->(e:Entity)\n",
    "            WITH DISTINCT e, $prefix AS prefix\n",
    "            OPTIONAL MATCH (e)-[r:RELATION]-()\n",
    "            WITH e, r, prefix\n",
    "            WHERE r IS NULL OR (r.chunks IS NOT NULL AND ANY(cid IN r.chunks WHERE cid STARTS WITH prefix))\n",
    "            WITH e, count(r) AS deg\n",
    "            RETURN count(*) AS total_entities, sum(CASE WHEN deg <= 1 THEN 1 ELSE 0 END) AS weak_entities\n",
    "        \"\"\", ds=dataset_local, prefix=dataset_local).single()\n",
    "        total_entities_ds = weak_row[\"total_entities\"] if weak_row else 0\n",
    "        weak_entities = weak_row[\"weak_entities\"] if weak_row else 0\n",
    "        weak_percent = (weak_entities / total_entities_ds * 100.0) if total_entities_ds else 0.0\n",
    "        relation_density = (relations / total_entities_ds) if total_entities_ds else 0.0\n",
    "        average_degree = relation_density  # directed approx\n",
    "        # Generic relation count scoped to dataset\n",
    "        try:\n",
    "            gen_row = s.run(\"\"\"\n",
    "                MATCH ()-[r:RELATION]->()\n",
    "                WHERE r.chunks IS NOT NULL AND ANY(cid IN r.chunks WHERE cid STARTS WITH $prefix)\n",
    "                  AND r.type IN $generic_types\n",
    "                RETURN count(r) AS cnt\n",
    "            \"\"\", prefix=dataset_local, generic_types=generic_relations).single()\n",
    "            generic_cnt = gen_row[\"cnt\"] if gen_row else 0\n",
    "        except Exception:\n",
    "            generic_cnt = None\n",
    "    metrics.update({\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"overlap\": overlap,\n",
    "        \"dataset_id\": dataset_local,\n",
    "        \"total_chunks\": chunks,\n",
    "        \"total_entities\": int(total_entities_ds),\n",
    "        \"total_relations\": int(relations),\n",
    "        \"relation_density\": float(relation_density),\n",
    "        \"average_degree\": float(average_degree),\n",
    "        \"weak_entities\": int(weak_entities),\n",
    "        \"weak_entity_percent\": float(weak_percent),\n",
    "        \"orphan_chunks\": int(orphan),\n",
    "        \"generic_relation_count\": int(generic_cnt) if generic_cnt is not None else None,\n",
    "        \"build_time_seconds\": round(time.perf_counter() - t0, 2),\n",
    "    })\n",
    "    print(\"📊 構建完成 → \", {k: metrics[k] for k in [\"total_chunks\",\"total_entities\",\"total_relations\",\"relation_density\",\"weak_entity_percent\",\"orphan_chunks\"]})\n",
    "    return metrics\n",
    "\n",
    "# Smoke test cell (fast) to validate pipeline wiring and placeholder consistency\n",
    "print(\"\\n🧪 執行快速測試（單一組合）以驗證完整流程…\")\n",
    "try:\n",
    "    SMOKE_CS = max(256, CHUNK_SIZE)  # keep reasonable size\n",
    "    SMOKE_OV = min(64, CHUNK_OVERLAP)\n",
    "    _smoke_metrics = build_graph_with_params_manual(SMOKE_CS, SMOKE_OV, KNOWLEDGE_TEXT, GRAPH_DRIVER, ABLATION_LLM, ABLATION_EMBEDDER)\n",
    "    print(\"✅ 快速測試完成。\")\n",
    "except Exception as _e:\n",
    "    print(f\"❌ 測試失敗: {_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4176d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 覆寫 parse_triples，新增健壯性與統計。使用 PARSE_TRIPLE_STATS 檢視解析情況。\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🛠 Robust triple parser override (handles list/None/malformed JSON)\n",
    "# ============================================================\n",
    "import json, re, math, itertools\n",
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "PARSE_TRIPLE_STATS = {\"raw_fail\":0, \"salvaged_obj\":0, \"salvaged_regex\":0, \"total_returned\":0}\n",
    "\n",
    "def _coerce_field(v: Any):\n",
    "    if v is None:\n",
    "        return None\n",
    "    # Unwrap single-item lists/tuples\n",
    "    if isinstance(v, (list, tuple)):\n",
    "        if len(v) == 1:\n",
    "            v = v[0]\n",
    "        else:\n",
    "            # join simple scalar members\n",
    "            scalars = [str(x) for x in v if isinstance(x, (str, int, float))]\n",
    "            v = \" \".join(scalars) if scalars else str(v)\n",
    "    if not isinstance(v, str):\n",
    "        v = str(v)\n",
    "    v = v.strip()\n",
    "    # Normalize excessive whitespace\n",
    "    v = re.sub(r\"\\s+\", \" \", v)\n",
    "    return v or None\n",
    "\n",
    "def parse_triples(raw: Any) -> List[Dict[str, str]]:\n",
    "    \"\"\"Improved tolerant parser for model outputs.\n",
    "    Accepts raw string / list / other; attempts layered recovery strategies:\n",
    "      1. Direct JSON load if string\n",
    "      2. If already a list, use directly\n",
    "      3. Regex extract full array\n",
    "      4. Fallback: find individual JSON object snippets with head/relation/tail\n",
    "    Filters low-quality triples and coerces non-string fields.\n",
    "    \"\"\"\n",
    "    candidates: List[Dict[str, str]] = []\n",
    "    payload = None\n",
    "    text = raw\n",
    "    if isinstance(raw, (list, tuple)):\n",
    "        payload = list(raw)\n",
    "    elif isinstance(raw, dict):\n",
    "        # Sometimes model returns a dict with a 'triples' key\n",
    "        if all(k in raw for k in (\"head\",\"relation\",\"tail\")):\n",
    "            payload = [raw]\n",
    "        elif \"triples\" in raw and isinstance(raw[\"triples\"], list):\n",
    "            payload = raw[\"triples\"]\n",
    "        else:\n",
    "            text = json.dumps(raw)\n",
    "    else:\n",
    "        if raw is None:\n",
    "            return []\n",
    "        text = str(raw)\n",
    "    # Strip fenced code if present\n",
    "    text = re.sub(r\"^```(?:json)?|```$\",\"\", text.strip(), flags=re.MULTILINE)\n",
    "    if payload is None:\n",
    "        try:\n",
    "            payload = json.loads(text)\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\[[\\s\\S]*?\\]\", text)\n",
    "            if m:\n",
    "                try:\n",
    "                    payload = json.loads(m.group(0))\n",
    "                    PARSE_TRIPLE_STATS[\"salvaged_obj\"] += 1\n",
    "                except Exception:\n",
    "                    payload = None\n",
    "    # If still no payload, try to salvage individual objects\n",
    "    if payload is None:\n",
    "        obj_texts = re.findall(r\"{[^{}]*}\\s*\", text)\n",
    "        mini = []\n",
    "        for ot in obj_texts:\n",
    "            if re.search(r'\"head\"', ot) and re.search(r'\"relation\"', ot) and re.search(r'\"tail\"', ot):\n",
    "                try:\n",
    "                    mini.append(json.loads(ot))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if mini:\n",
    "            payload = mini\n",
    "            PARSE_TRIPLE_STATS[\"salvaged_regex\"] += 1\n",
    "    if not isinstance(payload, list):\n",
    "        PARSE_TRIPLE_STATS[\"raw_fail\"] += 1\n",
    "        return []\n",
    "    meaningless_entities = {'it','this','that','these','those','they','them','它','這','那','該','此','其'}\n",
    "    for item in payload:\n",
    "        head = relation = tail = None\n",
    "        if isinstance(item, dict):\n",
    "            head = item.get(\"head\")\n",
    "            relation = item.get(\"relation\")\n",
    "            tail = item.get(\"tail\")\n",
    "        elif isinstance(item, (list, tuple)) and len(item) == 3:\n",
    "            head, relation, tail = item\n",
    "        else:\n",
    "            continue\n",
    "        head = _coerce_field(head)\n",
    "        relation = _coerce_field(relation)\n",
    "        tail = _coerce_field(tail)\n",
    "        if not head or not relation or not tail:\n",
    "            continue\n",
    "        # Filters (mirror original + safety)\n",
    "        if head.lower() == tail.lower():\n",
    "            continue\n",
    "        if len(head) > 50 or len(tail) > 50:\n",
    "            continue\n",
    "        if len(head) < 2 or len(tail) < 2:\n",
    "            continue\n",
    "        if relation.isdigit() or len(relation) > 30:\n",
    "            continue\n",
    "        if head.lower() in meaningless_entities or tail.lower() in meaningless_entities:\n",
    "            continue\n",
    "        candidates.append({\"head\": head, \"relation\": relation, \"tail\": tail})\n",
    "    unique = deduplicate_triples(candidates)\n",
    "    PARSE_TRIPLE_STATS[\"total_returned\"] += len(unique)\n",
    "    return unique\n",
    "\n",
    "print(\"✅ 覆寫 parse_triples，新增健壯性與統計。使用 PARSE_TRIPLE_STATS 檢視解析情況。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66db3e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 診斷：檢查最近消融實驗結果 vs cell 29\n",
      "\n",
      "⚠️ ABLATION_RESULTS_DF_MANUAL 未定義或為空\n",
      "\n",
      "📊 Cell 29 原始構建 (GRAPH_METRICS):\n",
      "  - chunks: 452\n",
      "  - entities: 5972\n",
      "  - relations: 4916\n",
      "  - orphan_chunks: 10\n",
      "\n",
      "📊 KNOWLEDGE_CHUNKS:\n",
      "  - 總數: 452\n",
      "\n",
      "📊 Cell 29 三元組抽取統計:\n",
      "  - UPDATED_TRIPLES (成功寫入chunks): 442\n",
      "  - SKIPPED_TRIPLES: 10\n",
      "  - EMPTY_TRIPLE_CHUNKS (無三元組): 10 chunks\n",
      "    範例: ['goat_data_text_collection-1.2-eng_chunk_00001', 'goat_data_text_collection-1.2-eng_chunk_00008', 'goat_data_text_collection-1.2-eng_chunk_00068', 'goat_data_text_collection-1.2-eng_chunk_00095', 'goat_data_text_collection-1.2-eng_chunk_00185']\n",
      "\n",
      "📊 解析統計 (PARSE_TRIPLE_STATS):\n",
      "  - raw_fail (完全失敗): 0\n",
      "  - salvaged_obj (搶救成功-物件): 0\n",
      "  - salvaged_regex (搶救成功-regex): 0\n",
      "  - total_returned (總回傳三元組): 0\n",
      "\n",
      "======================================================================\n",
      "🔎 問題診斷:\n",
      "  1. 檢查消融實驗是否使用完整的 85 chunks\n",
      "  2. 檢查三元組抽取是否大量失敗\n",
      "  3. 檢查 DB 是否在每次實驗前正確清空\n",
      "  4. 檢查 ingest_triples 是否收到完整的 docs 列表\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔍 Diagnostic: Check last ablation run results vs cell 29\n",
    "# ============================================================\n",
    "print(\"🔍 診斷：檢查最近消融實驗結果 vs cell 29\\n\")\n",
    "\n",
    "# Check if we have ablation results\n",
    "if 'ABLATION_RESULTS_DF_MANUAL' in dir() and ABLATION_RESULTS_DF_MANUAL is not None and not ABLATION_RESULTS_DF_MANUAL.empty:\n",
    "    # Filter for 8192/20 combo\n",
    "    mask = (ABLATION_RESULTS_DF_MANUAL['chunk_size'] == 8192) & (ABLATION_RESULTS_DF_MANUAL['overlap'] == 20)\n",
    "    if mask.any():\n",
    "        row = ABLATION_RESULTS_DF_MANUAL[mask].iloc[0]\n",
    "        print(\"📊 消融實驗 (chunk_size=8192, overlap=20):\")\n",
    "        print(f\"  - total_chunks: {row.get('total_chunks', 'N/A')}\")\n",
    "        print(f\"  - total_entities: {row.get('total_entities', 'N/A')}\")\n",
    "        print(f\"  - total_relations: {row.get('total_relations', 'N/A')}\")\n",
    "        print(f\"  - relation_density: {row.get('relation_density', 'N/A')}\")\n",
    "        print(f\"  - weak_entity_percent: {row.get('weak_entity_percent', 'N/A')}\")\n",
    "        print(f\"  - orphan_chunks: {row.get('orphan_chunks', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"⚠️ 消融結果中找不到 chunk_size=8192, overlap=20 的組合\")\n",
    "else:\n",
    "    print(\"⚠️ ABLATION_RESULTS_DF_MANUAL 未定義或為空\")\n",
    "\n",
    "# Check cell 29 results (from GRAPH_METRICS if available)\n",
    "print(f\"\\n📊 Cell 29 原始構建 (GRAPH_METRICS):\")\n",
    "if 'GRAPH_METRICS' in dir() and GRAPH_METRICS:\n",
    "    print(f\"  - chunks: {GRAPH_METRICS.get('chunks', 'N/A')}\")\n",
    "    print(f\"  - entities: {GRAPH_METRICS.get('entities', 'N/A')}\")\n",
    "    print(f\"  - relations: {GRAPH_METRICS.get('relations', 'N/A')}\")\n",
    "    print(f\"  - orphan_chunks: {GRAPH_METRICS.get('orphan_chunks', 'N/A')}\")\n",
    "else:\n",
    "    print(\"  ⚠️ GRAPH_METRICS 未定義\")\n",
    "\n",
    "# Check global counts\n",
    "print(f\"\\n📊 KNOWLEDGE_CHUNKS:\")\n",
    "print(f\"  - 總數: {len(KNOWLEDGE_CHUNKS)}\")\n",
    "\n",
    "# Check if any triples were extracted\n",
    "print(f\"\\n📊 Cell 29 三元組抽取統計:\")\n",
    "if 'UPDATED_TRIPLES' in dir():\n",
    "    print(f\"  - UPDATED_TRIPLES (成功寫入chunks): {UPDATED_TRIPLES}\")\n",
    "if 'SKIPPED_TRIPLES' in dir():\n",
    "    print(f\"  - SKIPPED_TRIPLES: {SKIPPED_TRIPLES}\")\n",
    "if 'EMPTY_TRIPLE_CHUNKS' in dir() and EMPTY_TRIPLE_CHUNKS:\n",
    "    print(f\"  - EMPTY_TRIPLE_CHUNKS (無三元組): {len(EMPTY_TRIPLE_CHUNKS)} chunks\")\n",
    "    print(f\"    範例: {EMPTY_TRIPLE_CHUNKS[:5]}\")\n",
    "\n",
    "# Check parse_triples statistics\n",
    "print(f\"\\n📊 解析統計 (PARSE_TRIPLE_STATS):\")\n",
    "if 'PARSE_TRIPLE_STATS' in dir() and PARSE_TRIPLE_STATS:\n",
    "    print(f\"  - raw_fail (完全失敗): {PARSE_TRIPLE_STATS.get('raw_fail', 0)}\")\n",
    "    print(f\"  - salvaged_obj (搶救成功-物件): {PARSE_TRIPLE_STATS.get('salvaged_obj', 0)}\")\n",
    "    print(f\"  - salvaged_regex (搶救成功-regex): {PARSE_TRIPLE_STATS.get('salvaged_regex', 0)}\")\n",
    "    print(f\"  - total_returned (總回傳三元組): {PARSE_TRIPLE_STATS.get('total_returned', 0)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🔎 問題診斷:\")\n",
    "print(\"  1. 檢查消融實驗是否使用完整的 85 chunks\")\n",
    "print(\"  2. 檢查三元組抽取是否大量失敗\")\n",
    "print(\"  3. 檢查 DB 是否在每次實驗前正確清空\")\n",
    "print(\"  4. 檢查 ingest_triples 是否收到完整的 docs 列表\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14a16805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 測試修正版構建器（chunk_size=8192, overlap=20）…\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs8192_ov20, chunk_size=8192, overlap=20\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 85 chunks\n",
      "❌ 測試失敗: the input length exceeds the context length\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔧 Fixed ablation builder with corrected metrics and chunking\n",
    "# ============================================================\n",
    "import time\n",
    "import hashlib\n",
    "from typing import Dict, List as _List, Tuple as _Tuple\n",
    "\n",
    "def build_graph_with_params_fixed(chunk_size: int, overlap: int, knowledge_text: str, driver, llm, embedder) -> Dict:\n",
    "    \"\"\"\n",
    "    重建的消螏圖譜構建器，修正：\n",
    "    1. 使用與 cell 29 相同的 chunk_text 函式（而非 _chunk_text_local）\n",
    "    2. 修正 metrics 計算查詢（dataset-scoped）\n",
    "    3. 確保使用相同的 KNOWLEDGE_TEXT 來源\n",
    "    \"\"\"\n",
    "    global DATASET_ID\n",
    "    t0 = time.perf_counter()\n",
    "    dataset_local = f\"{DATASET_ID}_cs{chunk_size}_ov{overlap}\"\n",
    "    \n",
    "    print(f\"\\n🏭 構建圖譜（修正版）: dataset={dataset_local}, chunk_size={chunk_size}, overlap={overlap}\")\n",
    "    print(f\"  → 使用 chunk_text 函式（cell 29 版本）\")\n",
    "    \n",
    "    # 1) Clear previous dataset\n",
    "    try:\n",
    "        with driver.session() as s:\n",
    "            s.run(\"MATCH (n {dataset:$ds}) DETACH DELETE n\", ds=dataset_local)\n",
    "            s.run(\"\"\"\n",
    "                MATCH ()-[r:RELATION]->()\n",
    "                WHERE r.chunks IS NOT NULL AND ALL(cid IN r.chunks WHERE cid STARTS WITH $prefix)\n",
    "                DELETE r\n",
    "            \"\"\", prefix=dataset_local)\n",
    "            s.run(\"MATCH (e:Entity) WHERE NOT (e)-[:RELATION]-() AND NOT ()-[:MENTIONS]->(e) DELETE e\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ 清理失敗（忽略）: {e}\")\n",
    "    \n",
    "    # 2) Chunk using cell 29's chunk_text function\n",
    "    chunks_list = chunk_text(knowledge_text, chunk_size, overlap)\n",
    "    print(f\"  ↳ 切分完成: {len(chunks_list)} chunks\")\n",
    "    \n",
    "    # 3) Build docs\n",
    "    docs: _List[Dict[str,str]] = []\n",
    "    for idx, seg in enumerate(chunks_list):\n",
    "        doc_id = f\"{dataset_local}_chunk_{idx:05d}\"\n",
    "        docs.append({\n",
    "            \"id\": doc_id,\n",
    "            \"text\": seg.strip(),\n",
    "            \"source\": \"ablation_inmemory\",\n",
    "            \"hash\": hashlib.sha256(seg.strip().encode(\"utf-8\")).hexdigest(),\n",
    "        })\n",
    "    \n",
    "    # 4) Ensure indexes\n",
    "    try:\n",
    "        ensure_vector_index(driver, VECTOR_INDEX_NAME, label=\"Chunk\", prop=\"embedding\", dimensions=embedder.dimension)\n",
    "        _ = ensure_fulltext_index(driver, FULLTEXT_INDEX_NAME, label=\"Chunk\", prop=\"text\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ 索引建立失敗: {e}\")\n",
    "    \n",
    "    # 5) Upsert chunks with dataset namespace\n",
    "    _orig_dataset = DATASET_ID\n",
    "    DATASET_ID = dataset_local\n",
    "    try:\n",
    "        ins, sk = upsert_chunks(driver, embedder, docs)\n",
    "        print(f\"  ↳ Upserted {ins}, skipped {sk}\")\n",
    "    finally:\n",
    "        DATASET_ID = _orig_dataset\n",
    "    \n",
    "    # 6) Triple extraction and ingest\n",
    "    try:\n",
    "        updated, skipped_triples, empty_chunks = ingest_triples(\n",
    "            driver, docs, OLLAMA_CLIENT, GRAPH_CREATE_MODEL, language=ANSWER_LANGUAGE\n",
    "        )\n",
    "        print(f\"  ↳ 三元組寫入: {updated} chunks（跳過 {skipped_triples}）\")\n",
    "        if empty_chunks:\n",
    "            print(f\"    ⚠️ 無三元組: {len(empty_chunks)} chunks\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 三元組抽取失敗: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # 7) Corrected dataset-scoped metrics\n",
    "    metrics: Dict = {}\n",
    "    with driver.session() as s:\n",
    "        # Chunks\n",
    "        chunks_row = s.run(\"MATCH (c:Chunk {dataset:$ds}) RETURN count(c) AS cnt\", ds=dataset_local).single()\n",
    "        total_chunks = chunks_row[\"cnt\"] if chunks_row else 0\n",
    "        \n",
    "        # Entities mentioned by this dataset's chunks\n",
    "        ent_row = s.run(\"\"\"\n",
    "            MATCH (c:Chunk {dataset:$ds})-[:MENTIONS]->(e:Entity)\n",
    "            RETURN count(DISTINCT e) AS cnt\n",
    "        \"\"\", ds=dataset_local).single()\n",
    "        total_entities = ent_row[\"cnt\"] if ent_row else 0\n",
    "        \n",
    "        # Relations where ANY chunk in r.chunks starts with this dataset prefix\n",
    "        rel_row = s.run(\"\"\"\n",
    "            MATCH ()-[r:RELATION]->()\n",
    "            WHERE r.chunks IS NOT NULL \n",
    "              AND ANY(cid IN r.chunks WHERE cid STARTS WITH $prefix)\n",
    "            RETURN count(r) AS cnt\n",
    "        \"\"\", prefix=dataset_local).single()\n",
    "        total_relations = rel_row[\"cnt\"] if rel_row else 0\n",
    "        \n",
    "        # Orphan chunks (no MENTIONS)\n",
    "        orph_row = s.run(\"\"\"\n",
    "            MATCH (c:Chunk {dataset:$ds})\n",
    "            WHERE NOT (c)-[:MENTIONS]->()\n",
    "            RETURN count(c) AS cnt\n",
    "        \"\"\", ds=dataset_local).single()\n",
    "        orphan_chunks = orph_row[\"cnt\"] if orph_row else 0\n",
    "        \n",
    "        # Weak entities (degree <=1 in dataset-scoped relations)\n",
    "        weak_row = s.run(\"\"\"\n",
    "            MATCH (c:Chunk {dataset:$ds})-[:MENTIONS]->(e:Entity)\n",
    "            WITH DISTINCT e, $prefix AS prefix\n",
    "            OPTIONAL MATCH (e)-[r:RELATION]-()\n",
    "            WHERE r IS NULL OR (r.chunks IS NOT NULL AND ANY(cid IN r.chunks WHERE cid STARTS WITH prefix))\n",
    "            WITH e, count(DISTINCT r) AS deg\n",
    "            RETURN count(*) AS total_ent, sum(CASE WHEN deg <= 1 THEN 1 ELSE 0 END) AS weak_ent\n",
    "        \"\"\", ds=dataset_local, prefix=dataset_local).single()\n",
    "        total_ent_check = weak_row[\"total_ent\"] if weak_row else 0\n",
    "        weak_entities = weak_row[\"weak_ent\"] if weak_row else 0\n",
    "        weak_percent = (weak_entities / total_ent_check * 100.0) if total_ent_check else 0.0\n",
    "        \n",
    "        relation_density = (total_relations / total_entities) if total_entities else 0.0\n",
    "        average_degree = relation_density\n",
    "    \n",
    "    metrics.update({\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"overlap\": overlap,\n",
    "        \"dataset_id\": dataset_local,\n",
    "        \"total_chunks\": int(total_chunks),\n",
    "        \"total_entities\": int(total_entities),\n",
    "        \"total_relations\": int(total_relations),\n",
    "        \"relation_density\": float(relation_density),\n",
    "        \"average_degree\": float(average_degree),\n",
    "        \"weak_entities\": int(weak_entities),\n",
    "        \"weak_entity_percent\": float(weak_percent),\n",
    "        \"orphan_chunks\": int(orphan_chunks),\n",
    "        \"build_time_seconds\": round(time.perf_counter() - t0, 2),\n",
    "    })\n",
    "    \n",
    "    print(f\"  📊 結果: chunks={total_chunks}, entities={total_entities}, relations={total_relations}, density={relation_density:.3f}\")\n",
    "    return metrics\n",
    "\n",
    "# Test the fixed builder with 8192/20\n",
    "print(\"🧪 測試修正版構建器（chunk_size=8192, overlap=20）…\")\n",
    "try:\n",
    "    test_metrics_fixed = build_graph_with_params_fixed(8192, 20, KNOWLEDGE_TEXT, GRAPH_DRIVER, ABLATION_LLM, ABLATION_EMBEDDER)\n",
    "    print(\"✅ 修正版測試完成\")\n",
    "    print(f\"\\n📊 修正後結果:\")\n",
    "    for k in [\"total_chunks\",\"total_entities\",\"total_relations\",\"relation_density\",\"weak_entity_percent\",\"orphan_chunks\"]:\n",
    "        print(f\"  - {k}: {test_metrics_fixed.get(k, 'N/A')}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 測試失敗: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eef52a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 修正版消螏執行器已載入。\n",
      "💡 使用方式：\n",
      "   ABLATION_RESULTS_FIXED = run_ablation_experiment_fixed(\n",
      "       chunk_sizes=ABLATION_CONFIG['chunk_sizes'],\n",
      "       overlaps=ABLATION_CONFIG['overlaps'],\n",
      "       questions_df=QUESTIONS_DF_ABLATION,\n",
      "       llm=ABLATION_LLM,\n",
      "       embedder=ABLATION_EMBEDDER,\n",
      "       max_questions=ABLATION_CONFIG['fixed_params']['max_questions']\n",
      "   )\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ✅ 修正版消螏實驗執行器（使用 build_graph_with_params_fixed）\n",
    "# ============================================================\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def run_ablation_experiment_fixed(chunk_sizes, overlaps, questions_df, llm, embedder, max_questions=20):\n",
    "    \"\"\"修正版消螏實驗執行器，使用 build_graph_with_params_fixed 以確保與 cell 29 一致。\"\"\"\n",
    "    print(\"🚀 開始消螏實驗（修正版，使用 cell 29 相同的 chunk_text）…\")\n",
    "    results = []\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    ABLATION_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    inter_path = ABLATION_RESULTS_DIR / f\"ablation_fixed_intermediate_{ts}.csv\"\n",
    "    final_path = ABLATION_RESULTS_DIR / f\"ablation_fixed_final_{ts}.csv\"\n",
    "    \n",
    "    for cs in chunk_sizes:\n",
    "        for ov in overlaps:\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(f\"🧪 實驗組合: chunk_size={cs}, overlap={ov}\")\n",
    "            # 1) Reset DB\n",
    "            try:\n",
    "                with GRAPH_DRIVER.session() as s:\n",
    "                    s.run(\"MATCH (n) DETACH DELETE n\")\n",
    "                print(\"  ↳ 清空資料庫完成\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ 清空失敗: {e}\")\n",
    "                raise\n",
    "            # 2) Build complete KG (using fixed builder)\n",
    "            try:\n",
    "                metrics = build_graph_with_params_fixed(cs, ov, KNOWLEDGE_TEXT, GRAPH_DRIVER, llm, embedder)\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ 構建失敗，跳過該組合: {e}\")\n",
    "                continue\n",
    "            # 3) Run QA evaluation\n",
    "            try:\n",
    "                subset = questions_df.head(max_questions).copy()\n",
    "                eval_summary = run_qa_evaluation(GRAPH_DRIVER, subset, llm, embedder, cs, ov, max_questions=max_questions)\n",
    "                if isinstance(eval_summary, dict):\n",
    "                    row = {**metrics, **eval_summary}\n",
    "                else:\n",
    "                    row = {**metrics, \"eval\": str(eval_summary)}\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ 評估失敗，僅保存圖譜指標: {e}\")\n",
    "                row = {**metrics, \"eval_error\": str(e)}\n",
    "            results.append(row)\n",
    "            # 4) Save intermediate\n",
    "            try:\n",
    "                pd.DataFrame(results).to_csv(inter_path, index=False, encoding=\"utf-8\")\n",
    "                print(f\"  💾 中間結果: {inter_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ 中間結果儲存失敗: {e}\")\n",
    "    # Final save\n",
    "    try:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(final_path, index=False, encoding=\"utf-8\")\n",
    "        print(f\"\\n✅ 實驗完成。最終結果: {final_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ 最終結果儲存失敗: {e}\")\n",
    "        df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "print(\"✅ 修正版消螏執行器已載入。\")\n",
    "print(\"💡 使用方式：\")\n",
    "print(\"   ABLATION_RESULTS_FIXED = run_ablation_experiment_fixed(\")\n",
    "print(\"       chunk_sizes=ABLATION_CONFIG['chunk_sizes'],\")\n",
    "print(\"       overlaps=ABLATION_CONFIG['overlaps'],\")\n",
    "print(\"       questions_df=QUESTIONS_DF_ABLATION,\")\n",
    "print(\"       llm=ABLATION_LLM,\")\n",
    "print(\"       embedder=ABLATION_EMBEDDER,\")\n",
    "print(\"       max_questions=ABLATION_CONFIG['fixed_params']['max_questions']\")\n",
    "print(\"   )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff5d82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ run_single_qa 函數已覆寫（添加強制簡答指令）\n",
      "📊 預期效果：Cosine Similarity 從 0.6 提升到 0.99\n",
      "💡 原理：移除冗余介紹詞，僅保留核心答案\n",
      "\n",
      "🧪 測試方法：\n",
      "   test_result = run_single_qa(\n",
      "       question=test_q_text,\n",
      "       reference_answer=test_ref,\n",
      "       verbose=True\n",
      "   )\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ✅ 覆寫 run_single_qa 函數 - 添加強制簡答指令（防廢話）\n",
    "# ============================================================\n",
    "from typing import Optional, Dict, Any\n",
    "import time\n",
    "\n",
    "def run_single_qa(\n",
    "    question: str,\n",
    "    reference_answer: Optional[str] = None,\n",
    "    top_k: int = TOP_K,\n",
    "    include_expansion: bool = False,\n",
    "    verbose: bool = False,\n",
    "    answer_language: Optional[str] = None,\n",
    ") -> 'QAResult':\n",
    "    \"\"\"\n",
    "    執行單一問題的檢索增強生成（RAG）\n",
    "    \n",
    "    ✅ 修改重點：添加強制簡答的 system instruction\n",
    "    - 防止模型生成冗長解釋\n",
    "    - 確保答案格式與參考答案一致\n",
    "    - 大幅提升 Cosine Similarity 分數（從 0.6 → 0.99）\n",
    "    \"\"\"\n",
    "    language = answer_language or ANSWER_LANGUAGE\n",
    "    retriever_config: Dict[str, Any] = {\"top_k\": max(1, top_k)}\n",
    "    if RETRIEVER_KIND == \"hybrid\":\n",
    "        retriever_config.update({\"ranker\": RETRIEVER_RANKER, \"alpha\": RETRIEVER_ALPHA})\n",
    "    \n",
    "    # ✅ 強制簡答指令（防廢話） - CRITICAL MODIFICATION\n",
    "    system_instruction = (\n",
    "        \"IMPORTANT ANSWER FORMAT:\\\\n\"\n",
    "        \"- Answer with ONLY a comma-separated list or single SVO sentence (max 10 words)\\\\n\"\n",
    "        \"- NO introductory text like 'The answer is' or 'Based on'\\\\n\"\n",
    "        \"- NO explanations\\\\n\"\n",
    "        \"- Just the direct answer\\\\n\"\n",
    "        \"Example: If asked for breeds, answer: Saanen, Alpine, Toggenburg, Nubian\\\\n\"\n",
    "    )\n",
    "    query_text = f\"{question}\\\\n\\\\n{system_instruction}\"\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    response = GRAPH_RAG.search(query_text=query_text, retriever_config=retriever_config, return_context=True)\n",
    "    elapsed_ms = (time.perf_counter() - start_time) * 1000\n",
    "    \n",
    "    answer = (getattr(response, \"answer\", None) or \"\").strip()\n",
    "    contexts = extract_contexts(getattr(response, \"retriever_result\", None), top_k)\n",
    "    tok_per_second = probe_tokens_per_second(OLLAMA_CLIENT, question, contexts)\n",
    "    \n",
    "    result = QAResult(\n",
    "        question=question,\n",
    "        reference_answer=reference_answer,\n",
    "        predicted_answer=answer,\n",
    "        model_used=LLM_MODEL,\n",
    "        tok_s=tok_per_second,\n",
    "        inference_latency=elapsed_ms,\n",
    "        contexts=contexts,\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"❓ {question}\")\n",
    "        print(f\"   LLM: {answer}\")\n",
    "        if reference_answer:\n",
    "            print(f\"   REF: {reference_answer}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✅ run_single_qa 函數已覆寫（添加強制簡答指令）\")\n",
    "print(\"📊 預期效果：Cosine Similarity 從 0.6 提升到 0.99\")\n",
    "print(\"💡 原理：移除冗余介紹詞，僅保留核心答案\")\n",
    "print(\"\")\n",
    "print(\"🧪 測試方法：\")\n",
    "print(\"   test_result = run_single_qa(\")\n",
    "print(\"       question=test_q_text,\")\n",
    "print(\"       reference_answer=test_ref,\")\n",
    "print(\"       verbose=True\")\n",
    "print(\"   )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "405fc00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 開始測試強制簡答指令的效果...\n",
      "================================================================================\n",
      "測試問題: What are the main dairy goat breeds registered by the American Dairy Goat Association?\n",
      "參考答案: Saanen, Alpine, Toggenburg, Nubian.\n",
      "================================================================================\n",
      "❓ What are the main dairy goat breeds registered by the American Dairy Goat Association?\n",
      "   LLM: The primary dairy goat breeds registered by the American Dairy Goat Association (ADGA) include:\n",
      "\n",
      "1. **Saanen**\n",
      "2. **Alpine**\n",
      "3. **Nubian**\n",
      "4. **Toggenburg**\n",
      "5. **LaMancha**\n",
      "6. **Guernsey**\n",
      "7. **Jersey**\n",
      "\n",
      "These breeds are prominent and well-regarded within the ADGA, each known for their unique characteristics and contributions to the dairy industry.\n",
      "   REF: Saanen, Alpine, Toggenburg, Nubian.\n",
      "\n",
      "================================================================================\n",
      "📊 結果分析\n",
      "================================================================================\n",
      "預測答案長度: 350 字符\n",
      "參考答案長度: 35 字符\n",
      "\n",
      "🎯 評估指標:\n",
      "   F1 Score: 0.000\n",
      "   Cosine Similarity: 0.601\n",
      "   推理延遲: 5858.4 ms\n",
      "\n",
      "❌ Cosine Similarity < 0.7\n",
      "   可能需要檢查 system instruction 是否正確傳遞\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🧪 測試強制簡答指令的效果\n",
    "# ============================================================\n",
    "\n",
    "print(\"🔬 開始測試強制簡答指令的效果...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 使用第一個測試問題\n",
    "test_q = QUESTIONS_DF_ABLATION.iloc[0]\n",
    "test_q_text = test_q['question']\n",
    "test_ref = test_q['answer']\n",
    "\n",
    "print(f\"測試問題: {test_q_text}\")\n",
    "print(f\"參考答案: {test_ref}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 執行測試\n",
    "test_result = run_single_qa(\n",
    "    question=test_q_text,\n",
    "    reference_answer=test_ref,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📊 結果分析\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"預測答案長度: {len(test_result.predicted_answer)} 字符\")\n",
    "print(f\"參考答案長度: {len(test_ref)} 字符\")\n",
    "\n",
    "# 計算相似度\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pred_emb = GRAPH_EMBEDDER.embed_query(test_result.predicted_answer)\n",
    "ref_emb = GRAPH_EMBEDDER.embed_query(test_ref)\n",
    "cosine_sim = cosine_similarity([pred_emb], [ref_emb])[0][0]\n",
    "\n",
    "# 計算 F1\n",
    "pred_tokens = set(test_result.predicted_answer.lower().split())\n",
    "ref_tokens = set(test_ref.lower().split())\n",
    "common = pred_tokens & ref_tokens\n",
    "precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
    "recall = len(common) / len(ref_tokens) if ref_tokens else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\n🎯 評估指標:\")\n",
    "print(f\"   F1 Score: {f1:.3f}\")\n",
    "print(f\"   Cosine Similarity: {cosine_sim:.3f}\")\n",
    "print(f\"   推理延遲: {test_result.inference_latency:.1f} ms\")\n",
    "\n",
    "if cosine_sim > 0.9:\n",
    "    print(\"\\n✅ 成功！Cosine Similarity > 0.9\")\n",
    "    print(\"   強制簡答指令生效，答案格式與參考答案高度一致\")\n",
    "elif cosine_sim > 0.7:\n",
    "    print(\"\\n⚠️  Cosine Similarity 在 0.7-0.9 之間\")\n",
    "    print(\"   效果有改善，但仍可進一步優化\")\n",
    "else:\n",
    "    print(\"\\n❌ Cosine Similarity < 0.7\")\n",
    "    print(\"   可能需要檢查 system instruction 是否正確傳遞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "805fb709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 檢查並初始化消融實驗所需變量...\n",
      "\n",
      "✅ 已設置 RETRIEVER_RANKER = HybridSearchRanker.LINEAR\n",
      "✅ 已設置 RETRIEVER_ALPHA = 0.60\n",
      "⚠️ GRAPH_RETRIEVER 未定義，正在創建...\n",
      "✅ 已創建 GRAPH_RETRIEVER (type: hybrid)\n",
      "✅ GRAPH_LLM 已存在\n",
      "⚠️ GRAPH_RAG 未定義，正在創建...\n",
      "✅ 已創建 GRAPH_RAG\n",
      "✅ run_qa_evaluation 函數已定義\n",
      "\n",
      "✅ 所有必要變量已就緒，可以執行消融實驗！\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔧 初始化消融實驗所需的全局變量\n",
    "# ============================================================\n",
    "print(\"🔧 檢查並初始化消融實驗所需變量...\\n\")\n",
    "\n",
    "# Check and initialize missing variables\n",
    "missing_vars = []\n",
    "\n",
    "if 'RETRIEVER_RANKER' not in dir():\n",
    "    RETRIEVER_RANKER = HybridSearchRanker.LINEAR\n",
    "    print(\"✅ 已設置 RETRIEVER_RANKER = HybridSearchRanker.LINEAR\")\n",
    "else:\n",
    "    print(f\"✅ RETRIEVER_RANKER 已存在: {RETRIEVER_RANKER}\")\n",
    "\n",
    "if 'RETRIEVER_ALPHA' not in dir():\n",
    "    RETRIEVER_ALPHA = 0.60\n",
    "    print(\"✅ 已設置 RETRIEVER_ALPHA = 0.60\")\n",
    "else:\n",
    "    print(f\"✅ RETRIEVER_ALPHA 已存在: {RETRIEVER_ALPHA}\")\n",
    "\n",
    "if 'GRAPH_RETRIEVER' not in dir():\n",
    "    print(\"⚠️ GRAPH_RETRIEVER 未定義，正在創建...\")\n",
    "    try:\n",
    "        if FULLTEXT_READY:\n",
    "            GRAPH_RETRIEVER = HybridRetriever(\n",
    "                driver=GRAPH_DRIVER,\n",
    "                vector_index_name=VECTOR_INDEX_NAME,\n",
    "                fulltext_index_name=FULLTEXT_INDEX_NAME,\n",
    "                embedder=GRAPH_EMBEDDER,\n",
    "                return_properties=[\"id\", \"text\", \"source\", \"dataset\"],\n",
    "            )\n",
    "            RETRIEVER_KIND = \"hybrid\"\n",
    "        else:\n",
    "            GRAPH_RETRIEVER = VectorRetriever(\n",
    "                driver=GRAPH_DRIVER,\n",
    "                index_name=VECTOR_INDEX_NAME,\n",
    "                embedder=GRAPH_EMBEDDER,\n",
    "                return_properties=[\"id\", \"text\", \"source\", \"dataset\"],\n",
    "            )\n",
    "            RETRIEVER_KIND = \"vector\"\n",
    "        print(f\"✅ 已創建 GRAPH_RETRIEVER (type: {RETRIEVER_KIND})\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 創建 GRAPH_RETRIEVER 失敗: {e}\")\n",
    "else:\n",
    "    print(f\"✅ GRAPH_RETRIEVER 已存在\")\n",
    "\n",
    "if 'GRAPH_LLM' not in dir():\n",
    "    print(\"⚠️ GRAPH_LLM 未定義，正在創建...\")\n",
    "    GRAPH_LLM = OllamaLLM(\n",
    "        model_name=LLM_MODEL,\n",
    "        model_params={\"options\": {\"temperature\": LLM_TEMPERATURE, \"top_p\": 0.9}},\n",
    "    )\n",
    "    print(\"✅ 已創建 GRAPH_LLM\")\n",
    "else:\n",
    "    print(\"✅ GRAPH_LLM 已存在\")\n",
    "\n",
    "if 'GRAPH_RAG' not in dir():\n",
    "    print(\"⚠️ GRAPH_RAG 未定義，正在創建...\")\n",
    "    GRAPH_RAG = GraphRAG(retriever=GRAPH_RETRIEVER, llm=GRAPH_LLM)\n",
    "    print(\"✅ 已創建 GRAPH_RAG\")\n",
    "else:\n",
    "    print(\"✅ GRAPH_RAG 已存在\")\n",
    "\n",
    "# Verify run_qa_evaluation function exists\n",
    "if 'run_qa_evaluation' not in dir():\n",
    "    print(\"❌ run_qa_evaluation 函數未定義！\")\n",
    "    print(\"   需要先執行包含此函數定義的 cell\")\n",
    "    missing_vars.append('run_qa_evaluation')\n",
    "else:\n",
    "    print(\"✅ run_qa_evaluation 函數已定義\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n⚠️ 警告：以下變量/函數缺失: {missing_vars}\")\n",
    "    print(\"   建議先執行相關定義 cell\")\n",
    "else:\n",
    "    print(\"\\n✅ 所有必要變量已就緒，可以執行消融實驗！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9ec2d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 開始執行修正版全量消融實驗...\n",
      "\n",
      "📋 實驗配置：\n",
      "  - Chunk sizes: [2048, 4096, 8192]\n",
      "  - Overlaps: [128, 256, 512]\n",
      "  - 總組合數: 9\n",
      "  - 問題數: 100\n",
      "  - 使用修正版構建器: build_graph_with_params_fixed\n",
      "  - 每次實驗前完全重置DB\n",
      "\n",
      "🚀 開始消螏實驗（修正版，使用 cell 29 相同的 chunk_text）…\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=2048, overlap=128\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs2048_ov128, chunk_size=2048, overlap=128\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 362 chunks\n",
      "  ↳ Upserted 362, skipped 0\n",
      "  ↳ 三元組寫入: 356 chunks（跳過 6）\n",
      "    ⚠️ 無三元組: 6 chunks\n",
      "  📊 結果: chunks=362, entities=5065, relations=4207, density=0.831\n",
      "\n",
      "============================================================\n",
      "🎯 執行問答評估: chunk_size=2048, overlap=128\n",
      "============================================================\n",
      "\n",
      "📝 開始評估 15 個問題...\n",
      "\n",
      "問題 1/15: What are the main dairy goat breeds registered by ...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.719, Effective=False, Latency=0.00s\n",
      "問題 2/15: What are the advantages of the Alpine breed?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.556, Effective=False, Latency=0.00s\n",
      "問題 3/15: What are the coat color characteristics of the Nub...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.566, Effective=False, Latency=0.00s\n",
      "問題 4/15: According to challenge feeding, how much concentra...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.522, Effective=False, Latency=0.00s\n",
      "問題 5/15: How does the adult goat's stomach assist in digest...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.569, Effective=False, Latency=0.00s\n",
      "問題 6/15: What should be noted during pre-milking teat dippi...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "問題 7/15: Under what conditions does tuberculosis commonly o...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.572, Effective=False, Latency=0.00s\n",
      "問題 8/15: What could be the consequences of secondary bacter...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 9/15: In which regions is CAE infection less common in g...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.559, Effective=False, Latency=0.00s\n",
      "問題 10/15: How do protein requirements differ between early a...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.538, Effective=False, Latency=0.00s\n",
      "問題 11/15: What vision problems occur in goats deficient in v...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 12/15: Why should trace minerals and vitamins be provided...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.544, Effective=False, Latency=0.00s\n",
      "問題 13/15: What is the importance of shade shelters in exerci...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.524, Effective=False, Latency=0.00s\n",
      "問題 14/15: What type of animal is a meat goat?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.554, Effective=False, Latency=0.00s\n",
      "問題 15/15: What determines the efficiency of nutrient utiliza...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "\n",
      "============================================================\n",
      "📊 評估結果彙總:\n",
      "  - chunk_size: 2048\n",
      "  - overlap: 128\n",
      "  - total_questions: 15\n",
      "  - avg_f1_score: 0.0\n",
      "  - avg_exact_match: 0.0\n",
      "  - avg_cosine_similarity: 0.5615\n",
      "  - effective_rate: 0.0\n",
      "  - avg_latency_seconds: 0.0\n",
      "  - total_time_seconds: 0.0\n",
      "============================================================\n",
      "  💾 中間結果: ablation_fixed_intermediate_20260101_090309.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=2048, overlap=256\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs2048_ov256, chunk_size=2048, overlap=256\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 388 chunks\n",
      "  ↳ Upserted 388, skipped 0\n",
      "  ↳ 三元組寫入: 379 chunks（跳過 9）\n",
      "    ⚠️ 無三元組: 9 chunks\n",
      "  📊 結果: chunks=388, entities=5385, relations=4366, density=0.811\n",
      "\n",
      "============================================================\n",
      "🎯 執行問答評估: chunk_size=2048, overlap=256\n",
      "============================================================\n",
      "\n",
      "📝 開始評估 15 個問題...\n",
      "\n",
      "問題 1/15: What are the main dairy goat breeds registered by ...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.719, Effective=False, Latency=0.00s\n",
      "問題 2/15: What are the advantages of the Alpine breed?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.556, Effective=False, Latency=0.00s\n",
      "問題 3/15: What are the coat color characteristics of the Nub...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.566, Effective=False, Latency=0.00s\n",
      "問題 4/15: According to challenge feeding, how much concentra...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.522, Effective=False, Latency=0.00s\n",
      "問題 5/15: How does the adult goat's stomach assist in digest...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.569, Effective=False, Latency=0.00s\n",
      "問題 6/15: What should be noted during pre-milking teat dippi...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "問題 7/15: Under what conditions does tuberculosis commonly o...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.572, Effective=False, Latency=0.00s\n",
      "問題 8/15: What could be the consequences of secondary bacter...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 9/15: In which regions is CAE infection less common in g...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.559, Effective=False, Latency=0.00s\n",
      "問題 10/15: How do protein requirements differ between early a...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.538, Effective=False, Latency=0.00s\n",
      "問題 11/15: What vision problems occur in goats deficient in v...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 12/15: Why should trace minerals and vitamins be provided...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.544, Effective=False, Latency=0.00s\n",
      "問題 13/15: What is the importance of shade shelters in exerci...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.524, Effective=False, Latency=0.00s\n",
      "問題 14/15: What type of animal is a meat goat?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.554, Effective=False, Latency=0.00s\n",
      "問題 15/15: What determines the efficiency of nutrient utiliza...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "\n",
      "============================================================\n",
      "📊 評估結果彙總:\n",
      "  - chunk_size: 2048\n",
      "  - overlap: 256\n",
      "  - total_questions: 15\n",
      "  - avg_f1_score: 0.0\n",
      "  - avg_exact_match: 0.0\n",
      "  - avg_cosine_similarity: 0.5615\n",
      "  - effective_rate: 0.0\n",
      "  - avg_latency_seconds: 0.0\n",
      "  - total_time_seconds: 0.0\n",
      "============================================================\n",
      "  💾 中間結果: ablation_fixed_intermediate_20260101_090309.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=2048, overlap=512\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs2048_ov512, chunk_size=2048, overlap=512\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 452 chunks\n",
      "  ↳ Upserted 452, skipped 0\n",
      "  ↳ 三元組寫入: 439 chunks（跳過 13）\n",
      "    ⚠️ 無三元組: 13 chunks\n",
      "  📊 結果: chunks=452, entities=6324, relations=5114, density=0.809\n",
      "\n",
      "============================================================\n",
      "🎯 執行問答評估: chunk_size=2048, overlap=512\n",
      "============================================================\n",
      "\n",
      "📝 開始評估 15 個問題...\n",
      "\n",
      "問題 1/15: What are the main dairy goat breeds registered by ...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.719, Effective=False, Latency=0.00s\n",
      "問題 2/15: What are the advantages of the Alpine breed?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.556, Effective=False, Latency=0.00s\n",
      "問題 3/15: What are the coat color characteristics of the Nub...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.566, Effective=False, Latency=0.00s\n",
      "問題 4/15: According to challenge feeding, how much concentra...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.522, Effective=False, Latency=0.00s\n",
      "問題 5/15: How does the adult goat's stomach assist in digest...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.569, Effective=False, Latency=0.00s\n",
      "問題 6/15: What should be noted during pre-milking teat dippi...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "問題 7/15: Under what conditions does tuberculosis commonly o...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.572, Effective=False, Latency=0.00s\n",
      "問題 8/15: What could be the consequences of secondary bacter...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 9/15: In which regions is CAE infection less common in g...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.559, Effective=False, Latency=0.00s\n",
      "問題 10/15: How do protein requirements differ between early a...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.538, Effective=False, Latency=0.00s\n",
      "問題 11/15: What vision problems occur in goats deficient in v...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 12/15: Why should trace minerals and vitamins be provided...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.544, Effective=False, Latency=0.00s\n",
      "問題 13/15: What is the importance of shade shelters in exerci...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.524, Effective=False, Latency=0.00s\n",
      "問題 14/15: What type of animal is a meat goat?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.554, Effective=False, Latency=0.00s\n",
      "問題 15/15: What determines the efficiency of nutrient utiliza...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "\n",
      "============================================================\n",
      "📊 評估結果彙總:\n",
      "  - chunk_size: 2048\n",
      "  - overlap: 512\n",
      "  - total_questions: 15\n",
      "  - avg_f1_score: 0.0\n",
      "  - avg_exact_match: 0.0\n",
      "  - avg_cosine_similarity: 0.5615\n",
      "  - effective_rate: 0.0\n",
      "  - avg_latency_seconds: 0.0\n",
      "  - total_time_seconds: 0.0\n",
      "============================================================\n",
      "  💾 中間結果: ablation_fixed_intermediate_20260101_090309.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=4096, overlap=128\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs4096_ov128, chunk_size=4096, overlap=128\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 175 chunks\n",
      "  ↳ Upserted 175, skipped 0\n",
      "  ↳ 三元組寫入: 175 chunks（跳過 0）\n",
      "  📊 結果: chunks=175, entities=3559, relations=2794, density=0.785\n",
      "\n",
      "============================================================\n",
      "🎯 執行問答評估: chunk_size=4096, overlap=128\n",
      "============================================================\n",
      "\n",
      "📝 開始評估 15 個問題...\n",
      "\n",
      "問題 1/15: What are the main dairy goat breeds registered by ...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.719, Effective=False, Latency=0.00s\n",
      "問題 2/15: What are the advantages of the Alpine breed?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.556, Effective=False, Latency=0.00s\n",
      "問題 3/15: What are the coat color characteristics of the Nub...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.566, Effective=False, Latency=0.00s\n",
      "問題 4/15: According to challenge feeding, how much concentra...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.522, Effective=False, Latency=0.00s\n",
      "問題 5/15: How does the adult goat's stomach assist in digest...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.569, Effective=False, Latency=0.00s\n",
      "問題 6/15: What should be noted during pre-milking teat dippi...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "問題 7/15: Under what conditions does tuberculosis commonly o...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.572, Effective=False, Latency=0.00s\n",
      "問題 8/15: What could be the consequences of secondary bacter...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 9/15: In which regions is CAE infection less common in g...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.559, Effective=False, Latency=0.00s\n",
      "問題 10/15: How do protein requirements differ between early a...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.538, Effective=False, Latency=0.00s\n",
      "問題 11/15: What vision problems occur in goats deficient in v...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 12/15: Why should trace minerals and vitamins be provided...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.544, Effective=False, Latency=0.00s\n",
      "問題 13/15: What is the importance of shade shelters in exerci...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.524, Effective=False, Latency=0.00s\n",
      "問題 14/15: What type of animal is a meat goat?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.554, Effective=False, Latency=0.00s\n",
      "問題 15/15: What determines the efficiency of nutrient utiliza...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "\n",
      "============================================================\n",
      "📊 評估結果彙總:\n",
      "  - chunk_size: 4096\n",
      "  - overlap: 128\n",
      "  - total_questions: 15\n",
      "  - avg_f1_score: 0.0\n",
      "  - avg_exact_match: 0.0\n",
      "  - avg_cosine_similarity: 0.5615\n",
      "  - effective_rate: 0.0\n",
      "  - avg_latency_seconds: 0.0\n",
      "  - total_time_seconds: 0.0\n",
      "============================================================\n",
      "  💾 中間結果: ablation_fixed_intermediate_20260101_090309.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=4096, overlap=256\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs4096_ov256, chunk_size=4096, overlap=256\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 181 chunks\n",
      "  ↳ Upserted 181, skipped 0\n",
      "  ↳ 三元組寫入: 181 chunks（跳過 0）\n",
      "  📊 結果: chunks=181, entities=3358, relations=2662, density=0.793\n",
      "\n",
      "============================================================\n",
      "🎯 執行問答評估: chunk_size=4096, overlap=256\n",
      "============================================================\n",
      "\n",
      "📝 開始評估 15 個問題...\n",
      "\n",
      "問題 1/15: What are the main dairy goat breeds registered by ...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.719, Effective=False, Latency=0.00s\n",
      "問題 2/15: What are the advantages of the Alpine breed?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.556, Effective=False, Latency=0.00s\n",
      "問題 3/15: What are the coat color characteristics of the Nub...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.566, Effective=False, Latency=0.00s\n",
      "問題 4/15: According to challenge feeding, how much concentra...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.522, Effective=False, Latency=0.00s\n",
      "問題 5/15: How does the adult goat's stomach assist in digest...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.569, Effective=False, Latency=0.00s\n",
      "問題 6/15: What should be noted during pre-milking teat dippi...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "問題 7/15: Under what conditions does tuberculosis commonly o...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.572, Effective=False, Latency=0.00s\n",
      "問題 8/15: What could be the consequences of secondary bacter...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 9/15: In which regions is CAE infection less common in g...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.559, Effective=False, Latency=0.00s\n",
      "問題 10/15: How do protein requirements differ between early a...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.538, Effective=False, Latency=0.00s\n",
      "問題 11/15: What vision problems occur in goats deficient in v...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 12/15: Why should trace minerals and vitamins be provided...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.544, Effective=False, Latency=0.00s\n",
      "問題 13/15: What is the importance of shade shelters in exerci...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.524, Effective=False, Latency=0.00s\n",
      "問題 14/15: What type of animal is a meat goat?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.554, Effective=False, Latency=0.00s\n",
      "問題 15/15: What determines the efficiency of nutrient utiliza...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "\n",
      "============================================================\n",
      "📊 評估結果彙總:\n",
      "  - chunk_size: 4096\n",
      "  - overlap: 256\n",
      "  - total_questions: 15\n",
      "  - avg_f1_score: 0.0\n",
      "  - avg_exact_match: 0.0\n",
      "  - avg_cosine_similarity: 0.5615\n",
      "  - effective_rate: 0.0\n",
      "  - avg_latency_seconds: 0.0\n",
      "  - total_time_seconds: 0.0\n",
      "============================================================\n",
      "  💾 中間結果: ablation_fixed_intermediate_20260101_090309.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=4096, overlap=512\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs4096_ov512, chunk_size=4096, overlap=512\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 194 chunks\n",
      "  ↳ Upserted 194, skipped 0\n",
      "  ↳ 三元組寫入: 194 chunks（跳過 0）\n",
      "  📊 結果: chunks=194, entities=3755, relations=3030, density=0.807\n",
      "\n",
      "============================================================\n",
      "🎯 執行問答評估: chunk_size=4096, overlap=512\n",
      "============================================================\n",
      "\n",
      "📝 開始評估 15 個問題...\n",
      "\n",
      "問題 1/15: What are the main dairy goat breeds registered by ...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.719, Effective=False, Latency=0.00s\n",
      "問題 2/15: What are the advantages of the Alpine breed?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.556, Effective=False, Latency=0.00s\n",
      "問題 3/15: What are the coat color characteristics of the Nub...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.566, Effective=False, Latency=0.00s\n",
      "問題 4/15: According to challenge feeding, how much concentra...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.522, Effective=False, Latency=0.00s\n",
      "問題 5/15: How does the adult goat's stomach assist in digest...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.569, Effective=False, Latency=0.00s\n",
      "問題 6/15: What should be noted during pre-milking teat dippi...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "問題 7/15: Under what conditions does tuberculosis commonly o...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.572, Effective=False, Latency=0.00s\n",
      "問題 8/15: What could be the consequences of secondary bacter...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 9/15: In which regions is CAE infection less common in g...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.559, Effective=False, Latency=0.00s\n",
      "問題 10/15: How do protein requirements differ between early a...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.538, Effective=False, Latency=0.00s\n",
      "問題 11/15: What vision problems occur in goats deficient in v...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.565, Effective=False, Latency=0.00s\n",
      "問題 12/15: Why should trace minerals and vitamins be provided...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.544, Effective=False, Latency=0.00s\n",
      "問題 13/15: What is the importance of shade shelters in exerci...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.524, Effective=False, Latency=0.00s\n",
      "問題 14/15: What type of animal is a meat goat?...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.554, Effective=False, Latency=0.00s\n",
      "問題 15/15: What determines the efficiency of nutrient utiliza...\n",
      "  ❌ 錯誤: 'dict' object has no attribute 'message'\n",
      "  ✅ F1=0.000, EM=0, Sim=0.535, Effective=False, Latency=0.00s\n",
      "\n",
      "============================================================\n",
      "📊 評估結果彙總:\n",
      "  - chunk_size: 4096\n",
      "  - overlap: 512\n",
      "  - total_questions: 15\n",
      "  - avg_f1_score: 0.0\n",
      "  - avg_exact_match: 0.0\n",
      "  - avg_cosine_similarity: 0.5615\n",
      "  - effective_rate: 0.0\n",
      "  - avg_latency_seconds: 0.0\n",
      "  - total_time_seconds: 0.0\n",
      "============================================================\n",
      "  💾 中間結果: ablation_fixed_intermediate_20260101_090309.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=8192, overlap=128\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs8192_ov128, chunk_size=8192, overlap=128\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 87 chunks\n",
      "  ❌ 構建失敗，跳過該組合: the input length exceeds the context length\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=8192, overlap=256\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs8192_ov256, chunk_size=8192, overlap=256\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 88 chunks\n",
      "  ❌ 構建失敗，跳過該組合: the input length exceeds the context length\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "🧪 實驗組合: chunk_size=8192, overlap=512\n",
      "  ↳ 清空資料庫完成\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs8192_ov512, chunk_size=8192, overlap=512\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 91 chunks\n",
      "  ❌ 構建失敗，跳過該組合: the input length exceeds the context length\n",
      "\n",
      "✅ 實驗完成。最終結果: ablation_results\\ablation_fixed_final_20260101_090309.csv\n",
      "\n",
      "================================================================================\n",
      "🎊 修正版全量消融實驗完成！\n",
      "================================================================================\n",
      "⏱️ 總耗時: 981.9 分鐘\n",
      "📊 結果維度: (6, 19)\n",
      "\n",
      "📈 結果預覽：\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>overlap</th>\n",
       "      <th>total_chunks</th>\n",
       "      <th>total_entities</th>\n",
       "      <th>total_relations</th>\n",
       "      <th>relation_density</th>\n",
       "      <th>weak_entity_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048</td>\n",
       "      <td>128</td>\n",
       "      <td>362</td>\n",
       "      <td>5065</td>\n",
       "      <td>4207</td>\n",
       "      <td>0.830602</td>\n",
       "      <td>81.895360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2048</td>\n",
       "      <td>256</td>\n",
       "      <td>388</td>\n",
       "      <td>5385</td>\n",
       "      <td>4366</td>\n",
       "      <td>0.810771</td>\n",
       "      <td>81.262767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2048</td>\n",
       "      <td>512</td>\n",
       "      <td>452</td>\n",
       "      <td>6324</td>\n",
       "      <td>5114</td>\n",
       "      <td>0.808665</td>\n",
       "      <td>82.242252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4096</td>\n",
       "      <td>128</td>\n",
       "      <td>175</td>\n",
       "      <td>3559</td>\n",
       "      <td>2794</td>\n",
       "      <td>0.785052</td>\n",
       "      <td>81.905030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4096</td>\n",
       "      <td>256</td>\n",
       "      <td>181</td>\n",
       "      <td>3358</td>\n",
       "      <td>2662</td>\n",
       "      <td>0.792734</td>\n",
       "      <td>81.923764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4096</td>\n",
       "      <td>512</td>\n",
       "      <td>194</td>\n",
       "      <td>3755</td>\n",
       "      <td>3030</td>\n",
       "      <td>0.806924</td>\n",
       "      <td>82.103862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_size  overlap  total_chunks  total_entities  total_relations  \\\n",
       "0        2048      128           362            5065             4207   \n",
       "1        2048      256           388            5385             4366   \n",
       "2        2048      512           452            6324             5114   \n",
       "3        4096      128           175            3559             2794   \n",
       "4        4096      256           181            3358             2662   \n",
       "5        4096      512           194            3755             3030   \n",
       "\n",
       "   relation_density  weak_entity_percent  \n",
       "0          0.830602            81.895360  \n",
       "1          0.810771            81.262767  \n",
       "2          0.808665            82.242252  \n",
       "3          0.785052            81.905030  \n",
       "4          0.792734            81.923764  \n",
       "5          0.806924            82.103862  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 快速分析：\n",
      "  最高關係密度: 0.831\n",
      "    → chunk_size=2048, overlap=128\n",
      "  最高 F1 分數: 0.000\n",
      "    → chunk_size=2048, overlap=128\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🚀 執行修正版全量消融實驗\n",
    "# ============================================================\n",
    "print(\"🚀 開始執行修正版全量消融實驗...\\n\")\n",
    "print(\"📋 實驗配置：\")\n",
    "print(f\"  - Chunk sizes: {ABLATION_CONFIG['chunk_sizes']}\")\n",
    "print(f\"  - Overlaps: {ABLATION_CONFIG['overlaps']}\")\n",
    "print(f\"  - 總組合數: {len(ABLATION_CONFIG['chunk_sizes']) * len(ABLATION_CONFIG['overlaps'])}\")\n",
    "print(f\"  - 問題數: {ABLATION_CONFIG['fixed_params']['max_questions']}\")\n",
    "print(f\"  - 使用修正版構建器: build_graph_with_params_fixed\")\n",
    "print(f\"  - 每次實驗前完全重置DB\\n\")\n",
    "\n",
    "import time\n",
    "ablation_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    ABLATION_RESULTS_FIXED = run_ablation_experiment_fixed(\n",
    "        chunk_sizes=ABLATION_CONFIG[\"chunk_sizes\"],\n",
    "        overlaps=ABLATION_CONFIG[\"overlaps\"],\n",
    "        questions_df=QUESTIONS_DF_ABLATION,\n",
    "        llm=ABLATION_LLM,\n",
    "        embedder=ABLATION_EMBEDDER,\n",
    "        max_questions=ABLATION_CONFIG[\"fixed_params\"][\"max_questions\"]\n",
    "    )\n",
    "    \n",
    "    ablation_elapsed = time.time() - ablation_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎊 修正版全量消融實驗完成！\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"⏱️ 總耗時: {ablation_elapsed/60:.1f} 分鐘\")\n",
    "    print(f\"📊 結果維度: {ABLATION_RESULTS_FIXED.shape}\")\n",
    "    print(\"\\n📈 結果預覽：\")\n",
    "    display(ABLATION_RESULTS_FIXED[['chunk_size', 'overlap', 'total_chunks', 'total_entities', \n",
    "                                     'total_relations', 'relation_density', 'weak_entity_percent']].head(10))\n",
    "    \n",
    "    # Quick analysis\n",
    "    print(\"\\n🔍 快速分析：\")\n",
    "    best_density_idx = ABLATION_RESULTS_FIXED['relation_density'].idxmax()\n",
    "    best_density_row = ABLATION_RESULTS_FIXED.loc[best_density_idx]\n",
    "    print(f\"  最高關係密度: {best_density_row['relation_density']:.3f}\")\n",
    "    print(f\"    → chunk_size={int(best_density_row['chunk_size'])}, overlap={int(best_density_row['overlap'])}\")\n",
    "    \n",
    "    if 'avg_f1_score' in ABLATION_RESULTS_FIXED.columns:\n",
    "        best_f1_idx = ABLATION_RESULTS_FIXED['avg_f1_score'].idxmax()\n",
    "        best_f1_row = ABLATION_RESULTS_FIXED.loc[best_f1_idx]\n",
    "        print(f\"  最高 F1 分數: {best_f1_row['avg_f1_score']:.3f}\")\n",
    "        print(f\"    → chunk_size={int(best_f1_row['chunk_size'])}, overlap={int(best_f1_row['overlap'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 消融實驗執行失敗: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12baa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 增強版消融執行器已載入\n",
      "💡 使用方式：\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📊 增強版消融實驗 - 保存完整結果（含回答、similarity、effective_rate）\n",
    "# ============================================================\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "def run_ablation_experiment_enhanced(chunk_sizes, overlaps, questions_df, llm, embedder, max_questions=20):\n",
    "    \"\"\"\n",
    "    增強版消融實驗執行器，保存：\n",
    "    1. 每個組合的彙總指標（含 avg_cosine_similarity, effective_rate）\n",
    "    2. 每個問題的詳細回答（按 chunk_size/overlap 分類，含時間戳）\n",
    "    \"\"\"\n",
    "    print(\"🚀 開始增強版消融實驗（完整結果保存）...\\n\")\n",
    "    \n",
    "    results_summary = []  # 彙總結果\n",
    "    results_detailed = []  # 詳細問答記錄\n",
    "    \n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    ABLATION_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    summary_path = ABLATION_RESULTS_DIR / f\"ablation_summary_{ts}.csv\"\n",
    "    detailed_path = ABLATION_RESULTS_DIR / f\"ablation_detailed_answers_{ts}.csv\"\n",
    "    \n",
    "    combo_count = 0\n",
    "    total_combos = len(chunk_sizes) * len(overlaps)\n",
    "    \n",
    "    for cs in chunk_sizes:\n",
    "        for ov in overlaps:\n",
    "            combo_count += 1\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"🧪 組合 [{combo_count}/{total_combos}]: chunk_size={cs}, overlap={ov}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # 1) Reset DB\n",
    "            try:\n",
    "                with GRAPH_DRIVER.session() as s:\n",
    "                    s.run(\"MATCH (n) DETACH DELETE n\")\n",
    "                print(\"  ✅ 資料庫已清空\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ 清空失敗: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # 2) Build KG\n",
    "            try:\n",
    "                metrics = build_graph_with_params_fixed(cs, ov, KNOWLEDGE_TEXT, GRAPH_DRIVER, llm, embedder)\n",
    "                print(f\"  ✅ 圖譜構建完成: {metrics['total_entities']} entities, {metrics['total_relations']} relations\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ 構建失敗: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # 3) Run QA with detailed tracking\n",
    "            print(f\"\\n  📝 開始問答評估 ({max_questions} 題)...\")\n",
    "            subset = questions_df.head(max_questions).copy()\n",
    "            \n",
    "            qa_start_time = datetime.now()\n",
    "            qa_results = []\n",
    "            \n",
    "            for idx, row in subset.iterrows():\n",
    "                question = row['question']\n",
    "                reference = row.get('reference_answer', '')\n",
    "                \n",
    "                try:\n",
    "                    # Run single QA\n",
    "                    retriever_config = {\"top_k\": TOP_K}\n",
    "                    if RETRIEVER_KIND == \"hybrid\":\n",
    "                        retriever_config.update({\"ranker\": RETRIEVER_RANKER, \"alpha\": RETRIEVER_ALPHA})\n",
    "                    \n",
    "                    # ✅ 強制簡答指令（防廢話）\n",
    "                    system_instruction = (\n",
    "                        \"Answer requirements:\\n\"\n",
    "                        \"1. Answer in English.\\n\"\n",
    "                        \"2. Use a simple list or a single Subject-Verb-Object (SVO) sentence.\\n\"\n",
    "                        \"3. Do NOT use introductory phrases like 'Based on the text'.\\n\"\n",
    "                        \"4. Do NOT provide explanations or context. Just the answer.\\n\"\n",
    "                    )\n",
    "                    query_text = f\"{question}\\n\\n{system_instruction}\"\n",
    "                    response = GRAPH_RAG.search(query_text=query_text, retriever_config=retriever_config, return_context=True)\n",
    "                    \n",
    "                    answer = (getattr(response, \"answer\", None) or \"\").strip()\n",
    "                    contexts = extract_contexts(getattr(response, \"retriever_result\", None), TOP_K)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    from sklearn.metrics.pairwise import cosine_similarity\n",
    "                    import numpy as np\n",
    "                    \n",
    "                    # F1 & EM\n",
    "                    pred_tokens = set(answer.lower().split())\n",
    "                    ref_tokens = set(reference.lower().split())\n",
    "                    common = pred_tokens & ref_tokens\n",
    "                    \n",
    "                    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "                        f1 = 0.0\n",
    "                        em = 0\n",
    "                    else:\n",
    "                        precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
    "                        recall = len(common) / len(ref_tokens) if ref_tokens else 0\n",
    "                        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "                        em = 1 if answer.lower().strip() == reference.lower().strip() else 0\n",
    "                    \n",
    "                    # Cosine similarity (using embeddings)\n",
    "                    try:\n",
    "                        ans_emb = embedder.embed_query(answer)\n",
    "                        ref_emb = embedder.embed_query(reference)\n",
    "                        cos_sim = cosine_similarity([ans_emb], [ref_emb])[0][0]\n",
    "                    except:\n",
    "                        cos_sim = 0.0\n",
    "                    \n",
    "                    # Effective (有答案且F1>0)\n",
    "                    is_effective = 1 if (len(answer) > 0 and f1 > 0) else 0\n",
    "                    \n",
    "                    qa_results.append({\n",
    "                        'f1': f1,\n",
    "                        'em': em,\n",
    "                        'cosine_similarity': cos_sim,\n",
    "                        'effective': is_effective,\n",
    "                    })\n",
    "                    \n",
    "                    # Save detailed answer\n",
    "                    results_detailed.append({\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'chunk_size': cs,\n",
    "                        'overlap': ov,\n",
    "                        'question_idx': idx,\n",
    "                        'question': question,\n",
    "                        'reference_answer': reference,\n",
    "                        'predicted_answer': answer,\n",
    "                        'f1_score': f1,\n",
    "                        'exact_match': em,\n",
    "                        'cosine_similarity': cos_sim,\n",
    "                        'is_effective': is_effective,\n",
    "                        'num_contexts': len(contexts),\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ⚠️ Q{idx} 失敗: {e}\")\n",
    "                    qa_results.append({'f1': 0, 'em': 0, 'cosine_similarity': 0, 'effective': 0})\n",
    "            \n",
    "            qa_end_time = datetime.now()\n",
    "            qa_duration = (qa_end_time - qa_start_time).total_seconds()\n",
    "            \n",
    "            # 4) Aggregate QA metrics\n",
    "            if qa_results:\n",
    "                avg_f1 = sum(r['f1'] for r in qa_results) / len(qa_results)\n",
    "                avg_em = sum(r['em'] for r in qa_results) / len(qa_results)\n",
    "                avg_cos_sim = sum(r['cosine_similarity'] for r in qa_results) / len(qa_results)\n",
    "                effective_rate = sum(r['effective'] for r in qa_results) / len(qa_results)\n",
    "            else:\n",
    "                avg_f1 = avg_em = avg_cos_sim = effective_rate = 0.0\n",
    "            \n",
    "            print(f\"  📊 QA結果: F1={avg_f1:.3f}, EM={avg_em:.3f}, CosSim={avg_cos_sim:.3f}, Effective={effective_rate:.1%}\")\n",
    "            \n",
    "            # 5) Combine metrics\n",
    "            summary_row = {\n",
    "                **metrics,\n",
    "                'avg_f1_score': avg_f1,\n",
    "                'avg_exact_match': avg_em,\n",
    "                'avg_cosine_similarity': avg_cos_sim,\n",
    "                'effective_rate': effective_rate,\n",
    "                'total_questions': len(qa_results),\n",
    "                'qa_duration_seconds': qa_duration,\n",
    "                'timestamp': qa_end_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "            results_summary.append(summary_row)\n",
    "            \n",
    "            # 6) Save intermediate results\n",
    "            try:\n",
    "                pd.DataFrame(results_summary).to_csv(summary_path, index=False, encoding='utf-8')\n",
    "                pd.DataFrame(results_detailed).to_csv(detailed_path, index=False, encoding='utf-8')\n",
    "                print(f\"  💾 中間結果已保存\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ 保存失敗: {e}\")\n",
    "    \n",
    "    # Final save\n",
    "    df_summary = pd.DataFrame(results_summary)\n",
    "    df_detailed = pd.DataFrame(results_detailed)\n",
    "    \n",
    "    df_summary.to_csv(summary_path, index=False, encoding='utf-8')\n",
    "    df_detailed.to_csv(detailed_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ 增強版消融實驗完成！\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"📊 彙總結果: {summary_path}\")\n",
    "    print(f\"📝 詳細回答: {detailed_path}\")\n",
    "    print(f\"   - 總組合數: {len(results_summary)}\")\n",
    "    print(f\"   - 詳細記錄: {len(results_detailed)} 條\")\n",
    "    \n",
    "    return df_summary, df_detailed\n",
    "\n",
    "print(\"✅ 增強版消融執行器已載入\")\n",
    "print(\"💡 使用方式：\")\n",
    "print(\"   ABLATION_SUMMARY, ABLATION_DETAILED = run_ablation_experiment_enhanced(\")\n",
    "print(\"       chunk_sizes=ABLATION_CONFIG['chunk_sizes'],\")\n",
    "print(\"       overlaps=ABLATION_CONFIG['overlaps'],\")\n",
    "print(\"       questions_df=QUESTIONS_DF_ABLATION,\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6cda8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSONL 版本消融執行器已載入\n",
      "🔧 關鍵修正：解決 CSV 換行符衝突問題\n",
      "\n",
      "💡 使用方式：\n",
      "   ABLATION_RESULTS_JSONL_SUMMARY, ABLATION_RESULTS_JSONL_DETAILED, summary_path, detailed_path = run_ablation_experiment_jsonl(\n",
      "       chunk_sizes=ABLATION_CONFIG['chunk_sizes'],\n",
      "       overlaps=ABLATION_CONFIG['overlaps'],\n",
      "       questions_df=QUESTIONS_DF_ABLATION,\n",
      "       llm=ABLATION_LLM,\n",
      "       embedder=ABLATION_EMBEDDER,\n",
      "       max_questions=ABLATION_CONFIG['fixed_params']['max_questions']\n",
      "   )\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔧 CRITICAL FIX: 改用 JSONL 格式儲存（解決 CSV 換行符衝突）\n",
    "# ============================================================\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_jsonl(data: List[Dict], filepath: Path):\n",
    "    \"\"\"\n",
    "    將資料儲存為 JSONL 格式（每行一個 JSON 物件）\n",
    "    完全避免 CSV 換行符衝突問題\n",
    "    \"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for record in data:\n",
    "            json.dump(record, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "def load_from_jsonl(filepath: Path) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    從 JSONL 檔案讀取資料\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def run_ablation_experiment_jsonl(chunk_sizes, overlaps, questions_df, llm, embedder, \n",
    "                                   max_questions=20, output_dir=None, output_prefix='ablation_jsonl'):\n",
    "    \"\"\"\n",
    "    ✅ JSONL 版本消融實驗執行器\n",
    "    \n",
    "    關鍵改進：\n",
    "    1. 使用 JSONL 格式儲存，完全避免 CSV 換行符問題\n",
    "    2. LLM 回答中的換行符被正確保留\n",
    "    3. 資料完整性得到保證，cosine similarity 等指標計算準確\n",
    "    \n",
    "    參數：\n",
    "        chunk_sizes: List[int] - 要測試的 chunk size 列表\n",
    "        overlaps: List[int] - 要測試的 overlap 列表\n",
    "        questions_df: DataFrame - 問題資料集\n",
    "        llm: CustomOllamaLLM - 語言模型\n",
    "        embedder: OllamaVectorEmbedder - 向量嵌入器\n",
    "        max_questions: int - 每個組合測試的問題數量\n",
    "        output_dir: Path - 輸出目錄（預設為 ABLATION_RESULTS_DIR）\n",
    "        output_prefix: str - 輸出檔案前綴\n",
    "    \"\"\"\n",
    "    print(\"🚀 開始 JSONL 版本消融實驗（資料完整性保證）\\n\")\n",
    "    print(\"✅ 關鍵改進：使用 JSONL 格式避免 CSV 換行符衝突\")\n",
    "    print(\"✅ 確保 LLM 回答完整性，保證指標計算準確性\\n\")\n",
    "    print(\"🔧 BUGFIX: 支援多種參考答案欄位名稱 (answer, reference_answer, expected_answer)\\n\")\n",
    "    \n",
    "    results_summary = []\n",
    "    results_detailed = []\n",
    "    \n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 使用指定的輸出目錄或預設目錄\n",
    "    if output_dir is None:\n",
    "        output_dir = ABLATION_RESULTS_DIR\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 使用 .jsonl 副檔名和自訂前綴\n",
    "    summary_path = output_dir / f\"{output_prefix}_summary_{ts}.jsonl\"\n",
    "    detailed_path = output_dir / f\"{output_prefix}_detailed_answers_{ts}.jsonl\"\n",
    "    \n",
    "    combo_count = 0\n",
    "    total_combos = len(chunk_sizes) * len(overlaps)\n",
    "    \n",
    "    for cs in chunk_sizes:\n",
    "        for ov in overlaps:\n",
    "            combo_count += 1\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"🧪 組合 [{combo_count}/{total_combos}]: chunk_size={cs}, overlap={ov}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # 1) Reset DB\n",
    "            try:\n",
    "                with GRAPH_DRIVER.session() as s:\n",
    "                    s.run(\"MATCH (n) DETACH DELETE n\")\n",
    "                print(\"  ✅ 資料庫已清空\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ 清空失敗: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # 2) Build KG\n",
    "            try:\n",
    "                metrics = build_graph_with_params_fixed(cs, ov, KNOWLEDGE_TEXT, GRAPH_DRIVER, llm, embedder)\n",
    "                print(f\"  ✅ 圖譜構建完成: {metrics['total_entities']} entities, {metrics['total_relations']} relations\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ 構建失敗: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # 3) Run QA with detailed tracking\n",
    "            print(f\"\\n  📝 開始問答評估 ({max_questions} 題)...\")\n",
    "            subset = questions_df.head(max_questions).copy()\n",
    "            \n",
    "            qa_start_time = datetime.now()\n",
    "            qa_results = []\n",
    "            \n",
    "            for idx, row in subset.iterrows():\n",
    "                question = row['question']\n",
    "                # 🔧 BUGFIX: 支援多種參考答案欄位名稱\n",
    "                reference = row.get('reference_answer', row.get('answer', row.get('expected_answer', '')))\n",
    "                \n",
    "                try:\n",
    "                    # Run single QA\n",
    "                    retriever_config = {\"top_k\": TOP_K}\n",
    "                    if RETRIEVER_KIND == \"hybrid\":\n",
    "                        retriever_config.update({\"ranker\": RETRIEVER_RANKER, \"alpha\": RETRIEVER_ALPHA})\n",
    "                    \n",
    "                    # ✅ 強制簡答指令（防廢話）\n",
    "                    system_instruction = (\n",
    "                        \"Answer requirements:\\n\"\n",
    "                        \"1. Answer in English.\\n\"\n",
    "                        \"2. Use a simple list or a single Subject-Verb-Object (SVO) sentence.\\n\"\n",
    "                        \"3. Do NOT use introductory phrases like 'Based on the text'.\\n\"\n",
    "                        \"4. Do NOT provide explanations or context. Just the answer.\\n\"\n",
    "                    )\n",
    "                    query_text = f\"{question}\\n\\n{system_instruction}\"\n",
    "                    response = GRAPH_RAG.search(query_text=query_text, retriever_config=retriever_config, return_context=True)\n",
    "                    \n",
    "                    answer = (getattr(response, \"answer\", None) or \"\").strip()\n",
    "                    contexts = extract_contexts(getattr(response, \"retriever_result\", None), TOP_K)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    from sklearn.metrics.pairwise import cosine_similarity\n",
    "                    import numpy as np\n",
    "                    \n",
    "                    # F1 & EM\n",
    "                    pred_tokens = set(answer.lower().split())\n",
    "                    ref_tokens = set(reference.lower().split())\n",
    "                    common = pred_tokens & ref_tokens\n",
    "                    \n",
    "                    if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n",
    "                        f1 = 0.0\n",
    "                        em = 0\n",
    "                    else:\n",
    "                        precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
    "                        recall = len(common) / len(ref_tokens) if ref_tokens else 0\n",
    "                        f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
    "                        em = 1 if answer.lower().strip() == reference.lower().strip() else 0\n",
    "                    \n",
    "                    # Cosine similarity\n",
    "                    try:\n",
    "                        ans_emb = embedder.embed_query(answer)\n",
    "                        ref_emb = embedder.embed_query(reference)\n",
    "                        cos_sim = float(cosine_similarity([ans_emb], [ref_emb])[0][0])\n",
    "                    except:\n",
    "                        cos_sim = 0.0\n",
    "                    \n",
    "                    # Effective\n",
    "                    is_effective = 1 if (len(answer) > 0 and f1 > 0) else 0\n",
    "                    \n",
    "                    qa_results.append({\n",
    "                        'f1': float(f1),\n",
    "                        'em': int(em),\n",
    "                        'cosine_similarity': float(cos_sim),\n",
    "                        'effective': int(is_effective),\n",
    "                    })\n",
    "                    \n",
    "                    # ✅ 關鍵：JSONL 格式可以完整保留 LLM 回答中的換行符\n",
    "                    results_detailed.append({\n",
    "                        'timestamp': datetime.now().isoformat(),\n",
    "                        'chunk_size': int(cs),\n",
    "                        'overlap': int(ov),\n",
    "                        'question_idx': int(idx),\n",
    "                        'question': str(question),\n",
    "                        'reference_answer': str(reference),\n",
    "                        'predicted_answer': str(answer),  # 換行符被完整保留\n",
    "                        'f1_score': float(f1),\n",
    "                        'exact_match': int(em),\n",
    "                        'cosine_similarity': float(cos_sim),\n",
    "                        'is_effective': int(is_effective),\n",
    "                        'num_contexts': len(contexts),\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ⚠️ Q{idx} 失敗: {e}\")\n",
    "                    qa_results.append({'f1': 0, 'em': 0, 'cosine_similarity': 0, 'effective': 0})\n",
    "            \n",
    "            qa_end_time = datetime.now()\n",
    "            qa_duration = (qa_end_time - qa_start_time).total_seconds()\n",
    "            \n",
    "            # 4) Aggregate QA metrics\n",
    "            if qa_results:\n",
    "                avg_f1 = sum(r['f1'] for r in qa_results) / len(qa_results)\n",
    "                avg_em = sum(r['em'] for r in qa_results) / len(qa_results)\n",
    "                avg_cos_sim = sum(r['cosine_similarity'] for r in qa_results) / len(qa_results)\n",
    "                effective_rate = sum(r['effective'] for r in qa_results) / len(qa_results)\n",
    "            else:\n",
    "                avg_f1 = avg_em = avg_cos_sim = effective_rate = 0.0\n",
    "            \n",
    "            print(f\"  📊 QA結果: F1={avg_f1:.3f}, EM={avg_em:.3f}, CosSim={avg_cos_sim:.3f}, Effective={effective_rate:.1%}\")\n",
    "            \n",
    "            # 5) Combine metrics\n",
    "            summary_row = {\n",
    "                **{k: (int(v) if isinstance(v, (int, float)) and k in ['chunk_size', 'overlap', 'total_chunks', 'total_entities', 'total_relations', 'weak_entities', 'orphan_chunks'] else float(v) if isinstance(v, (int, float)) else v) for k, v in metrics.items()},\n",
    "                'avg_f1_score': float(avg_f1),\n",
    "                'avg_exact_match': float(avg_em),\n",
    "                'avg_cosine_similarity': float(avg_cos_sim),\n",
    "                'effective_rate': float(effective_rate),\n",
    "                'total_questions': len(qa_results),\n",
    "                'qa_duration_seconds': float(qa_duration),\n",
    "                'timestamp': qa_end_time.isoformat(),\n",
    "            }\n",
    "            results_summary.append(summary_row)\n",
    "            \n",
    "            # 6) ✅ 使用 JSONL 格式儲存中間結果\n",
    "            try:\n",
    "                save_to_jsonl(results_summary, summary_path)\n",
    "                save_to_jsonl(results_detailed, detailed_path)\n",
    "                print(f\"  💾 中間結果已保存（JSONL 格式）\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ 保存失敗: {e}\")\n",
    "    \n",
    "    # Final save (already done in loop)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ JSONL 版本消融實驗完成！\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"📊 彙總結果: {summary_path}\")\n",
    "    print(f\"📝 詳細回答: {detailed_path}\")\n",
    "    print(f\"   - 格式: JSONL (每行一個 JSON 物件)\")\n",
    "    print(f\"   - 總組合數: {len(results_summary)}\")\n",
    "    print(f\"   - 詳細記錄: {len(results_detailed)} 條\")\n",
    "    print(f\"\\n✅ 資料完整性保證：LLM 回答中的換行符被正確保留\")\n",
    "    \n",
    "    # 轉為 DataFrame 供後續分析\n",
    "    df_summary = pd.DataFrame(results_summary)\n",
    "    df_detailed = pd.DataFrame(results_detailed)\n",
    "    \n",
    "    return df_summary, df_detailed, summary_path, detailed_path\n",
    "\n",
    "print(\"✅ JSONL 版本消融執行器已載入\")\n",
    "print(\"🔧 關鍵修正：解決 CSV 換行符衝突問題\")\n",
    "print(\"\\n💡 使用方式：\")\n",
    "print(\"   ABLATION_RESULTS_JSONL_SUMMARY, ABLATION_RESULTS_JSONL_DETAILED, summary_path, detailed_path = run_ablation_experiment_jsonl(\")\n",
    "print(\"       chunk_sizes=ABLATION_CONFIG['chunk_sizes'],\")\n",
    "print(\"       overlaps=ABLATION_CONFIG['overlaps'],\")\n",
    "print(\"       questions_df=QUESTIONS_DF_ABLATION,\")\n",
    "print(\"       llm=ABLATION_LLM,\")\n",
    "print(\"       embedder=ABLATION_EMBEDDER,\")\n",
    "print(\"       max_questions=ABLATION_CONFIG['fixed_params']['max_questions']\")\n",
    "print(\"   )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78755ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 開始 JSONL 版本消融實驗（資料完整性保證）\n",
      "\n",
      "✅ 關鍵改進：使用 JSONL 格式避免 CSV 換行符衝突\n",
      "✅ 確保 LLM 回答完整性，保證指標計算準確性\n",
      "\n",
      "🔧 BUGFIX: 支援多種參考答案欄位名稱 (answer, reference_answer, expected_answer)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "🧪 組合 [1/2]: chunk_size=2048, overlap=256\n",
      "================================================================================\n",
      "  ✅ 資料庫已清空\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs2048_ov256, chunk_size=2048, overlap=256\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 388 chunks\n",
      "  ↳ Upserted 388, skipped 0\n",
      "  ↳ 三元組寫入: 383 chunks（跳過 5）\n",
      "    ⚠️ 無三元組: 5 chunks\n",
      "  📊 結果: chunks=388, entities=5481, relations=4590, density=0.837\n",
      "  ✅ 圖譜構建完成: 5481 entities, 4590 relations\n",
      "\n",
      "  📝 開始問答評估 (150 題)...\n",
      "  📊 QA結果: F1=0.143, EM=0.000, CosSim=0.689, Effective=93.3%\n",
      "  💾 中間結果已保存（JSONL 格式）\n",
      "\n",
      "================================================================================\n",
      "🧪 組合 [2/2]: chunk_size=2048, overlap=512\n",
      "================================================================================\n",
      "  ✅ 資料庫已清空\n",
      "\n",
      "🏭 構建圖譜（修正版）: dataset=goat_data_text_collection-1.2-eng_cs2048_ov512, chunk_size=2048, overlap=512\n",
      "  → 使用 chunk_text 函式（cell 29 版本）\n",
      "  ↳ 切分完成: 452 chunks\n",
      "  ↳ Upserted 452, skipped 0\n",
      "  ↳ 三元組寫入: 443 chunks（跳過 9）\n",
      "    ⚠️ 無三元組: 9 chunks\n",
      "  📊 結果: chunks=452, entities=6097, relations=5057, density=0.829\n",
      "  ✅ 圖譜構建完成: 6097 entities, 5057 relations\n",
      "\n",
      "  📝 開始問答評估 (150 題)...\n",
      "  📊 QA結果: F1=0.127, EM=0.000, CosSim=0.704, Effective=93.3%\n",
      "  💾 中間結果已保存（JSONL 格式）\n",
      "\n",
      "================================================================================\n",
      "✅ JSONL 版本消融實驗完成！\n",
      "================================================================================\n",
      "📊 彙總結果: c:\\Users\\kbllm\\Desktop\\llm-KB\\rag\\ablation_FIXED_quick_summary_20260108_204303.jsonl\n",
      "📝 詳細回答: c:\\Users\\kbllm\\Desktop\\llm-KB\\rag\\ablation_FIXED_quick_detailed_answers_20260108_204303.jsonl\n",
      "   - 格式: JSONL (每行一個 JSON 物件)\n",
      "   - 總組合數: 2\n",
      "   - 詳細記錄: 30 條\n",
      "\n",
      "✅ 資料完整性保證：LLM 回答中的換行符被正確保留\n",
      "💡 請取消註釋上方其中一個選項來執行消融實驗\n",
      "\n",
      "✅ 所有修正已完成：\n",
      "  1. 參考答案欄位修正 ✅\n",
      "  2. 英文 prompt 修正 ✅\n",
      "  3. JSONL 格式保證資料完整性 ✅\n",
      "\n",
      "建議流程：\n",
      "  1️⃣ 先執行選項 1（快速驗證）\n",
      "  2️⃣ 確認結果正常\n",
      "  3️⃣ 再執行選項 2（完整實驗）\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🚀 執行完整消融實驗（所有問題已修正）\n",
    "# ============================================================\n",
    "\n",
    "# 選項 1：快速驗證（3 組合 × 5 題，~1-2 小時）\n",
    "# 建議先執行此選項確認一切正常\n",
    "results = run_ablation_experiment_jsonl(\n",
    "    chunk_sizes=[2048],\n",
    "    overlaps=[256,512],\n",
    "    questions_df=QUESTIONS_DF_ABLATION,\n",
    "    llm=ABLATION_LLM,\n",
    "    embedder=ABLATION_EMBEDDER,\n",
    "    max_questions=150,\n",
    "    output_dir=Path('c:/Users/kbllm/Desktop/llm-KB/rag'),\n",
    "    output_prefix='ablation_FIXED_quick'\n",
    ")\n",
    "\n",
    "# 選項 2：完整消融（9 組合 × 100 題，~20-24 小時）\n",
    "# 確認快速驗證成功後再執行\n",
    "# results = run_ablation_experiment_jsonl(\n",
    "#     chunk_sizes=[1024, 2048, 4096],\n",
    "#     overlaps=[128, 256, 512],\n",
    "#     questions_df=QUESTIONS_DF_ABLATION,\n",
    "#     llm=ABLATION_LLM,\n",
    "#     embedder=ABLATION_EMBEDDER,\n",
    "#     max_questions=100,\n",
    "#     output_dir=Path('c:/Users/kbllm/Desktop/llm-KB/rag'),\n",
    "#     output_prefix='ablation_FIXED_full'\n",
    "# )\n",
    "\n",
    "print(\"💡 請取消註釋上方其中一個選項來執行消融實驗\")\n",
    "print(\"\\n✅ 所有修正已完成：\")\n",
    "print(\"  1. 參考答案欄位修正 ✅\")\n",
    "print(\"  2. 英文 prompt 修正 ✅\")\n",
    "print(\"  3. JSONL 格式保證資料完整性 ✅\")\n",
    "print(\"\\n建議流程：\")\n",
    "print(\"  1️⃣ 先執行選項 1（快速驗證）\")\n",
    "print(\"  2️⃣ 確認結果正常\")\n",
    "print(\"  3️⃣ 再執行選項 2（完整實驗）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2862c39",
   "metadata": {},
   "source": [
    "# 🔍 多跳推理（Multi-Hop Reasoning）參數控制\n",
    "\n",
    "## 📊 當前檢索深度分析\n",
    "\n",
    "在 Graph-based RAG 架構中，**檢索深度（Retrieval Depth / Hops）** 決定了系統在知識圖譜中的遍歷範圍：\n",
    "\n",
    "### 🎯 目前狀態\n",
    "- **當前檢索模式**: `HybridRetriever` (向量 + 全文)\n",
    "- **實際跳數**: **1-hop（單跳）**\n",
    "  - 系統僅檢索與查詢向量最相似的 **Chunk 節點**\n",
    "  - 不進行圖譜遍歷（沒有沿著關係邊走向相鄰實體）\n",
    "\n",
    "### 📐 跳數說明\n",
    "\n",
    "| 跳數 | 範圍 | 說明 | 適用場景 |\n",
    "|------|------|------|----------|\n",
    "| **0-hop** | 僅查詢節點本身 | 無圖譜遍歷 | 向量檢索 |\n",
    "| **1-hop** | 直接相鄰節點 | `(chunk)-[:MENTIONS]->(entity)` | 當前實現 |\n",
    "| **2-hop** | 相鄰的相鄰 | `(chunk)-[:MENTIONS]->(e1)-[:RELATION]->(e2)` | 簡單推理 |\n",
    "| **3-hop** | 三度關聯 | `(e1)-[:R1]->(e2)-[:R2]->(e3)-[:R3]->(e4)` | 複雜推理 |\n",
    "\n",
    "### 🔬 多跳推理的價值\n",
    "\n",
    "**範例場景：** \"維生素A缺乏對山羊繁殖的影響\"\n",
    "\n",
    "- **1-hop**: 找到提到 \"維生素A\" 或 \"繁殖\" 的 Chunk\n",
    "- **2-hop**: \n",
    "  - `(Chunk)-[:MENTIONS]->(維生素A)-[:CAUSES]->(免疫力下降)`\n",
    "  - `(免疫力下降)-[:AFFECTS]->(繁殖性能)`\n",
    "- **3-hop**: \n",
    "  - `(維生素A)->(免疫力)->(發情週期)->(繁殖率)`\n",
    "  - 連接多個因果鏈\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 下一步：實現多跳推理\n",
    "\n",
    "以下 Cell 提供：\n",
    "1. 多跳檢索參數配置\n",
    "2. 自定義 Cypher 查詢支持 2/3-hop\n",
    "3. 多跳推理消融實驗框架\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90a84082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎛️ 多跳推理參數配置\n",
      "============================================================\n",
      "當前檢索深度: 1-hop\n",
      "最大支持跳數: 3\n",
      "消融實驗測試值: [0, 1, 2, 3]\n",
      "\n",
      "配置詳情:\n",
      "  - retrieval_depth: 1\n",
      "  - expand_entities: True\n",
      "  - expand_chunks: True\n",
      "  - min_relation_score: 0.3\n",
      "  - max_entities_per_hop: 10\n",
      "============================================================\n",
      "\n",
      "💡 使用說明:\n",
      "  1. 修改 RETRIEVAL_DEPTH 來切換跳數 (1/2/3)\n",
      "  2. 執行下方 Cell 來創建多跳檢索器\n",
      "  3. 使用 HOP_VALUES 進行消融實驗比較不同跳數效果\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🎛️ 多跳推理參數配置\n",
    "# ============================================================\n",
    "\n",
    "# 🔧 全域多跳參數\n",
    "RETRIEVAL_DEPTH = 1  # 預設為 1-hop（當前實現）\n",
    "MAX_HOPS = 3         # 最大支持跳數\n",
    "HOP_VALUES = [0,1, 2, 3]  # 消融實驗測試值0是vector-only\n",
    "\n",
    "# 📊 多跳檢索配置\n",
    "MULTIHOP_CONFIG = {\n",
    "    \"retrieval_depth\": RETRIEVAL_DEPTH,  # 當前使用的跳數\n",
    "    \"expand_entities\": True,              # 是否擴展實體鄰居\n",
    "    \"expand_chunks\": True,                # 是否包含相關 Chunks\n",
    "    \"min_relation_score\": 0.3,           # 關係最小權重閾值\n",
    "    \"max_entities_per_hop\": 10,          # 每跳最多擴展實體數\n",
    "}\n",
    "\n",
    "print(\"🎛️ 多跳推理參數配置\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"當前檢索深度: {RETRIEVAL_DEPTH}-hop\")\n",
    "print(f\"最大支持跳數: {MAX_HOPS}\")\n",
    "print(f\"消融實驗測試值: {HOP_VALUES}\")\n",
    "print(\"\\n配置詳情:\")\n",
    "for key, value in MULTIHOP_CONFIG.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n💡 使用說明:\")\n",
    "print(\"  1. 修改 RETRIEVAL_DEPTH 來切換跳數 (1/2/3)\")\n",
    "print(\"  2. 執行下方 Cell 來創建多跳檢索器\")\n",
    "print(\"  3. 使用 HOP_VALUES 進行消融實驗比較不同跳數效果\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52d2cb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ test_multihop_retrieval 函數已定義\n",
      "\n",
      "💡 使用範例:\n",
      "  results = test_multihop_retrieval(\n",
      "      question=\"What are the effects of vitamin A deficiency in goats?\",\n",
      "      reference_answer=\"Vitamin A deficiency causes...\",\n",
      "      hop_values=[1, 2, 3]\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🧪 多跳推理測試與比較\n",
    "# ============================================================\n",
    "\n",
    "def test_multihop_retrieval(question: str, reference_answer: str, hop_values: List[int] = [1, 2, 3]):\n",
    "    \"\"\"\n",
    "    測試不同跳數的檢索效果\n",
    "    \"\"\"\n",
    "    print(f\"🔍 測試問題: {question}\")\n",
    "    print(f\"📝 參考答案: {reference_answer}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for hops in hop_values:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🎯 測試 {hops}-hop 檢索\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # 創建多跳檢索器\n",
    "            multihop_retriever = MultiHopRetriever(\n",
    "                driver=GRAPH_DRIVER,\n",
    "                vector_index_name=VECTOR_INDEX_NAME,\n",
    "                embedder=GRAPH_EMBEDDER,\n",
    "                retrieval_depth=hops,\n",
    "                max_entities_per_hop=100,\n",
    "            )\n",
    "            \n",
    "            # 執行檢索\n",
    "            search_result = multihop_retriever.search(\n",
    "                query_text=question,\n",
    "                top_k=TOP_K\n",
    "            )\n",
    "            \n",
    "            # 顯示檢索結果（records 是 Neo4j Record 對象）\n",
    "            print(f\"\\n📊 檢索到 {len(search_result.records)} 個結果:\")\n",
    "            for idx, record in enumerate(search_result.records[:3]):  # 只顯示前3個\n",
    "                node = record['node']  # Chunk 節點\n",
    "                score = record['score']\n",
    "                \n",
    "                print(f\"\\n  [{idx+1}] Score: {score:.3f}\")\n",
    "                print(f\"      Chunk ID: {node.get('id', 'N/A')}\")\n",
    "                print(f\"      Text: {node.get('text', '')[:150]}...\")\n",
    "            \n",
    "            # 構建上下文並生成答案（從 Neo4j Record 中提取文本）\n",
    "            contexts = []\n",
    "            for record in search_result.records[:TOP_K]:\n",
    "                chunk_node = record['node']\n",
    "                text = chunk_node.get('text', '')\n",
    "                if text:\n",
    "                    contexts.append(text)\n",
    "            \n",
    "            context = \"\\n\\n\".join(contexts)\n",
    "            prompt = f\"\"\"Based on the following context, answer the question in {ANSWER_LANGUAGE}.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "IMPORTANT: Answer with ONLY a comma-separated list or a single short sentence (max 10 words). NO introductory phrases like \\\"The answer is\\\" or \\\"Based on\\\". NO explanations. NO formatting. Just the direct answer.\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            response = GRAPH_LLM.invoke(prompt)\n",
    "            predicted = response.content\n",
    "            \n",
    "            # 計算指標\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            pred_emb = GRAPH_EMBEDDER.embed_query(predicted)\n",
    "            ref_emb = GRAPH_EMBEDDER.embed_query(reference_answer)\n",
    "            cosine_sim = cosine_similarity([pred_emb], [ref_emb])[0][0]\n",
    "            \n",
    "            # 簡易 F1（基於詞彙重疊）\n",
    "            pred_tokens = set(predicted.lower().split())\n",
    "            ref_tokens = set(reference_answer.lower().split())\n",
    "            common = pred_tokens & ref_tokens\n",
    "            precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
    "            recall = len(common) / len(ref_tokens) if ref_tokens else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            results[hops] = {\n",
    "                'predicted': predicted,\n",
    "                'f1': f1,\n",
    "                'cosine_sim': cosine_sim,\n",
    "                'num_contexts': len(search_result.records)\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n✅ {hops}-hop 結果:\")\n",
    "            print(f\"   - 預測答案: {predicted[:100]}...\")\n",
    "            print(f\"   - F1 Score: {f1:.3f}\")\n",
    "            print(f\"   - Cosine Similarity: {cosine_sim:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {hops}-hop 測試失敗: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results[hops] = None\n",
    "    \n",
    "    # 比較結果\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"📊 多跳推理效果比較\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Hops':<8} {'F1':<10} {'CosSim':<10} {'Contexts':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    for hops in hop_values:\n",
    "        if results[hops]:\n",
    "            r = results[hops]\n",
    "            print(f\"{hops:<8} {r['f1']:<10.3f} {r['cosine_sim']:<10.3f} {r['num_contexts']:<10}\")\n",
    "        else:\n",
    "            print(f\"{hops:<8} {'FAILED':<10} {'FAILED':<10} {'FAILED':<10}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"✅ test_multihop_retrieval 函數已定義\")\n",
    "print(\"\\n💡 使用範例:\")\n",
    "print('  results = test_multihop_retrieval(')\n",
    "print('      question=\"What are the effects of vitamin A deficiency in goats?\",')\n",
    "print('      reference_answer=\"Vitamin A deficiency causes...\",')\n",
    "print('      hop_values=[1, 2, 3]')\n",
    "print('  )')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e154c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ run_multihop_ablation 函數已定義\n",
      "\n",
      "💡 執行範例:\n",
      "  summary, detailed = run_multihop_ablation(\n",
      "      questions_df=QUESTIONS_DF_ABLATION,\n",
      "      hop_values=[1, 2, 3],\n",
      "      max_questions=20,\n",
      "      output_dir=Path('./multihop_results')\n",
      "  )\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔬 多跳推理消融實驗\n",
    "# ============================================================\n",
    "\n",
    "def run_multihop_ablation(\n",
    "    questions_df: pd.DataFrame,\n",
    "    hop_values: List[int] = [0,1, 2, 3],\n",
    "    max_questions: int = 100,\n",
    "    output_dir: Path = Path(\"./multihop_results\")\n",
    "):\n",
    "    \"\"\"\n",
    "    執行多跳推理的消融實驗\n",
    "    \n",
    "    參數:\n",
    "        questions_df: 問題資料集\n",
    "        hop_values: 要測試的跳數列表\n",
    "        max_questions: 測試問題數量\n",
    "        output_dir: 結果輸出目錄\n",
    "    \n",
    "    返回:\n",
    "        (summary_df, detailed_df): 摘要和詳細結果\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(\"🔬 開始多跳推理消融實驗\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"測試跳數: {hop_values}\")\n",
    "    print(f\"問題數量: {max_questions}\")\n",
    "    print(f\"結果目錄: {output_dir}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for hop in hop_values:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🎯 實驗 {hop}-hop (共 {len(hop_values)} 組)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # 創建檢索器\n",
    "        retriever = MultiHopRetriever(\n",
    "            driver=GRAPH_DRIVER,\n",
    "            vector_index_name=VECTOR_INDEX_NAME,\n",
    "            embedder=GRAPH_EMBEDDER,\n",
    "            retrieval_depth=hop,\n",
    "            max_entities_per_hop=100,\n",
    "        )\n",
    "        \n",
    "        hop_results = []\n",
    "        \n",
    "        for idx, row in questions_df.head(max_questions).iterrows():\n",
    "            question = row['question']\n",
    "            reference = row.get('answer', row.get('reference_answer', ''))\n",
    "            \n",
    "            try:\n",
    "                # 檢索\n",
    "                search_result = retriever.search(query_text=question, top_k=TOP_K)\n",
    "                \n",
    "                # 生成答案（從 Neo4j Records 提取文本）\n",
    "                contexts = []\n",
    "                for record in search_result.records[:TOP_K]:\n",
    "                    chunk_node = record['node']\n",
    "                    text = chunk_node.get('text', '')\n",
    "                    if text:\n",
    "                        contexts.append(text)\n",
    "                \n",
    "                context = \"\\n\\n\".join(contexts)\n",
    "                prompt = f\"\"\"Based on the following context, answer the question in {ANSWER_LANGUAGE}.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "IMPORTANT: Answer with ONLY a comma-separated list or a single short sentence (max 10 words). NO introductory phrases like \\\"The answer is\\\" or \\\"Based on\\\". NO explanations. NO formatting. Just the direct answer.\n",
    "Answer:\"\"\"\n",
    "                \n",
    "                response = GRAPH_LLM.invoke(prompt)\n",
    "                predicted = response.content\n",
    "                \n",
    "                # 計算指標\n",
    "                pred_emb = GRAPH_EMBEDDER.embed_query(predicted)\n",
    "                ref_emb = GRAPH_EMBEDDER.embed_query(reference)\n",
    "                cosine_sim = float(cosine_similarity([pred_emb], [ref_emb])[0][0])\n",
    "                \n",
    "                pred_tokens = set(predicted.lower().split())\n",
    "                ref_tokens = set(reference.lower().split())\n",
    "                common = pred_tokens & ref_tokens\n",
    "                precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
    "                recall = len(common) / len(ref_tokens) if ref_tokens else 0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                em = 1 if predicted.lower().strip() == reference.lower().strip() else 0\n",
    "                \n",
    "                # 記錄結果\n",
    "                result = {\n",
    "                    'hop': hop,\n",
    "                    'question_id': idx,\n",
    "                    'question': question,\n",
    "                    'reference': reference,\n",
    "                    'predicted': predicted,\n",
    "                    'f1': float(f1),\n",
    "                    'em': int(em),\n",
    "                    'cosine_sim': cosine_sim,\n",
    "                    'num_contexts': len(search_result.records),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                hop_results.append(result)\n",
    "                all_results.append(result)\n",
    "                \n",
    "                if (idx + 1) % 5 == 0:\n",
    "                    print(f\"  ✓ 已完成 {idx + 1}/{max_questions} 題\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Q{idx} 失敗: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 計算該跳數的平均指標\n",
    "        if hop_results:\n",
    "            avg_f1 = sum(r['f1'] for r in hop_results) / len(hop_results)\n",
    "            avg_em = sum(r['em'] for r in hop_results) / len(hop_results)\n",
    "            avg_cosine = sum(r['cosine_sim'] for r in hop_results) / len(hop_results)\n",
    "            \n",
    "            print(f\"\\n✅ {hop}-hop 完成:\")\n",
    "            print(f\"   - Avg F1: {avg_f1:.3f}\")\n",
    "            print(f\"   - Avg EM: {avg_em:.3f}\")\n",
    "            print(f\"   - Avg CosSim: {avg_cosine:.3f}\")\n",
    "    \n",
    "    # 保存結果\n",
    "    detailed_df = pd.DataFrame(all_results)\n",
    "    detailed_path = output_dir / f\"multihop_detailed_{timestamp}.csv\"\n",
    "    detailed_df.to_csv(detailed_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    # 計算摘要\n",
    "    summary_data = []\n",
    "    for hop in hop_values:\n",
    "        hop_data = detailed_df[detailed_df['hop'] == hop]\n",
    "        if len(hop_data) > 0:\n",
    "            summary_data.append({\n",
    "                'hop': hop,\n",
    "                'avg_f1': hop_data['f1'].mean(),\n",
    "                'avg_em': hop_data['em'].mean(),\n",
    "                'avg_cosine_sim': hop_data['cosine_sim'].mean(),\n",
    "                'total_questions': len(hop_data),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_path = output_dir / f\"multihop_summary_{timestamp}.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✅ 多跳推理消融實驗完成！\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"📊 摘要: {summary_path}\")\n",
    "    print(f\"📝 詳細: {detailed_path}\")\n",
    "    print(f\"\\n結果預覽:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return summary_df, detailed_df\n",
    "\n",
    "\n",
    "print(\"✅ run_multihop_ablation 函數已定義\")\n",
    "print(\"\\n💡 執行範例:\")\n",
    "print(\"  summary, detailed = run_multihop_ablation(\")\n",
    "print(\"      questions_df=QUESTIONS_DF_ABLATION,\")\n",
    "print(\"      hop_values=[1, 2, 3],\")\n",
    "print(\"      max_questions=20,\")\n",
    "print(\"      output_dir=Path('./multihop_results')\")\n",
    "print(\"  )\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 檢查依賴...\n",
      "✅ MultiHopRetriever 已定義\n",
      "\n",
      "🔍 選項 1: 單問題多跳比較\n",
      "================================================================================\n",
      "測試問題: What are the main dairy goat breeds registered by the American Dairy Goat Association?\n",
      "🔍 測試問題: What are the main dairy goat breeds registered by the American Dairy Goat Association?\n",
      "📝 參考答案: Saanen, Alpine, Toggenburg, Nubian.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "🎯 測試 1-hop 檢索\n",
      "================================================================================\n",
      "\n",
      "📊 檢索到 15 個結果:\n",
      "\n",
      "  [1] Score: 0.880\n",
      "      Chunk ID: goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00162\n",
      "      Text: th the United States and Taiwan. In recent years, a new breed, originally registered as the Melan, has been developed and registered in Australia. It ...\n",
      "\n",
      "  [2] Score: 0.879\n",
      "      Chunk ID: goat_data_text_collection-1.2-eng_chunk_00162\n",
      "      Text: th the United States and Taiwan. In recent years, a new breed, originally registered as the Melan, has been developed and registered in Australia. It ...\n",
      "\n",
      "  [3] Score: 0.875\n",
      "      Chunk ID: goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00183\n",
      "      Text: essively high G/I ratios, such as those in India or Brazil, which only graze cattle, are not considered advanced. However, China's G/I ratio has not y...\n",
      "\n",
      "✅ 1-hop 結果:\n",
      "   - 預測答案: \n",
      "\n",
      "The primary dairy goat breeds registered by the American Dairy Goat Association (ADGA) include:\n",
      "\n",
      "1...\n",
      "   - F1 Score: 0.000\n",
      "   - Cosine Similarity: 0.586\n",
      "\n",
      "================================================================================\n",
      "🎯 測試 2-hop 檢索\n",
      "================================================================================\n",
      "\n",
      "📊 檢索到 30 個結果:\n",
      "\n",
      "  [1] Score: 0.880\n",
      "      Chunk ID: goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00162\n",
      "      Text: th the United States and Taiwan. In recent years, a new breed, originally registered as the Melan, has been developed and registered in Australia. It ...\n",
      "\n",
      "  [2] Score: 0.616\n",
      "      Chunk ID: goat_data_text_collection-1.2-eng_cs2048_ov512_chunk_00161\n",
      "      Text: d influx of cheap Australian and New Zealand lamb into Taiwan, domestic mutton farmers should remain calm and diligently prepare for market differenti...\n",
      "\n",
      "  [3] Score: 0.616\n",
      "      Chunk ID: goat_data_text_collection-1.2-eng_chunk_00412\n",
      "      Text: in older lambs and less frequently in adult sheep. It is characterized by a rapid rise in temperature (104-106°F), accompanied by vomiting, bloody dia...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🚀 快速開始：多跳推理測試\n",
    "# ============================================================\n",
    "\n",
    "# ⚠️ 先檢查必要的類別是否已定義\n",
    "print(\"🔍 檢查依賴...\")\n",
    "if 'MultiHopRetriever' not in globals():\n",
    "    print(\"❌ 錯誤：MultiHopRetriever 未定義！\")\n",
    "    print(\"\\n📌 解決方法：\")\n",
    "    print(\"   1. 滾動到 Notebook 最頂部\")\n",
    "    print(\"   2. 執行 Cell 1（⚠️ 重要說明）\")\n",
    "    print(\"   3. 執行 Cell 2（🔍 MultiHopRetriever 類別定義）\")\n",
    "    print(\"   4. 然後回到此處重新執行本 Cell\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"✅ MultiHopRetriever 已定義\")\n",
    "    \n",
    "    # 選項 1：單個問題測試（建議先執行）\n",
    "    print(\"\\n🔍 選項 1: 單問題多跳比較\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 從問題集中選一個問題\n",
    "    if 'QUESTIONS_DF_ABLATION' in globals() and len(QUESTIONS_DF_ABLATION) > 0:\n",
    "        test_q = QUESTIONS_DF_ABLATION.iloc[0]\n",
    "        print(f\"測試問題: {test_q['question']}\")\n",
    "        \n",
    "        # 執行 1-hop, 2-hop, 3-hop 比較\n",
    "        results = test_multihop_retrieval(\n",
    "            question=test_q['question'],\n",
    "            reference_answer=test_q.get('answer', test_q.get('reference_answer', '')),\n",
    "            hop_values=[0,1, 2, 3]\n",
    "        )\n",
    "    else:\n",
    "        print(\"⚠️ 請先載入 QUESTIONS_DF_ABLATION\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔬 選項 2: 完整消融實驗\")\n",
    "print(\"=\"*80)\n",
    "print(\"# 測試 20 個問題，比較 1/2/3-hop 的效果\")\n",
    "print(\"# summary, detailed = run_multihop_ablation(\")\n",
    "print(\"#     questions_df=QUESTIONS_DF_ABLATION,\")\n",
    "print(\"#     hop_values=[1, 2, 3],\")\n",
    "print(\"#     max_questions=20,\")\n",
    "print(\"#     output_dir=Path('./multihop_results')\")\n",
    "print(\"# )\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 當前配置摘要\")\n",
    "print(\"=\"*80)\n",
    "print(f\"檢索深度: {MULTIHOP_CONFIG['retrieval_depth']}-hop\")\n",
    "print(f\"向量索引: {VECTOR_INDEX_NAME}\")\n",
    "print(f\"Top-K: {TOP_K}\")\n",
    "print(f\"LLM: {LLM_MODEL}\")\n",
    "print(f\"Embedder: {EMBED_MODEL}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d90b4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 查看最近一次測試的完整答案：\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🎯 1-hop 預測答案：\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Saanen, Nubian, Toggenburg\n",
      "--------------------------------------------------------------------------------\n",
      "答案長度: 28 字符\n",
      "F1: 0.286 | CosSim: 0.974\n",
      "\n",
      "\n",
      "🎯 2-hop 預測答案：\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Saanen, Albanian, Nubian, Toggenburg\n",
      "--------------------------------------------------------------------------------\n",
      "答案長度: 38 字符\n",
      "F1: 0.250 | CosSim: 0.990\n",
      "\n",
      "\n",
      "🎯 3-hop 預測答案：\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "The main dairy goat breeds registered by the American Dairy Goat Association include the Saanen, Albanian, Nubian, and Toggenburg.\n",
      "--------------------------------------------------------------------------------\n",
      "答案長度: 132 字符\n",
      "F1: 0.105 | CosSim: 0.599\n",
      "\n",
      "\n",
      "📌 參考答案（目標格式）：\n",
      "--------------------------------------------------------------------------------\n",
      "Saanen, Alpine, Toggenburg, Nubian.\n",
      "--------------------------------------------------------------------------------\n",
      "參考答案長度: 35 字符\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🔍 查看完整的預測答案（分析答案長度問題）\n",
    "# ============================================================\n",
    "\n",
    "print(\"📝 查看最近一次測試的完整答案：\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'results' in dir() and results:\n",
    "    for hop, result in results.items():\n",
    "        if result:\n",
    "            print(f\"\\n🎯 {hop}-hop 預測答案：\")\n",
    "            print(\"-\" * 80)\n",
    "            print(result['predicted'])\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"答案長度: {len(result['predicted'])} 字符\")\n",
    "            print(f\"F1: {result['f1']:.3f} | CosSim: {result['cosine_sim']:.3f}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"❌ 沒有可用的測試結果，請先執行上方的測試 cell\")\n",
    "    \n",
    "print(\"\\n📌 參考答案（目標格式）：\")\n",
    "print(\"-\" * 80)\n",
    "print(test_ref)\n",
    "print(\"-\" * 80)\n",
    "print(f\"參考答案長度: {len(test_ref)} 字符\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03a608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 開始小規模消融實驗驗證...\n",
      "================================================================================\n",
      "🔬 開始多跳推理消融實驗\n",
      "================================================================================\n",
      "測試跳數: [1, 2, 3]\n",
      "問題數量: 3\n",
      "結果目錄: multihop_results\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "🎯 實驗 1-hop (共 3 組)\n",
      "================================================================================\n",
      "\n",
      "✅ 1-hop 完成:\n",
      "   - Avg F1: 0.173\n",
      "   - Avg EM: 0.000\n",
      "   - Avg CosSim: 0.734\n",
      "\n",
      "================================================================================\n",
      "🎯 實驗 2-hop (共 3 組)\n",
      "================================================================================\n",
      "\n",
      "✅ 2-hop 完成:\n",
      "   - Avg F1: 0.146\n",
      "   - Avg EM: 0.000\n",
      "   - Avg CosSim: 0.753\n",
      "\n",
      "================================================================================\n",
      "🎯 實驗 3-hop (共 3 組)\n",
      "================================================================================\n",
      "\n",
      "✅ 3-hop 完成:\n",
      "   - Avg F1: 0.219\n",
      "   - Avg EM: 0.000\n",
      "   - Avg CosSim: 0.774\n",
      "\n",
      "================================================================================\n",
      "✅ 多跳推理消融實驗完成！\n",
      "================================================================================\n",
      "📊 摘要: multihop_results\\multihop_summary_20260108_043342.csv\n",
      "📝 詳細: multihop_results\\multihop_detailed_20260108_043342.csv\n",
      "\n",
      "結果預覽:\n",
      " hop   avg_f1  avg_em  avg_cosine_sim  total_questions                  timestamp\n",
      "   1 0.172541     0.0        0.733795                3 2026-01-08T04:34:13.727376\n",
      "   2 0.146296     0.0        0.752770                3 2026-01-08T04:34:13.727376\n",
      "   3 0.218519     0.0        0.773681                3 2026-01-08T04:34:13.728714\n",
      "\n",
      "================================================================================\n",
      "📊 消融實驗摘要結果\n",
      "================================================================================\n",
      "   hop    avg_f1  avg_em  avg_cosine_sim  total_questions  \\\n",
      "0    1  0.172541     0.0        0.733795                3   \n",
      "1    2  0.146296     0.0        0.752770                3   \n",
      "2    3  0.218519     0.0        0.773681                3   \n",
      "\n",
      "                    timestamp  \n",
      "0  2026-01-08T04:34:13.727376  \n",
      "1  2026-01-08T04:34:13.727376  \n",
      "2  2026-01-08T04:34:13.728714  \n",
      "\n",
      "================================================================================\n",
      "📋 詳細結果（前5筆）\n",
      "================================================================================\n",
      "   hop  question_id                                           question  \\\n",
      "0    1            0  What are the main dairy goat breeds registered...   \n",
      "1    1            1       What are the advantages of the Alpine breed?   \n",
      "2    1            2  What are the coat color characteristics of the...   \n",
      "3    2            0  What are the main dairy goat breeds registered...   \n",
      "4    2            1       What are the advantages of the Alpine breed?   \n",
      "\n",
      "                                           reference  \\\n",
      "0                Saanen, Alpine, Toggenburg, Nubian.   \n",
      "1  Strong constitution, endurance for long travel...   \n",
      "2  Coat color is variable, with brown shades bein...   \n",
      "3                Saanen, Alpine, Toggenburg, Nubian.   \n",
      "4  Strong constitution, endurance for long travel...   \n",
      "\n",
      "                                           predicted        f1  em  \\\n",
      "0           \\n\\nSaanen, Albanian, Nubian, Toggenburg  0.250000   0   \n",
      "1  \\n\\nwell-proportioned limbs, high-quality offs...  0.074074   0   \n",
      "2  \\n\\nThe context does not specify the coat colo...  0.193548   0   \n",
      "3           \\n\\nSaanen, Albanian, Nubian, Toggenburg  0.250000   0   \n",
      "4  \\n\\ngood body shape, skin and coat quality, hi...  0.133333   0   \n",
      "\n",
      "   cosine_sim  num_contexts                   timestamp  \n",
      "0    0.989852             4  2026-01-08T04:33:46.696363  \n",
      "1    0.600991             4  2026-01-08T04:33:49.240627  \n",
      "2    0.610542             4  2026-01-08T04:33:52.505666  \n",
      "3    0.989852             8  2026-01-08T04:33:57.178303  \n",
      "4    0.560077             8  2026-01-08T04:33:59.902772  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 🧪 驗證：運行小規模消融實驗（3個問題）\n",
    "# ============================================================\n",
    "\n",
    "print(\"🔬 開始消融實驗驗證...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 運行 3 個問題的消融實驗\n",
    "summary_df, detailed_df = run_multihop_ablation(\n",
    "    questions_df=QUESTIONS_DF_ABLATION,\n",
    "    hop_values=[0,1, 2, 3],\n",
    "    max_questions=150,\n",
    "    output_dir=Path('./multihop_results')\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📊 消融實驗摘要結果\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📋 詳細結果（前5筆）\")\n",
    "print(\"=\" * 80)\n",
    "print(detailed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "26cac7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 多跳推理消融實驗結果摘要\n",
      "================================================================================\n",
      " hop   avg_f1  avg_em  avg_cosine_sim  total_questions                  timestamp\n",
      "   1 0.107804     0.0        0.670728               15 2026-01-08T04:31:30.133120\n",
      "   2 0.133549     0.0        0.663277               15 2026-01-08T04:31:30.133120\n",
      "   3 0.071822     0.0        0.650531               15 2026-01-08T04:31:30.133120\n",
      "\n",
      "================================================================================\n",
      "🎯 最佳配置分析\n",
      "================================================================================\n",
      "✨ 最佳 F1 配置: 2-hop\n",
      "   - Avg F1: 0.134\n",
      "   - Avg CosSim: 0.663\n",
      "   - Avg EM: 0.000\n",
      "\n",
      "🎯 最佳 Cosine Similarity 配置: 1-hop\n",
      "   - Avg F1: 0.108\n",
      "   - Avg CosSim: 0.671\n",
      "   - Avg EM: 0.000\n",
      "\n",
      "================================================================================\n",
      "💡 結論\n",
      "================================================================================\n",
      "多跳推理功能已成功實現並運行！\n",
      "✅ 1-hop, 2-hop, 3-hop 都可以正常檢索和生成答案\n",
      "✅ 可以執行完整的消融實驗來比較不同跳數的效果\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 📊 查看消融實驗結果摘要\n",
    "# ============================================================\n",
    "\n",
    "print(\"📊 多跳推理消融實驗結果摘要\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎯 最佳配置分析\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_f1_row = summary_df.loc[summary_df['avg_f1'].idxmax()]\n",
    "best_cosine_row = summary_df.loc[summary_df['avg_cosine_sim'].idxmax()]\n",
    "\n",
    "print(f\"✨ 最佳 F1 配置: {best_f1_row['hop']}-hop\")\n",
    "print(f\"   - Avg F1: {best_f1_row['avg_f1']:.3f}\")\n",
    "print(f\"   - Avg CosSim: {best_f1_row['avg_cosine_sim']:.3f}\")\n",
    "print(f\"   - Avg EM: {best_f1_row['avg_em']:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 最佳 Cosine Similarity 配置: {best_cosine_row['hop']}-hop\")\n",
    "print(f\"   - Avg F1: {best_cosine_row['avg_f1']:.3f}\")\n",
    "print(f\"   - Avg CosSim: {best_cosine_row['avg_cosine_sim']:.3f}\")\n",
    "print(f\"   - Avg EM: {best_cosine_row['avg_em']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"💡 結論\")\n",
    "print(\"=\" * 80)\n",
    "print(\"多跳推理功能已成功實現並運行！\")\n",
    "print(\"✅ 1-hop, 2-hop, 3-hop 都可以正常檢索和生成答案\")\n",
    "print(\"✅ 可以執行完整的消融實驗來比較不同跳數的效果\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f330e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📚 多跳推理實現總結\n",
    "\n",
    "### ✅ 已完成\n",
    "\n",
    "1. **參數配置** (Cell 53)\n",
    "   - `RETRIEVAL_DEPTH`: 當前檢索深度（預設 1-hop）\n",
    "   - `MULTIHOP_CONFIG`: 完整多跳配置字典\n",
    "   - `HOP_VALUES`: 消融實驗測試值 [1, 2, 3]\n",
    "\n",
    "2. **MultiHopRetriever 類別** (Cell 1)\n",
    "   - 支持 1/2/3-hop 檢索\n",
    "   - 自定義 Cypher 查詢生成\n",
    "   - 自動路徑追蹤和分數衰減\n",
    "\n",
    "3. **測試函數** (Cell 55)\n",
    "   - `test_multihop_retrieval()`: 單問題多跳比較\n",
    "   - 即時顯示檢索路徑和結果\n",
    "\n",
    "4. **消融實驗框架** (Cell 56)\n",
    "   - `run_multihop_ablation()`: 批量多跳實驗\n",
    "   - CSV 結果導出\n",
    "   - 自動統計分析\n",
    "\n",
    "### 🎯 使用流程\n",
    "\n",
    "```python\n",
    "# 步驟 1: 配置參數（在 Cell 53 修改）\n",
    "RETRIEVAL_DEPTH = 2  # 切換到 2-hop\n",
    "\n",
    "# 步驟 2: 單問題測試（Cell 57）\n",
    "results = test_multihop_retrieval(\n",
    "    question=\"What causes vitamin A deficiency?\",\n",
    "    reference_answer=\"...\",\n",
    "    hop_values=[1, 2, 3]\n",
    ")\n",
    "\n",
    "# 步驟 3: 完整實驗（Cell 57）\n",
    "summary, detailed = run_multihop_ablation(\n",
    "    questions_df=QUESTIONS_DF_ABLATION,\n",
    "    hop_values=[1, 2, 3],\n",
    "    max_questions=20\n",
    ")\n",
    "```\n",
    "\n",
    "### 📊 預期效果\n",
    "\n",
    "| Hop | 優勢 | 劣勢 |\n",
    "|-----|------|------|\n",
    "| **1-hop** | 快速、精準 | 無法推理 |\n",
    "| **2-hop** | 簡單推理、相關實體 | 略慢 |\n",
    "| **3-hop** | 複雜推理、遠程關聯 | 可能引入噪音 |\n",
    "\n",
    "### 🔬 實驗建議\n",
    "\n",
    "1. **快速驗證**: 先用 5-10 題測試各跳數\n",
    "2. **完整對比**: 20-50 題比較 1/2/3-hop\n",
    "3. **場景分析**: 分析哪些問題類型適合多跳\n",
    "\n",
    "### 🚨 注意事項\n",
    "\n",
    "- **圖譜規模**: 跳數越高，查詢越慢\n",
    "- **數據品質**: 需要高品質的實體和關係\n",
    "- **分數衰減**: 2/3-hop 自動降低遠程節點權重\n",
    "- **索引需求**: 確保 `chunk_embeddings` 索引存在\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🎯 Reranking 評估：函數定義與配置\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# 核心函數 1: 二次 Reranking（基於相似度過濾）\n",
    "# ============================================================\n",
    "def rerank_contexts_by_similarity(contexts, question, embedder, threshold=0.5, top_n=5):\n",
    "    \"\"\"\n",
    "    對檢索到的 contexts 進行二次排序\n",
    "    \n",
    "    Args:\n",
    "        contexts: 檢索到的文本片段列表\n",
    "        question: 用戶問題\n",
    "        embedder: 嵌入模型\n",
    "        threshold: 最小相似度閾值（過濾低質量結果）\n",
    "        top_n: 返回前 N 個結果\n",
    "    \n",
    "    Returns:\n",
    "        (排序後的 contexts, 相似度分數列表)\n",
    "    \"\"\"\n",
    "    if not contexts:\n",
    "        return [], []\n",
    "    \n",
    "    question_emb = embedder.embed_query(question)\n",
    "    context_scores = []\n",
    "    \n",
    "    for ctx in contexts:\n",
    "        ctx_text = ctx.get('text', '') if isinstance(ctx, dict) else str(ctx)\n",
    "        if not ctx_text.strip():\n",
    "            continue\n",
    "            \n",
    "        ctx_emb = embedder.embed_query(ctx_text)\n",
    "        similarity = float(cosine_similarity([question_emb], [ctx_emb])[0][0])\n",
    "        \n",
    "        if similarity >= threshold:\n",
    "            context_scores.append({\n",
    "                'context': ctx,\n",
    "                'text': ctx_text,\n",
    "                'similarity': similarity\n",
    "            })\n",
    "    \n",
    "    context_scores.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    top_contexts = context_scores[:top_n]\n",
    "    \n",
    "    return [item['context'] for item in top_contexts], [item['similarity'] for item in top_contexts]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 核心函數 2: 計算評估指標\n",
    "# ============================================================\n",
    "def calculate_metrics(predicted_answer, reference_answer, embedder):\n",
    "    \"\"\"計算 F1, Exact Match, Cosine Similarity\"\"\"\n",
    "    \n",
    "    if not predicted_answer.strip() or not reference_answer.strip():\n",
    "        return {'f1': 0.0, 'exact_match': 0.0, 'cosine_similarity': 0.0}\n",
    "    \n",
    "    # F1 Score\n",
    "    pred_tokens = set(predicted_answer.lower().split())\n",
    "    ref_tokens = set(reference_answer.lower().split())\n",
    "    common = pred_tokens & ref_tokens\n",
    "    \n",
    "    if len(common) == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        precision = len(common) / len(pred_tokens) if pred_tokens else 0.0\n",
    "        recall = len(common) / len(ref_tokens) if ref_tokens else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # Exact Match\n",
    "    exact_match = 1.0 if predicted_answer.strip().lower() == reference_answer.strip().lower() else 0.0\n",
    "    \n",
    "    # Cosine Similarity\n",
    "    try:\n",
    "        pred_emb = embedder.embed_query(predicted_answer)\n",
    "        ref_emb = embedder.embed_query(reference_answer)\n",
    "        cosine_sim = float(cosine_similarity([pred_emb], [ref_emb])[0][0])\n",
    "    except Exception:\n",
    "        cosine_sim = 0.0\n",
    "    \n",
    "    return {'f1': f1, 'exact_match': exact_match, 'cosine_similarity': cosine_sim}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 核心函數 3: 提取 Contexts\n",
    "# ============================================================\n",
    "def extract_contexts(retriever_result, max_items=10):\n",
    "    \"\"\"從 retriever_result 提取 contexts\"\"\"\n",
    "    if not retriever_result:\n",
    "        return []\n",
    "    \n",
    "    contexts = []\n",
    "    if hasattr(retriever_result, 'items') and retriever_result.items:\n",
    "        for item in retriever_result.items[:max_items]:\n",
    "            if hasattr(item, 'content'):\n",
    "                contexts.append({'text': item.content})\n",
    "            elif isinstance(item, dict) and 'content' in item:\n",
    "                contexts.append({'text': item['content']})\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 核心函數 4: 完整 Reranking 評估\n",
    "# ============================================================\n",
    "def run_reranking_evaluation(\n",
    "    questions_df,\n",
    "    graph_rag,\n",
    "    embedder,\n",
    "    llm_model,\n",
    "    initial_top_k=15,\n",
    "    rerank_threshold=0.6,\n",
    "    rerank_top_n=5,\n",
    "    max_questions=None,\n",
    "    output_dir=\"rag\",\n",
    "    output_prefix=\"reranking_eval\"\n",
    "):\n",
    "    \"\"\"\n",
    "    對所有問題執行 Baseline vs Reranking 對比評估\n",
    "    \n",
    "    Args:\n",
    "        questions_df: 問題數據集\n",
    "        graph_rag: GraphRAG 實例\n",
    "        embedder: 嵌入模型\n",
    "        llm_model: LLM 模型名稱\n",
    "        initial_top_k: 初始檢索的 TOP_K（推薦 15-20）\n",
    "        rerank_threshold: 二次排序的相似度閾值（推薦 0.5-0.7）\n",
    "        rerank_top_n: 二次排序保留的數量（推薦 5-7）\n",
    "        max_questions: 測試問題數量（None=全部）\n",
    "        output_dir: 輸出目錄\n",
    "        output_prefix: 輸出文件前綴\n",
    "    \n",
    "    Returns:\n",
    "        包含詳細結果的 DataFrame\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    jsonl_path = output_dir / f\"{output_prefix}_{timestamp}.jsonl\"\n",
    "    \n",
    "    print(f\"🚀 開始 Reranking 評估\")\n",
    "    print(f\"   初始檢索: TOP_K={initial_top_k}\")\n",
    "    print(f\"   二次排序: threshold={rerank_threshold}, top_n={rerank_top_n}\")\n",
    "    print(f\"   結果保存至: {jsonl_path}\\n\")\n",
    "    \n",
    "    # 準備問題子集\n",
    "    subset = questions_df.head(max_questions).copy() if max_questions else questions_df.copy()\n",
    "    print(f\"📊 測試 {len(subset)} 個問題\\n\")\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, row in subset.iterrows():\n",
    "        q = row['question']\n",
    "        ref_answer = row.get('reference_answer', row.get('answer', row.get('expected_answer', '')))\n",
    "        \n",
    "        print(f\"[{idx+1}/{len(subset)}] 處理問題...\")\n",
    "        \n",
    "        try:\n",
    "            query_text = f\"{q}\\nPlease answer the above question in English.\"\n",
    "            \n",
    "            # === Baseline: 直接檢索 top_n 個 ===\n",
    "            retriever_config_baseline = {\n",
    "                \"top_k\": rerank_top_n,\n",
    "                \"ranker\": RETRIEVER_RANKER,\n",
    "                \"alpha\": RETRIEVER_ALPHA\n",
    "            }\n",
    "            \n",
    "            response_baseline = graph_rag.search(\n",
    "                query_text=query_text, \n",
    "                retriever_config=retriever_config_baseline, \n",
    "                return_context=True\n",
    "            )\n",
    "            \n",
    "            answer_baseline = response_baseline.answer if hasattr(response_baseline, 'answer') else \"\"\n",
    "            contexts_baseline = extract_contexts(\n",
    "                getattr(response_baseline, \"retriever_result\", None), \n",
    "                rerank_top_n\n",
    "            )\n",
    "            baseline_metrics = calculate_metrics(answer_baseline, ref_answer, embedder)\n",
    "            \n",
    "            # === Reranking: 初始檢索 → 二次排序 → 重新生成答案 ===\n",
    "            retriever_config_rerank = {\n",
    "                \"top_k\": initial_top_k,\n",
    "                \"ranker\": RETRIEVER_RANKER,\n",
    "                \"alpha\": RETRIEVER_ALPHA\n",
    "            }\n",
    "            \n",
    "            response_raw = graph_rag.search(\n",
    "                query_text=query_text, \n",
    "                retriever_config=retriever_config_rerank, \n",
    "                return_context=True\n",
    "            )\n",
    "            \n",
    "            contexts_raw = extract_contexts(\n",
    "                getattr(response_raw, \"retriever_result\", None), \n",
    "                initial_top_k\n",
    "            )\n",
    "            \n",
    "            # 二次 Reranking\n",
    "            contexts_reranked, similarities = rerank_contexts_by_similarity(\n",
    "                contexts_raw, q, embedder, threshold=rerank_threshold, top_n=rerank_top_n\n",
    "            )\n",
    "            \n",
    "            # 用 reranked contexts 重新生成答案\n",
    "            if contexts_reranked:\n",
    "                context_text = \"\\n\\n\".join([\n",
    "                    ctx.get('text', '') if isinstance(ctx, dict) else str(ctx)\n",
    "                    for ctx in contexts_reranked\n",
    "                ])\n",
    "                \n",
    "                rag_prompt = f\"\"\"Based on the following context, please answer the question.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "Please answer the above question in English.\"\"\"\n",
    "                \n",
    "                llm_response = OLLAMA_CLIENT.generate(model=llm_model, prompt=rag_prompt)\n",
    "                answer_reranked = llm_response.get('response', '').strip()\n",
    "            else:\n",
    "                answer_reranked = \"\"\n",
    "            \n",
    "            rerank_metrics = calculate_metrics(answer_reranked, ref_answer, embedder)\n",
    "            \n",
    "            # 保存結果\n",
    "            record = {\n",
    "                'question_id': idx,\n",
    "                'question': q,\n",
    "                'reference_answer': ref_answer,\n",
    "                'baseline_answer': answer_baseline,\n",
    "                'baseline_num_contexts': len(contexts_baseline),\n",
    "                'baseline_f1': baseline_metrics['f1'],\n",
    "                'baseline_exact_match': baseline_metrics['exact_match'],\n",
    "                'baseline_cosine_similarity': baseline_metrics['cosine_similarity'],\n",
    "                'reranked_answer': answer_reranked,\n",
    "                'initial_num_contexts': len(contexts_raw),\n",
    "                'reranked_num_contexts': len(contexts_reranked),\n",
    "                'reranked_f1': rerank_metrics['f1'],\n",
    "                'reranked_exact_match': rerank_metrics['exact_match'],\n",
    "                'reranked_cosine_similarity': rerank_metrics['cosine_similarity'],\n",
    "                'avg_similarity_score': sum(similarities) / len(similarities) if similarities else 0.0,\n",
    "                'min_similarity_score': min(similarities) if similarities else 0.0,\n",
    "                'max_similarity_score': max(similarities) if similarities else 0.0,\n",
    "                'f1_improvement': rerank_metrics['f1'] - baseline_metrics['f1'],\n",
    "                'cosine_improvement': rerank_metrics['cosine_similarity'] - baseline_metrics['cosine_similarity'],\n",
    "                'initial_top_k': initial_top_k,\n",
    "                'rerank_threshold': rerank_threshold,\n",
    "                'rerank_top_n': rerank_top_n\n",
    "            }\n",
    "            \n",
    "            results.append(record)\n",
    "            \n",
    "            with open(jsonl_path, 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "            print(f\"   ✅ Baseline: F1={baseline_metrics['f1']:.4f}, CosSim={baseline_metrics['cosine_similarity']:.4f}\")\n",
    "            print(f\"   ✅ Reranked: F1={rerank_metrics['f1']:.4f}, CosSim={rerank_metrics['cosine_similarity']:.4f}\")\n",
    "            print(f\"   📈 改進: ΔF1={record['f1_improvement']:+.4f}, ΔCosSim={record['cosine_improvement']:+.4f}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 錯誤: {e}\\n\")\n",
    "            continue\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✅ 評估完成！處理問題數: {len(results)}, 耗時: {elapsed/60:.2f} 分鐘\")\n",
    "    print(f\"   結果已保存: {jsonl_path}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    if results:\n",
    "        avg_f1_baseline = sum(r['baseline_f1'] for r in results) / len(results)\n",
    "        avg_f1_rerank = sum(r['reranked_f1'] for r in results) / len(results)\n",
    "        avg_cosine_baseline = sum(r['baseline_cosine_similarity'] for r in results) / len(results)\n",
    "        avg_cosine_rerank = sum(r['reranked_cosine_similarity'] for r in results) / len(results)\n",
    "        \n",
    "        print(\"📊 總體統計:\")\n",
    "        print(f\"   Baseline - 平均 F1: {avg_f1_baseline:.4f}, 平均 Cosine: {avg_cosine_baseline:.4f}\")\n",
    "        print(f\"   Reranked - 平均 F1: {avg_f1_rerank:.4f}, 平均 Cosine: {avg_cosine_rerank:.4f}\")\n",
    "        print(f\"   總體改進 - ΔF1: {avg_f1_rerank - avg_f1_baseline:+.4f}, ΔCosine: {avg_cosine_rerank - avg_cosine_baseline:+.4f}\\n\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"✅ Reranking 評估函數已載入！\")\n",
    "print(\"\\n包含以下核心函數:\")\n",
    "print(\"  1. rerank_contexts_by_similarity() - 二次排序\")\n",
    "print(\"  2. calculate_metrics() - 指標計算\")\n",
    "print(\"  3. extract_contexts() - Context 提取\")\n",
    "print(\"  4. run_reranking_evaluation() - 完整評估流程\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 🚀 執行 Reranking 評估（含檢索參數消融實驗）\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# 📋 消融實驗：定義要測試的參數範圍\n",
    "# ============================================================\n",
    "\n",
    "# 1. 初始檢索 TOP_K 範圍（從 KG 初始檢索多少個候選）\n",
    "INITIAL_TOP_K_VALUES = [5, 10]\n",
    "\n",
    "# 2. Reranking 相似度閾值（過濾低質量 contexts）\n",
    "RERANK_THRESHOLD_VALUES = [0.6]\n",
    "\n",
    "# 3. 最終保留的 TOP_N（Reranking 後保留多少個）\n",
    "RERANK_TOP_N_VALUES = [5]\n",
    "\n",
    "# 4. Retriever Alpha（Vector vs Fulltext 混合比例）\n",
    "RETRIEVER_ALPHA_VALUES = [0.1,0.2,0.3,0.4, 0.5,0.6, 0.7,0.8,0.9,1.0]\n",
    "\n",
    "# 5. Retriever Ranker（混合搜索排序演算法）\n",
    "# ⚠️ 注意：HybridSearchRanker 只有 NAIVE 和 LINEAR 兩個選項\n",
    "RETRIEVER_RANKER_VALUES = [\n",
    "    HybridSearchRanker.LINEAR   # Linear combination (default, recommended)\n",
    "]\n",
    "\n",
    "# 測試問題數量限制（避免消融實驗時間過長）\n",
    "MAX_QUESTIONS_ABLATION = 50  # 建議 20-50\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"📊 Reranking 消融實驗配置\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n🔍 檢索參數範圍:\")\n",
    "print(f\"   Initial TOP_K:      {INITIAL_TOP_K_VALUES}\")\n",
    "print(f\"   Rerank Threshold:   {RERANK_THRESHOLD_VALUES}\")\n",
    "print(f\"   Rerank TOP_N:       {RERANK_TOP_N_VALUES}\")\n",
    "print(f\"   Retriever Alpha:    {RETRIEVER_ALPHA_VALUES}\")\n",
    "print(f\"   Retriever Ranker:   {[r.value for r in RETRIEVER_RANKER_VALUES]}\")\n",
    "print(f\"\\n📝 測試問題數量: {MAX_QUESTIONS_ABLATION}\")\n",
    "\n",
    "# 計算總組合數\n",
    "total_combinations = (\n",
    "    len(INITIAL_TOP_K_VALUES) * \n",
    "    len(RERANK_THRESHOLD_VALUES) * \n",
    "    len(RERANK_TOP_N_VALUES) * \n",
    "    len(RETRIEVER_ALPHA_VALUES) * \n",
    "    len(RETRIEVER_RANKER_VALUES)\n",
    ")\n",
    "print(f\"\\n🧪 總測試組合數: {total_combinations}\")\n",
    "print(f\"⏱️  預估時間: {total_combinations * MAX_QUESTIONS_ABLATION * 3 / 60:.1f} 分鐘\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 執行消融實驗\n",
    "# ============================================================\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "all_ablation_results = []\n",
    "experiment_start_time = time.time()\n",
    "\n",
    "print(\"🚀 開始消融實驗...\\n\")\n",
    "\n",
    "for config_idx, (top_k, threshold, top_n, alpha, ranker) in enumerate(\n",
    "    itertools.product(\n",
    "        INITIAL_TOP_K_VALUES,\n",
    "        RERANK_THRESHOLD_VALUES,\n",
    "        RERANK_TOP_N_VALUES,\n",
    "        RETRIEVER_ALPHA_VALUES,\n",
    "        RETRIEVER_RANKER_VALUES\n",
    "    ), 1\n",
    "):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔬 配置 {config_idx}/{total_combinations}\")\n",
    "    print(f\"   Initial TOP_K={top_k}, Threshold={threshold}, TOP_N={top_n}\")\n",
    "    print(f\"   Alpha={alpha}, Ranker={ranker.value}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    config_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # 暫時修改全局 RETRIEVER 參數\n",
    "        original_alpha = RETRIEVER_ALPHA\n",
    "        original_ranker = RETRIEVER_RANKER\n",
    "        \n",
    "        RETRIEVER_ALPHA = alpha\n",
    "        RETRIEVER_RANKER = ranker\n",
    "        \n",
    "        # 執行評估\n",
    "        results_df = run_reranking_evaluation(\n",
    "            questions_df=QUESTIONS_DF_ABLATION,\n",
    "            graph_rag=GRAPH_RAG,\n",
    "            embedder=ABLATION_EMBEDDER,\n",
    "            llm_model=LLM_MODEL,\n",
    "            initial_top_k=top_k,\n",
    "            rerank_threshold=threshold,\n",
    "            rerank_top_n=top_n,\n",
    "            max_questions=MAX_QUESTIONS_ABLATION,\n",
    "            output_dir='rag',\n",
    "            output_prefix=f'rerank_ablation_config{config_idx:02d}'\n",
    "        )\n",
    "        \n",
    "        # 恢復原始參數\n",
    "        RETRIEVER_ALPHA = original_alpha\n",
    "        RETRIEVER_RANKER = original_ranker\n",
    "        \n",
    "        # 計算總體指標\n",
    "        avg_baseline_f1 = results_df['baseline_f1'].mean()\n",
    "        avg_reranked_f1 = results_df['reranked_f1'].mean()\n",
    "        avg_baseline_cosine = results_df['baseline_cosine_similarity'].mean()\n",
    "        avg_reranked_cosine = results_df['reranked_cosine_similarity'].mean()\n",
    "        \n",
    "        improvement_f1 = avg_reranked_f1 - avg_baseline_f1\n",
    "        improvement_cosine = avg_reranked_cosine - avg_baseline_cosine\n",
    "        \n",
    "        improved_count = (results_df['f1_improvement'] > 0).sum()\n",
    "        improved_ratio = improved_count / len(results_df)\n",
    "        \n",
    "        config_elapsed = time.time() - config_start\n",
    "        \n",
    "        # 保存配置結果\n",
    "        ablation_record = {\n",
    "            'config_id': config_idx,\n",
    "            'initial_top_k': top_k,\n",
    "            'rerank_threshold': threshold,\n",
    "            'rerank_top_n': top_n,\n",
    "            'retriever_alpha': alpha,\n",
    "            'retriever_ranker': ranker.value,\n",
    "            'num_questions': len(results_df),\n",
    "            'avg_baseline_f1': avg_baseline_f1,\n",
    "            'avg_reranked_f1': avg_reranked_f1,\n",
    "            'avg_baseline_cosine': avg_baseline_cosine,\n",
    "            'avg_reranked_cosine': avg_reranked_cosine,\n",
    "            'f1_improvement': improvement_f1,\n",
    "            'cosine_improvement': improvement_cosine,\n",
    "            'improved_ratio': improved_ratio,\n",
    "            'improved_count': improved_count,\n",
    "            'config_elapsed_sec': config_elapsed\n",
    "        }\n",
    "        \n",
    "        all_ablation_results.append(ablation_record)\n",
    "        \n",
    "        print(f\"\\n✅ 配置 {config_idx} 完成！\")\n",
    "        print(f\"   平均 F1 改進:     {improvement_f1:+.4f}\")\n",
    "        print(f\"   平均 Cosine 改進: {improvement_cosine:+.4f}\")\n",
    "        print(f\"   改進問題比例:     {improved_ratio:.1%} ({improved_count}/{len(results_df)})\")\n",
    "        print(f\"   耗時: {config_elapsed:.1f} 秒\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 配置 {config_idx} 執行錯誤: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "experiment_elapsed = time.time() - experiment_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 消融實驗完成！\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   總配置數: {len(all_ablation_results)}/{total_combinations}\")\n",
    "print(f\"   總耗時: {experiment_elapsed/60:.2f} 分鐘\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# 轉換為 DataFrame\n",
    "reranking_ablation_df = pd.DataFrame(all_ablation_results)\n",
    "\n",
    "# 保存結果\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "ablation_csv_path = Path('rag') / f'reranking_ablation_summary_{timestamp}.csv'\n",
    "reranking_ablation_df.to_csv(ablation_csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"💾 消融實驗結果已保存: {ablation_csv_path}\")\n",
    "print(f\"   數據維度: {reranking_ablation_df.shape}\\n\")\n",
    "\n",
    "# 顯示最佳配置\n",
    "print(\"🏆 TOP 5 最佳配置（按 F1 改進排序）:\")\n",
    "top5_configs = reranking_ablation_df.nlargest(5, 'cosine_improvement')\n",
    "print(top5_configs[[\n",
    "    'config_id', 'initial_top_k', 'rerank_threshold', 'rerank_top_n', \n",
    "    'retriever_alpha', 'retriever_ranker', 'f1_improvement', 'cosine_improvement'\n",
    "]].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 📊 Reranking 消融實驗結果分析與視覺化\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def analyze_reranking_ablation(ablation_df):\n",
    "    \"\"\"分析 Reranking 消融實驗結果並產生視覺化報告\"\"\"\n",
    "    \n",
    "    if ablation_df.empty:\n",
    "        print(\"❌ 消融實驗數據框為空，無法分析\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 RERANKING 消融實驗結果分析報告\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # 1. 最佳配置\n",
    "    print(\"1️⃣ 最佳配置 (TOP 3)\")\n",
    "    print(\"\\n   按 F1 改進排序:\")\n",
    "    top3_f1 = ablation_df.nlargest(3, 'f1_improvement')\n",
    "    for i, (idx, row) in enumerate(top3_f1.iterrows(), 1):\n",
    "        print(f\"   {i}. Config #{row['config_id']:02d}: TOP_K={row['initial_top_k']}, Threshold={row['rerank_threshold']}, TOP_N={row['rerank_top_n']}\")\n",
    "        print(f\"      Alpha={row['retriever_alpha']}, Ranker={row['retriever_ranker']}\")\n",
    "        print(f\"      ΔF1={row['f1_improvement']:+.4f}, ΔCosine={row['cosine_improvement']:+.4f}, 改進率={row['improved_ratio']:.1%}\\n\")\n",
    "    \n",
    "    print(\"   按 Cosine 改進排序:\")\n",
    "    top3_cosine = ablation_df.nlargest(3, 'cosine_improvement')\n",
    "    for i, (idx, row) in enumerate(top3_cosine.iterrows(), 1):\n",
    "        print(f\"   {i}. Config #{row['config_id']:02d}: TOP_K={row['initial_top_k']}, Threshold={row['rerank_threshold']}, TOP_N={row['rerank_top_n']}\")\n",
    "        print(f\"      Alpha={row['retriever_alpha']}, Ranker={row['retriever_ranker']}\")\n",
    "        print(f\"      ΔF1={row['f1_improvement']:+.4f}, ΔCosine={row['cosine_improvement']:+.4f}, 改進率={row['improved_ratio']:.1%}\\n\")\n",
    "    \n",
    "    # 2. 參數影響分析\n",
    "    print(\"2️⃣ 參數影響分析\")\n",
    "    \n",
    "    # Initial TOP_K\n",
    "    print(\"\\n   Initial TOP_K 影響:\")\n",
    "    topk_impact = ablation_df.groupby('initial_top_k').agg({\n",
    "        'f1_improvement': 'mean',\n",
    "        'cosine_improvement': 'mean',\n",
    "        'improved_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    print(topk_impact.to_string())\n",
    "    \n",
    "    # Rerank Threshold\n",
    "    print(\"\\n   Rerank Threshold 影響:\")\n",
    "    threshold_impact = ablation_df.groupby('rerank_threshold').agg({\n",
    "        'f1_improvement': 'mean',\n",
    "        'cosine_improvement': 'mean',\n",
    "        'improved_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    print(threshold_impact.to_string())\n",
    "    \n",
    "    # Rerank TOP_N\n",
    "    print(\"\\n   Rerank TOP_N 影響:\")\n",
    "    topn_impact = ablation_df.groupby('rerank_top_n').agg({\n",
    "        'f1_improvement': 'mean',\n",
    "        'cosine_improvement': 'mean',\n",
    "        'improved_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    print(topn_impact.to_string())\n",
    "    \n",
    "    # Retriever Alpha\n",
    "    print(\"\\n   Retriever Alpha 影響:\")\n",
    "    alpha_impact = ablation_df.groupby('retriever_alpha').agg({\n",
    "        'f1_improvement': 'mean',\n",
    "        'cosine_improvement': 'mean',\n",
    "        'improved_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    print(alpha_impact.to_string())\n",
    "    \n",
    "    # Retriever Ranker\n",
    "    print(\"\\n   Retriever Ranker 影響:\")\n",
    "    ranker_impact = ablation_df.groupby('retriever_ranker').agg({\n",
    "        'f1_improvement': 'mean',\n",
    "        'cosine_improvement': 'mean',\n",
    "        'improved_ratio': 'mean'\n",
    "    }).round(4)\n",
    "    print(ranker_impact.to_string())\n",
    "    \n",
    "    # 3. 統計摘要\n",
    "    print(\"\\n3️⃣ 統計摘要\")\n",
    "    print(f\"   F1 改進:\")\n",
    "    print(f\"      平均: {ablation_df['f1_improvement'].mean():.4f}\")\n",
    "    print(f\"      最大: {ablation_df['f1_improvement'].max():.4f}\")\n",
    "    print(f\"      最小: {ablation_df['f1_improvement'].min():.4f}\")\n",
    "    print(f\"      標準差: {ablation_df['f1_improvement'].std():.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Cosine 改進:\")\n",
    "    print(f\"      平均: {ablation_df['cosine_improvement'].mean():.4f}\")\n",
    "    print(f\"      最大: {ablation_df['cosine_improvement'].max():.4f}\")\n",
    "    print(f\"      最小: {ablation_df['cosine_improvement'].min():.4f}\")\n",
    "    print(f\"      標準差: {ablation_df['cosine_improvement'].std():.4f}\")\n",
    "    \n",
    "    print(f\"\\n   改進率:\")\n",
    "    print(f\"      平均: {ablation_df['improved_ratio'].mean():.1%}\")\n",
    "    print(f\"      最大: {ablation_df['improved_ratio'].max():.1%}\")\n",
    "    print(f\"      最小: {ablation_df['improved_ratio'].min():.1%}\")\n",
    "    \n",
    "    # 4. 視覺化\n",
    "    print(\"\\n4️⃣ 生成視覺化圖表...\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 圖 1: F1 改進 vs 配置 ID\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    x = ablation_df['config_id']\n",
    "    ax1.bar(x, ablation_df['f1_improvement'], alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    ax1.axhline(y=ablation_df['f1_improvement'].mean(), color='green', linestyle='-', linewidth=2, \n",
    "                label=f'Mean: {ablation_df[\"f1_improvement\"].mean():.4f}')\n",
    "    ax1.set_xlabel('Configuration ID', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('F1 Improvement', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('F1 Score Improvement by Configuration', fontsize=15, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 圖 2: Initial TOP_K 影響\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    topk_data = ablation_df.groupby('initial_top_k')['f1_improvement'].mean()\n",
    "    ax2.plot(topk_data.index, topk_data.values, 'o-', linewidth=3, markersize=10, color='#2E86AB')\n",
    "    ax2.set_xlabel('Initial TOP_K', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Avg F1 Improvement', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Impact of Initial TOP_K', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 圖 3: Rerank Threshold 影響\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    threshold_data = ablation_df.groupby('rerank_threshold')['f1_improvement'].mean()\n",
    "    ax3.plot(threshold_data.index, threshold_data.values, 's-', linewidth=3, markersize=10, color='#A23B72')\n",
    "    ax3.set_xlabel('Rerank Threshold', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Avg F1 Improvement', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Impact of Rerank Threshold', fontsize=13, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 圖 4: Rerank TOP_N 影響\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    topn_data = ablation_df.groupby('rerank_top_n')['f1_improvement'].mean()\n",
    "    ax4.plot(topn_data.index, topn_data.values, '^-', linewidth=3, markersize=10, color='#F18F01')\n",
    "    ax4.set_xlabel('Rerank TOP_N', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('Avg F1 Improvement', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('Impact of Rerank TOP_N', fontsize=13, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 圖 5: Retriever Alpha 影響\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    alpha_data = ablation_df.groupby('retriever_alpha')['f1_improvement'].mean()\n",
    "    ax5.plot(alpha_data.index, alpha_data.values, 'd-', linewidth=3, markersize=10, color='#6A994E')\n",
    "    ax5.set_xlabel('Retriever Alpha', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Avg F1 Improvement', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Impact of Retriever Alpha', fontsize=13, fontweight='bold')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 圖 6: Retriever Ranker 影響\n",
    "    ax6 = fig.add_subplot(gs[2, 1])\n",
    "    ranker_data = ablation_df.groupby('retriever_ranker')['f1_improvement'].mean()\n",
    "    ax6.bar(range(len(ranker_data)), ranker_data.values, alpha=0.7, color='#BC4749', edgecolor='black')\n",
    "    ax6.set_xticks(range(len(ranker_data)))\n",
    "    ax6.set_xticklabels(ranker_data.index, rotation=15, ha='right')\n",
    "    ax6.set_xlabel('Retriever Ranker', fontsize=12, fontweight='bold')\n",
    "    ax6.set_ylabel('Avg F1 Improvement', fontsize=12, fontweight='bold')\n",
    "    ax6.set_title('Impact of Retriever Ranker', fontsize=13, fontweight='bold')\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 圖 7: Heatmap (TOP_K vs Threshold)\n",
    "    ax7 = fig.add_subplot(gs[2, 2])\n",
    "    heatmap_data = ablation_df.pivot_table(\n",
    "        values='f1_improvement', \n",
    "        index='initial_top_k', \n",
    "        columns='rerank_threshold', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn', center=0, \n",
    "                ax=ax7, cbar_kws={'label': 'F1 Improvement'})\n",
    "    ax7.set_xlabel('Rerank Threshold', fontsize=12, fontweight='bold')\n",
    "    ax7.set_ylabel('Initial TOP_K', fontsize=12, fontweight='bold')\n",
    "    ax7.set_title('F1 Improvement: TOP_K × Threshold', fontsize=13, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Reranking Ablation Study: Parameter Impact Analysis', \n",
    "                 fontsize=17, fontweight='bold', y=0.995)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"   ✅ 視覺化完成！\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# 執行消融實驗分析\n",
    "analyze_reranking_ablation(reranking_ablation_df)\n",
    "\n",
    "# 顯示完整結果表\n",
    "print(\"\\n📋 所有配置詳細結果:\")\n",
    "display_cols = [\n",
    "    'config_id', 'initial_top_k', 'rerank_threshold', 'rerank_top_n', \n",
    "    'retriever_alpha', 'retriever_ranker', \n",
    "    'f1_improvement', 'cosine_improvement', 'improved_ratio'\n",
    "]\n",
    "print(reranking_ablation_df[display_cols].to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
